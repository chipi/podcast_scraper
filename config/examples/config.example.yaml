# Example podcast scraper configuration
#
# Values specified here are used as defaults unless overridden on the CLI
#
# Note: Some settings (like OPENAI_API_KEY, WORKERS, TIMEOUT) are better set via .env file
# See config/examples/.env.example for environment variable options

rss: https://example.com/feed.xml
output_dir: output_podcast_example  # Can also be set via OUTPUT_DIR env var
max_episodes: 10

# Provider configuration - mixed example (can use different providers for different capabilities)
# Options: openai, gemini, mistral, deepseek, grok, ollama (API-based), whisper (local Whisper), spacy (local spaCy), transformers (local HuggingFace)
transcription_provider: whisper  # Options: whisper, openai, gemini, mistral (Note: deepseek, grok, and ollama do NOT support transcription)
speaker_detector_provider: spacy  # Options: spacy, openai, gemini, mistral, deepseek, grok, ollama
summary_provider: transformers  # Options: transformers, openai, gemini, mistral, deepseek, grok, ollama

transcribe_missing: true  # Enable automatic transcription of missing transcripts
whisper_model: base.en  # Whisper model to use (tiny, tiny.en, base, base.en, small, small.en, medium, medium.en, large, large-v1, large-v2, large-v3)

delay_ms: 250
language: en
auto_speakers: true
cache_detected_hosts: true
# known_hosts:  # Optional: show-level host names override (used when RSS metadata doesn't provide clean host names)
#   - Darian Woods  # Example: Use when RSS author is "NPR" (organization) instead of actual host names
#   - Erika Beras
screenplay: true
speaker_names:  # Optional: manual speaker names fallback (used only if auto-detection fails)
  - Host
  - Guest
run_id: experiment
workers: 4  # Number of parallel download workers. Can also be set via WORKERS env var
log_level: INFO  # Can also be set via LOG_LEVEL env var (DEBUG, INFO, WARNING, ERROR)

# Optional: Uncomment to enable
# skip_existing: true  # Skip episodes with existing output (transcripts/metadata)
# reuse_media: true  # Reuse existing media files instead of re-downloading (for faster testing)
# dry_run: true  # Dry run mode (no actual downloads or processing)
# log_file: logs/podcast_scraper.log  # Optional: path to log file (logs written to both console and file)

generate_metadata: true  # Generate metadata documents alongside transcripts
generate_summaries: true  # Generate summaries for episodes
metadata_format: json  # json or yaml (json recommended for database ingestion)
# metadata_subdirectory: metadata  # Optional: subdirectory for metadata files (null = same as transcripts)

# OpenAI configuration (required if using any OpenAI providers)
# Note: OPENAI_API_KEY should be set in .env file (see config/examples/.env.example)
# openai_api_key: your-key-here  # Not recommended - use .env file instead
# openai_api_base: http://localhost:8000/v1  # Optional: Custom OpenAI API base URL (e.g., for E2E testing)

# Gemini configuration (required if using any Gemini providers)
# Note: GEMINI_API_KEY should be set in .env file (see config/examples/.env.example)
# gemini_api_key: your-key-here  # Not recommended - use .env file instead
# gemini_api_base: http://localhost:8000/v1beta  # Optional: Custom Gemini API base URL (e.g., for E2E testing)

# Anthropic API key should be set via environment variable:
# export ANTHROPIC_API_KEY=your-api-key-here
# Or in .env file: ANTHROPIC_API_KEY=your-api-key-here

# DeepSeek configuration (required if using any DeepSeek providers)
# Note: DEEPSEEK_API_KEY should be set in .env file (see config/examples/.env.example)
# Note: DeepSeek does NOT support transcription (no audio API)
# deepseek_api_key: your-key-here  # Not recommended - use .env file instead
# deepseek_api_base: http://localhost:8000/v1  # Optional: Custom DeepSeek API base URL (e.g., for E2E testing)

# Mistral configuration (required if using any Mistral providers)
# Note: MISTRAL_API_KEY should be set in .env file (see config/examples/.env.example)
# Mistral is a full-stack provider (transcription + speaker detection + summarization) with EU data residency
# mistral_api_key: your-key-here  # Not recommended - use .env file instead
# mistral_api_base: http://localhost:8000/v1  # Optional: Custom Mistral API base URL (e.g., for E2E testing)

# Grok configuration (required if using any Grok providers)
# Note: GROK_API_KEY should be set in .env file (see config/examples/.env.example)
# Note: Grok does NOT support transcription (no audio API)
# Grok offers real-time information access via X/Twitter integration
# grok_api_key: your-key-here  # Not recommended - use .env file instead
# grok_api_base: http://localhost:8000/v1  # Optional: Custom Grok API base URL (e.g., for E2E testing)

# Ollama configuration (required if using any Ollama providers)
# Note: OLLAMA_API_BASE can be set in .env file (see config/examples/.env.example) or defaults to http://localhost:11434/v1
# Note: Ollama does NOT support transcription (no audio API)
# Ollama is a local, self-hosted solution with ZERO API costs - all processing happens on your local hardware
# Perfect for privacy, offline operation, or air-gapped environments
# Prerequisites: Ollama must be installed and running locally. Models must be pulled (e.g., ollama pull llama3.3)
# ollama_api_base: http://localhost:11434/v1  # Optional: Custom Ollama API base URL (default: http://localhost:11434/v1)

# OpenAI model configuration (optional - defaults are used if not specified)
# openai_transcription_model: whisper-1  # Default: whisper-1
# openai_speaker_model: gpt-4o-mini  # Default: gpt-4o-mini (use gpt-4o for production)
# openai_summary_model: gpt-4o-mini  # Default: gpt-4o-mini (use gpt-4o for production)
# openai_temperature: 0.3  # Default: 0.3 (0.0-2.0)

# Gemini model configuration (optional - defaults are used if not specified)
# Environment-based defaults:
#   Test/Dev: gemini-1.5-flash (fast, cheap)
#   Production: gemini-1.5-pro (higher quality)
# gemini_transcription_model: gemini-1.5-pro  # Default: environment-based (gemini-1.5-flash for test, gemini-1.5-pro for prod)
# gemini_speaker_model: gemini-1.5-pro  # Default: environment-based
# gemini_summary_model: gemini-1.5-pro  # Default: environment-based
# gemini_temperature: 0.3  # Default: 0.3 (0.0-2.0, lower = more deterministic)
# gemini_max_tokens: null  # Default: null (use model default)

# Mistral model configuration (optional - defaults are used if not specified)
# Environment-based defaults:
#   Test/Dev: voxtral-mini-latest (transcription), mistral-small-latest (speaker/summary)
#   Production: voxtral-mini-latest (transcription), mistral-large-latest (speaker/summary)
# mistral_transcription_model: voxtral-mini-latest  # Default: environment-based (voxtral-mini-latest)
# mistral_speaker_model: mistral-large-latest  # Default: environment-based (mistral-small-latest for test, mistral-large-latest for prod)
# mistral_summary_model: mistral-large-latest  # Default: environment-based (mistral-small-latest for test, mistral-large-latest for prod)
# mistral_temperature: 0.3  # Default: 0.3 (0.0-1.0, lower = more deterministic)
# mistral_max_tokens: null  # Default: null (use model default)

# DeepSeek model configuration (optional - defaults are used if not specified)
# Environment-based defaults:
#   Test/Dev: deepseek-chat (fast, cheap - 95% cheaper than OpenAI)
#   Production: deepseek-chat (same model, but can be configured)
# deepseek_speaker_model: deepseek-chat  # Default: environment-based (deepseek-chat)
# deepseek_summary_model: deepseek-chat  # Default: environment-based (deepseek-chat)
# deepseek_temperature: 0.3  # Default: 0.3 (0.0-2.0, lower = more deterministic)
# deepseek_max_tokens: null  # Default: null (use model default)

# Grok model configuration (optional - defaults are used if not specified)
# Environment-based defaults:
#   Test/Dev: llama-3.1-8b-instant (ultra-fast, free tier)
#   Production: llama-3.3-70b-versatile (best quality, still 10x faster than other providers)
# grok_speaker_model: grok-2  # Default: environment-based (grok-beta for test, grok-2 for prod)
# grok_summary_model: grok-2  # Default: environment-based (grok-beta for test, grok-2 for prod)
# grok_temperature: 0.3  # Default: 0.3 (0.0-2.0, lower = more deterministic)
# grok_max_tokens: null  # Default: null (use model default)

# Ollama model configuration (optional - defaults are used if not specified)
# Default: llama3.1:8b (specific tag, not :latest - avoids unexpected model size resolution)
# ollama_speaker_model: llama3.1:8b  # Default: llama3.1:8b
# ollama_summary_model: llama3.1:8b  # Default: llama3.1:8b
# ollama_temperature: 0.3  # Default: 0.3 (0.0-2.0, lower = more deterministic)
# ollama_max_tokens: null  # Default: null (use model default)
# ollama_timeout: 120  # Default: 120 seconds (local inference can be slow, increase for large models)

# ML-specific settings (used with local transformers/whisper/spaCy providers)
# Defaults: Pegasus-CNN (map) + LED-base (reduce) - production baseline (baseline_ml_prod_authority_v1)
# whisper_device: auto  # Options: cpu, cuda, mps, auto. Can also be set via WHISPER_DEVICE env var
# summary_model: pegasus-cnn  # Options: "pegasus-cnn" (default/prod), "bart-small" (dev), "bart-large", "fast", "pegasus", "long", "long-fast", or direct model ID like "google/pegasus-cnn_dailymail"
# summary_reduce_model: long-fast  # REDUCE model for map-reduce summarization. Options: "long-fast" (default), "long", or direct model ID like "allenai/led-base-16384"
# ner_model: en_core_web_trf  # spaCy NER model for speaker detection. Options: "en_core_web_trf" (default/prod, higher quality), "en_core_web_sm" (dev, fast). Defaults based on environment
# summary_device: auto  # Options: cpu, cuda, mps, auto. Can also be set via SUMMARY_DEVICE env var
# mps_exclusive: true  # Serialize GPU work on MPS to prevent memory contention (default: true). Can also be set via MPS_EXCLUSIVE env var. When enabled and both Whisper and summarization use MPS, transcription completes before summarization starts. I/O operations remain parallel.
# Legacy fields removed: summary_max_length and summary_min_length
# Use summary_map_params and summary_reduce_params instead (see below)
# summary_chunk_size: 2048  # Chunk size for long transcripts in tokens (null = model max length)
# summary_cache_dir: ~/.cache/huggingface/hub  # Custom cache directory for transformer models. Can also be set via SUMMARY_CACHE_DIR or CACHE_DIR env var
# summary_prompt: null  # Optional: custom prompt/instruction to guide summarization

# Advanced ML generation parameters (aligned with baseline_ml_prod_authority_v1)
# These fields are always present with defaults, but you can override them for fine-grained control
# Defaults match the production baseline config values - customize as needed
# summary_map_params:
#   max_new_tokens: 200  # Default: 200 (Pegasus-CNN baseline)
#   min_new_tokens: 80   # Default: 80 (Pegasus-CNN baseline)
#   num_beams: 6         # Default: 6 (Pegasus-CNN baseline, optimized)
#   no_repeat_ngram_size: 3  # Default: 3 (prevents repetition)
#   length_penalty: 1.0  # Default: 1.0 (no length bias)
#   early_stopping: true # Default: true (Pegasus-CNN baseline)
#   repetition_penalty: 1.1  # Default: 1.1 (Pegasus-CNN baseline, optimized)
# summary_reduce_params:
#   max_new_tokens: 650  # Default: 650 (LED-base baseline)
#   min_new_tokens: 220  # Default: 220 (LED-base baseline)
#   num_beams: 4         # Default: 4 (LED-base baseline)
#   no_repeat_ngram_size: 3  # Default: 3 (prevents repetition)
#   length_penalty: 1.0  # Default: 1.0 (no length bias)
#   early_stopping: false # Default: false (LED-base baseline, ensures min_new_tokens)
#   repetition_penalty: 1.12  # Default: 1.12 (LED-base baseline, optimized)
# summary_tokenize:
#   map_max_input_tokens: 1024  # Default: 1024 (from baseline config)
#   reduce_max_input_tokens: 4096  # Default: 4096 (from baseline config)
#   truncation: true  # Default: true (from baseline config)

# Parallelism settings (useful for both OpenAI and ML providers)
# transcription_parallelism: 1  # Number of episodes to transcribe in parallel (Whisper ignores >1, OpenAI uses for parallel API calls). Can also be set via TRANSCRIPTION_PARALLELISM env var
# processing_parallelism: 2  # Number of episodes to process (metadata/summarization) in parallel. Can also be set via PROCESSING_PARALLELISM env var
# summary_batch_size: 1  # Episode-level parallelism: Number of episodes to summarize in parallel. Can also be set via SUMMARY_BATCH_SIZE env var
# summary_chunk_parallelism: 1  # Chunk-level parallelism: Number of chunks to process in parallel within a single episode (CPU-bound, local providers only). Can also be set via SUMMARY_CHUNK_PARALLELISM env var

# Audio preprocessing settings (RFC-040: Optimize audio for API providers with file size limits)
# preprocessing_enabled: false  # Enable audio preprocessing before transcription (default: false)
#   # Preprocessing optimizes audio for API providers (e.g., OpenAI Whisper 25MB limit)
#   # Benefits: File size reduction, API compatibility, cost savings, performance
#   # Requires: ffmpeg installed on system
# preprocessing_cache_dir: null  # Custom cache directory for preprocessed audio (default: .cache/preprocessing)
# preprocessing_sample_rate: 16000  # Target sample rate in Hz (default: 16000, recommended for speech)
# preprocessing_silence_threshold: "-50dB"  # Silence detection threshold (default: -50dB)
# preprocessing_silence_duration: 2.0  # Minimum silence duration to remove in seconds (default: 2.0)
# preprocessing_target_loudness: -16  # Target loudness in LUFS for normalization (default: -16)

# Transcript cleaning configuration (Issue #418: LLM-Based Semantic Transcript Cleaning)
# Strategy for cleaning transcripts before summarization
# Options: "pattern" (regex-based, default for ML providers), "llm" (LLM-based semantic), "hybrid" (pattern + conditional LLM, default for LLM providers)
# transcript_cleaning_strategy: hybrid  # Default: hybrid for LLM providers, pattern for ML providers

# Provider-specific cleaning model configuration (optional - defaults to summary model if not specified)
# Use cheaper/faster models for cleaning to reduce costs
# openai_cleaning_model: gpt-4o-mini  # Default: gpt-4o-mini (cheaper than summary model)
# openai_cleaning_temperature: 0.2  # Default: 0.2 (lower = more deterministic)
# openai_cleaning_max_tokens: null  # Default: null (80-90% of input length)
# openai_cleaning_llm_threshold: 0.10  # Default: 0.10 (use LLM if pattern reduces < 10%)

# gemini_cleaning_model: gemini-2.0-flash  # Default: gemini-2.0-flash (gemini-1.5-flash not available in new API)
# gemini_cleaning_temperature: 0.2
# gemini_cleaning_max_tokens: null
# gemini_cleaning_llm_threshold: 0.10

# anthropic_cleaning_model: claude-3-5-haiku-latest  # Default: claude-3-5-haiku-latest (cheaper)
# anthropic_cleaning_temperature: 0.2
# anthropic_cleaning_max_tokens: null
# anthropic_cleaning_llm_threshold: 0.10

# mistral_cleaning_model: mistral-small-latest  # Default: mistral-small-latest (cheaper)
# mistral_cleaning_temperature: 0.2
# mistral_cleaning_max_tokens: null
# mistral_cleaning_llm_threshold: 0.10

# deepseek_cleaning_model: deepseek-chat  # Default: deepseek-chat
# deepseek_cleaning_temperature: 0.2
# deepseek_cleaning_max_tokens: null
# deepseek_cleaning_llm_threshold: 0.10

# grok_cleaning_model: grok-3-mini  # Default: grok-3-mini (cheaper)
# grok_cleaning_temperature: 0.2
# grok_cleaning_max_tokens: null
# grok_cleaning_llm_threshold: 0.10

# ollama_cleaning_model: llama3.1:8b  # Default: llama3.1:8b (smaller/faster)
# ollama_cleaning_temperature: 0.2
# ollama_cleaning_max_tokens: null
# ollama_cleaning_llm_threshold: 0.10

# Advanced settings
# save_cleaned_transcript: true  # Save cleaned transcript to separate file (e.g., episode.cleaned.txt) for testing
