{
  "_comment": "Example podcast scraper configuration. Note: Some settings (like OPENAI_API_KEY, WORKERS, TIMEOUT) are better set via .env file. See examples/.env.example for environment variable options. Priority: CLI args > Config file > Environment variables > Defaults",

  "rss": "https://example.com/feed.xml",
  "output_dir": "output_podcast_example",
  "max_episodes": 10,

  "_comment_providers": "Provider configuration - mixed example (can use different providers for different capabilities). Options: openai (API-based), whisper (local Whisper), spacy (local spaCy), transformers (local HuggingFace)",
  "transcription_provider": "whisper",
  "speaker_detector_provider": "spacy",
  "summary_provider": "transformers",

  "transcribe_missing": true,
  "whisper_model": "base.en",

  "delay_ms": 250,
  "language": "en",
  "auto_speakers": true,
  "cache_detected_hosts": true,
  "screenplay": true,
  "speaker_names": ["Host", "Guest"],
  "run_id": "experiment",
  "workers": 4,
  "log_level": "INFO",

  "_comment_optional": "Optional settings (uncommented for reference): skip_existing, reuse_media, dry_run, log_file",

  "generate_metadata": true,
  "generate_summaries": true,
  "metadata_format": "json",

  "_comment_openai": "OpenAI configuration (required if using any OpenAI providers). Note: OPENAI_API_KEY should be set in .env file. Optional model settings: openai_transcription_model (default: whisper-1), openai_speaker_model (default: gpt-4o-mini), openai_summary_model (default: gpt-4o-mini), openai_temperature (default: 0.3)",

  "_comment_ml": "ML-specific settings (used with local transformers/whisper providers). Optional: whisper_device (cpu/cuda/mps/auto), summary_model (bart-large/bart-small/fast/pegasus/long/long-fast), summary_reduce_model (long-fast/long), summary_device (cpu/cuda/mps/auto), summary_chunk_size (2048), summary_cache_dir, summary_prompt. Note: summary_max_length and summary_min_length are removed - use summary_map_params and summary_reduce_params instead",
  "_comment_ml_params": "Advanced ML generation parameters (aligned with baseline_bart_small_led_long_fast.yaml). These fields are always present with defaults, but you can override them for fine-grained control. Defaults: summary_map_params: {max_new_tokens: 200, min_new_tokens: 80, num_beams: 4, no_repeat_ngram_size: 3, length_penalty: 1.0, early_stopping: true, repetition_penalty: 1.3}, summary_reduce_params: {max_new_tokens: 650, min_new_tokens: 220, num_beams: 4, no_repeat_ngram_size: 3, length_penalty: 1.0, early_stopping: true, repetition_penalty: 1.3}, summary_tokenize: {map_max_input_tokens: 1024, reduce_max_input_tokens: 4096, truncation: true}",

  "_comment_parallelism": "Parallelism settings (useful for both OpenAI and ML providers). Optional: transcription_parallelism (1), processing_parallelism (2), summary_batch_size (1), summary_chunk_parallelism (1)",

  "_comment_advanced": "Advanced settings. Optional: save_cleaned_transcript (true)",

  "_comment_preprocessing": "Audio preprocessing settings (RFC-040: Optimize audio for API providers with file size limits). Optional: preprocessing_enabled (false), preprocessing_cache_dir (null), preprocessing_sample_rate (16000), preprocessing_silence_threshold (-50dB), preprocessing_silence_duration (2.0), preprocessing_target_loudness (-16). Note: Requires ffmpeg installed on system. Benefits: File size reduction, API compatibility, cost savings, performance"
}
