{
  "_comment": "Example podcast scraper configuration. Note: Some settings (like OPENAI_API_KEY, WORKERS, TIMEOUT) are better set via .env file. See examples/.env.example for environment variable options. Priority: CLI args > Config file > Environment variables > Defaults",

  "rss": "https://example.com/feed.xml",
  "output_dir": "output_podcast_example",
  "max_episodes": 10,

  "_comment_providers": "Provider configuration - mixed example (can use different providers for different capabilities). Options: openai, gemini, mistral, deepseek, grok, ollama (API-based), whisper (local Whisper), spacy (local spaCy), transformers (local HuggingFace). Note: deepseek, grok, and ollama do NOT support transcription (no audio API). Mistral is a full-stack provider (transcription + speaker detection + summarization) with EU data residency. Grok offers real-time information access via X/Twitter integration. Ollama is a local, self-hosted solution with ZERO API costs - perfect for privacy and offline operation",
  "transcription_provider": "whisper",
  "speaker_detector_provider": "spacy",
  "summary_provider": "transformers",

  "transcribe_missing": true,
  "whisper_model": "base.en",

  "delay_ms": 250,
  "language": "en",
  "auto_speakers": true,
  "cache_detected_hosts": true,
  "screenplay": true,
  "speaker_names": ["Host", "Guest"],
  "run_id": "experiment",
  "workers": 4,
  "log_level": "INFO",

  "_comment_optional": "Optional settings (uncommented for reference): skip_existing, reuse_media, dry_run, log_file",

  "generate_metadata": true,
  "generate_summaries": true,
  "metadata_format": "json",

  "_comment_openai": "OpenAI configuration (required if using any OpenAI providers). Note: OPENAI_API_KEY should be set in .env file. Optional model settings: openai_transcription_model (default: whisper-1), openai_speaker_model (default: gpt-4o-mini), openai_summary_model (default: gpt-4o-mini), openai_temperature (default: 0.3)",
  "_comment_gemini": "Gemini configuration (required if using any Gemini providers). Note: GEMINI_API_KEY should be set in .env file. Optional model settings: gemini_transcription_model (default: environment-based, gemini-1.5-flash for test/dev, gemini-1.5-pro for prod), gemini_speaker_model (default: environment-based), gemini_summary_model (default: environment-based), gemini_temperature (default: 0.3), gemini_max_tokens (default: null)",
  "_comment_mistral": "Mistral configuration (required if using any Mistral providers). Note: MISTRAL_API_KEY should be set in .env file. Mistral is a full-stack provider (transcription + speaker detection + summarization) with EU data residency. Optional model settings: mistral_transcription_model (default: environment-based, voxtral-mini-latest), mistral_speaker_model (default: environment-based, mistral-small-latest for test/dev, mistral-large-latest for prod), mistral_summary_model (default: environment-based, mistral-small-latest for test/dev, mistral-large-latest for prod), mistral_temperature (default: 0.3), mistral_max_tokens (default: null)",
  "_comment_deepseek": "DeepSeek configuration (required if using any DeepSeek providers). Note: DEEPSEEK_API_KEY should be set in .env file. Note: DeepSeek does NOT support transcription (no audio API). Optional model settings: deepseek_speaker_model (default: environment-based, deepseek-chat), deepseek_summary_model (default: environment-based, deepseek-chat), deepseek_temperature (default: 0.3), deepseek_max_tokens (default: null). DeepSeek is 95% cheaper than OpenAI.",
  "_comment_grok": "Grok configuration (required if using any Grok providers). Note: GROK_API_KEY should be set in .env file. Note: Grok does NOT support transcription (no audio API). Grok offers real-time information access via X/Twitter integration. Optional model settings: grok_speaker_model (default: environment-based, grok-beta for test/dev, grok-2 for prod), grok_summary_model (default: environment-based), grok_temperature (default: 0.3), grok_max_tokens (default: null)",
  "_comment_ollama": "Ollama configuration (required if using any Ollama providers). Note: OLLAMA_API_BASE can be set in .env file or defaults to http://localhost:11434/v1. Note: Ollama does NOT support transcription (no audio API). Ollama is a local, self-hosted solution with ZERO API costs - all processing happens on your local hardware. Perfect for privacy, offline operation, or air-gapped environments. Prerequisites: Ollama must be installed and running locally. Models must be pulled (e.g., ollama pull llama3.3). Optional model settings: ollama_speaker_model (default: environment-based, llama3.2:latest for test/dev, llama3.3:latest for prod), ollama_summary_model (default: environment-based), ollama_temperature (default: 0.3), ollama_max_tokens (default: null), ollama_timeout (default: 120 seconds)",

  "_comment_ml": "ML-specific settings (used with local transformers/whisper/spaCy providers). Optional: whisper_device (cpu/cuda/mps/auto), summary_model (pegasus-cnn/prod or bart-small/dev, bart-large, fast, pegasus, long, long-fast), summary_reduce_model (long-fast/long), summary_device (cpu/cuda/mps/auto), mps_exclusive (true/false, default: true - serializes GPU work on MPS to prevent memory contention), ner_model (en_core_web_trf/prod or en_core_web_sm/dev), summary_chunk_size (2048), summary_cache_dir, summary_prompt. Note: summary_max_length and summary_min_length are removed - use summary_map_params and summary_reduce_params instead. Defaults: Pegasus-CNN (map) + LED-base (reduce) - production baseline, en_core_web_trf (NER) - production baseline",
  "_comment_ml_params": "Advanced ML generation parameters (aligned with baseline_ml_prod_authority_v1). These fields are always present with defaults, but you can override them for fine-grained control. Defaults: summary_map_params: {max_new_tokens: 200, min_new_tokens: 80, num_beams: 6, no_repeat_ngram_size: 3, length_penalty: 1.0, early_stopping: true, repetition_penalty: 1.1}, summary_reduce_params: {max_new_tokens: 650, min_new_tokens: 220, num_beams: 4, no_repeat_ngram_size: 3, length_penalty: 1.0, early_stopping: false, repetition_penalty: 1.12}, summary_tokenize: {map_max_input_tokens: 1024, reduce_max_input_tokens: 4096, truncation: true}",

  "_comment_parallelism": "Parallelism settings (useful for both OpenAI and ML providers). Optional: transcription_parallelism (1), processing_parallelism (2), summary_batch_size (1), summary_chunk_parallelism (1)",

  "_comment_advanced": "Advanced settings. Optional: save_cleaned_transcript (true)",

  "_comment_cleaning": "Transcript cleaning configuration (Issue #418: LLM-Based Semantic Transcript Cleaning). Optional: transcript_cleaning_strategy (pattern/llm/hybrid, default: hybrid for LLM providers, pattern for ML providers). Provider-specific cleaning settings: {provider}_cleaning_model (defaults to summary model), {provider}_cleaning_temperature (default: 0.2), {provider}_cleaning_max_tokens (default: null, 80-90% of input), {provider}_cleaning_llm_threshold (default: 0.10 for hybrid strategy). Use cheaper models for cleaning to reduce costs. Hybrid strategy uses pattern-based cleaning first, then conditionally applies LLM cleaning when needed (reduces LLM calls by 70-90%).",

  "_comment_preprocessing": "Audio preprocessing settings (RFC-040: Optimize audio for API providers with file size limits). Optional: preprocessing_enabled (false), preprocessing_cache_dir (null), preprocessing_sample_rate (16000), preprocessing_silence_threshold (-50dB), preprocessing_silence_duration (2.0), preprocessing_target_loudness (-16). Note: Requires ffmpeg installed on system. Benefits: File size reduction, API compatibility, cost savings, performance"
}
