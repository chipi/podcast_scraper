# Example podcast scraper configuration
#
# Values specified here are used as defaults unless overridden on the CLI
#
# Many settings can also be configured via environment variables (see examples/.env.example)
# Priority: CLI args > Config file > Environment variables > Defaults
#
# Note: Some settings (like OPENAI_API_KEY, WORKERS, TIMEOUT) are better set via .env file
# See examples/.env.example for environment variable options

rss: <https://example.com/feed.xml>
output_dir: output_podcast_example  # Can also be set via OUTPUT_DIR env var
max_episodes: 10
transcribe_missing: true  # Default: true (automatically transcribe missing transcripts)
whisper_model: base.en  # Default: base.en (matches actual model used for English)
delay_ms: 250
language: en
auto_speakers: true
cache_detected_hosts: true
screenplay: true

# Optional: manual speaker names fallback (used only if auto-detection fails)
speaker_names:
  - Host
  - Guest

run_id: experiment
log_file: null  # Optional: path to log file (logs written to both console and file). Can also be set via LOG_FILE env var
skip_existing: false  # Skip episodes with existing output (transcripts/metadata)
reuse_media: false  # Reuse existing media files instead of re-downloading (for faster testing)
dry_run: false
generate_metadata: true  # Generate metadata documents alongside transcripts
metadata_format: json  # json or yaml (json recommended for database ingestion)
metadata_subdirectory: null  # Optional subdirectory for metadata files (null = same as transcripts)

# Parallelism settings
# These can also be set via environment variables (see examples/.env.example):
# - WORKERS (number of parallel download workers)
# - TRANSCRIPTION_PARALLELISM (episodes to transcribe in parallel)
# - PROCESSING_PARALLELISM (episodes to process in parallel)
# - SUMMARY_BATCH_SIZE (episodes to summarize in parallel)
# - SUMMARY_CHUNK_PARALLELISM (chunks to process in parallel within episode)
# - TIMEOUT (request timeout in seconds)
# - SUMMARY_DEVICE (device for model execution: cpu, cuda, mps, or null for auto)

workers: 4  # Can also be set via WORKERS env var
transcription_parallelism: 1  # Can also be set via TRANSCRIPTION_PARALLELISM env var
processing_parallelism: 2  # Can also be set via PROCESSING_PARALLELISM env var
timeout: 45  # Can also be set via TIMEOUT env var

# Provider selection (can use different providers for different capabilities)
transcription_provider: whisper  # whisper (local) or openai
speaker_detector_provider: spacy  # spacy (local spaCy) or openai (deprecated: ner)
summary_provider: transformers  # transformers (local HuggingFace) or openai (deprecated: local)

# OpenAI configuration
# IMPORTANT: Prefer setting OPENAI_API_KEY via .env file (see examples/.env.example)
# API keys should never be committed to version control
openai_api_key: null  # Optional: OpenAI API key (prefer OPENAI_API_KEY env var or .env file)
openai_api_base: null  # Optional: Custom OpenAI API base URL (e.g., "http://localhost:8000/v1" for E2E testing). Can also be set via OPENAI_API_BASE env var

# Summarization settings (requires torch and transformers dependencies)
generate_summaries: true  # Generate summaries for episodes
summary_model: null  # Optional: MAP model key (e.g., "bart-large", "bart-small", "fast") or direct model ID (e.g., "facebook/bart-large-cnn"). Defaults to "bart-large" for fast chunk summarization
summary_reduce_model: null  # Optional: REDUCE model key (e.g., "long-fast", "long") or direct model ID (e.g., "allenai/led-base-16384"). Defaults to "long-fast" (LED) for accurate final combine
summary_max_length: 150  # Maximum summary length in tokens
summary_min_length: 30  # Minimum summary length in tokens
summary_device: null  # Optional: cuda, mps, cpu, or null for auto-detection. Can also be set via SUMMARY_DEVICE env var
summary_batch_size: 1  # Episode-level parallelism: Number of episodes to summarize in parallel (default: 1). Can also be set via SUMMARY_BATCH_SIZE env var
summary_chunk_parallelism: 1  # Chunk-level parallelism: Number of chunks to process in parallel within a single episode (CPU-bound, local providers only, default: 1). Can also be set via SUMMARY_CHUNK_PARALLELISM env var
summary_chunk_size: null  # Optional: chunk size for long transcripts in tokens (null = model max length)
summary_cache_dir: null  # Optional: custom cache directory for transformer models (default: ~/.cache/huggingface/hub). Can also be set via SUMMARY_CACHE_DIR or CACHE_DIR env var
summary_prompt: null  # Optional: custom prompt/instruction to guide summarization (default: built-in prompt)
save_cleaned_transcript: true  # Save cleaned transcript to separate file (e.g., episode.cleaned.txt) for testing (default: true)
