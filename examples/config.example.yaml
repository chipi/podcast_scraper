# Example podcast scraper configuration
#
# Values specified here are used as defaults unless overridden on the CLI
#
# Note: Some settings (like OPENAI_API_KEY, WORKERS, TIMEOUT) are better set via .env file
# See examples/.env.example for environment variable options

rss: https://example.com/feed.xml
output_dir: output_podcast_example  # Can also be set via OUTPUT_DIR env var
max_episodes: 10

# Provider configuration - mixed example (can use different providers for different capabilities)
# Options: openai (API-based), whisper (local Whisper), spacy (local spaCy), transformers (local HuggingFace)
transcription_provider: whisper  # Options: openai, whisper
speaker_detector_provider: spacy  # Options: openai, spacy
summary_provider: transformers  # Options: openai, transformers

transcribe_missing: true  # Enable automatic transcription of missing transcripts
whisper_model: base.en  # Whisper model to use (tiny, tiny.en, base, base.en, small, small.en, medium, medium.en, large, large-v1, large-v2, large-v3)

delay_ms: 250
language: en
auto_speakers: true
cache_detected_hosts: true
# known_hosts:  # Optional: show-level host names override (used when RSS metadata doesn't provide clean host names)
#   - Darian Woods  # Example: Use when RSS author is "NPR" (organization) instead of actual host names
#   - Erika Beras
screenplay: true
speaker_names:  # Optional: manual speaker names fallback (used only if auto-detection fails)
  - Host
  - Guest
run_id: experiment
workers: 4  # Number of parallel download workers. Can also be set via WORKERS env var
log_level: INFO  # Can also be set via LOG_LEVEL env var (DEBUG, INFO, WARNING, ERROR)

# Optional: Uncomment to enable
# skip_existing: true  # Skip episodes with existing output (transcripts/metadata)
# reuse_media: true  # Reuse existing media files instead of re-downloading (for faster testing)
# dry_run: true  # Dry run mode (no actual downloads or processing)
# log_file: logs/podcast_scraper.log  # Optional: path to log file (logs written to both console and file)

generate_metadata: true  # Generate metadata documents alongside transcripts
generate_summaries: true  # Generate summaries for episodes
metadata_format: json  # json or yaml (json recommended for database ingestion)
# metadata_subdirectory: metadata  # Optional: subdirectory for metadata files (null = same as transcripts)

# OpenAI configuration (required if using any OpenAI providers)
# Note: OPENAI_API_KEY should be set in .env file (see examples/.env.example)
# openai_api_key: your-key-here  # Not recommended - use .env file instead
# openai_api_base: http://localhost:8000/v1  # Optional: Custom OpenAI API base URL (e.g., for E2E testing)

# OpenAI model configuration (optional - defaults are used if not specified)
# openai_transcription_model: whisper-1  # Default: whisper-1
# openai_speaker_model: gpt-4o-mini  # Default: gpt-4o-mini (use gpt-4o for production)
# openai_summary_model: gpt-4o-mini  # Default: gpt-4o-mini (use gpt-4o for production)
# openai_temperature: 0.3  # Default: 0.3 (0.0-2.0)

# ML-specific settings (used with local transformers/whisper/spaCy providers)
# Defaults: Pegasus-CNN (map) + LED-base (reduce) - production baseline (baseline_ml_prod_authority_v1)
# whisper_device: auto  # Options: cpu, cuda, mps, auto. Can also be set via WHISPER_DEVICE env var
# summary_model: pegasus-cnn  # Options: "pegasus-cnn" (default/prod), "bart-small" (dev), "bart-large", "fast", "pegasus", "long", "long-fast", or direct model ID like "google/pegasus-cnn_dailymail"
# summary_reduce_model: long-fast  # REDUCE model for map-reduce summarization. Options: "long-fast" (default), "long", or direct model ID like "allenai/led-base-16384"
# ner_model: en_core_web_trf  # spaCy NER model for speaker detection. Options: "en_core_web_trf" (default/prod, higher quality), "en_core_web_sm" (dev, fast). Defaults based on environment
# summary_device: auto  # Options: cpu, cuda, mps, auto. Can also be set via SUMMARY_DEVICE env var
# mps_exclusive: true  # Serialize GPU work on MPS to prevent memory contention (default: true). Can also be set via MPS_EXCLUSIVE env var. When enabled and both Whisper and summarization use MPS, transcription completes before summarization starts. I/O operations remain parallel.
# Legacy fields removed: summary_max_length and summary_min_length
# Use summary_map_params and summary_reduce_params instead (see below)
# summary_chunk_size: 2048  # Chunk size for long transcripts in tokens (null = model max length)
# summary_cache_dir: ~/.cache/huggingface/hub  # Custom cache directory for transformer models. Can also be set via SUMMARY_CACHE_DIR or CACHE_DIR env var
# summary_prompt: null  # Optional: custom prompt/instruction to guide summarization

# Advanced ML generation parameters (aligned with baseline_ml_prod_authority_v1)
# These fields are always present with defaults, but you can override them for fine-grained control
# Defaults match the production baseline config values - customize as needed
# summary_map_params:
#   max_new_tokens: 200  # Default: 200 (Pegasus-CNN baseline)
#   min_new_tokens: 80   # Default: 80 (Pegasus-CNN baseline)
#   num_beams: 6         # Default: 6 (Pegasus-CNN baseline, optimized)
#   no_repeat_ngram_size: 3  # Default: 3 (prevents repetition)
#   length_penalty: 1.0  # Default: 1.0 (no length bias)
#   early_stopping: true # Default: true (Pegasus-CNN baseline)
#   repetition_penalty: 1.1  # Default: 1.1 (Pegasus-CNN baseline, optimized)
# summary_reduce_params:
#   max_new_tokens: 650  # Default: 650 (LED-base baseline)
#   min_new_tokens: 220  # Default: 220 (LED-base baseline)
#   num_beams: 4         # Default: 4 (LED-base baseline)
#   no_repeat_ngram_size: 3  # Default: 3 (prevents repetition)
#   length_penalty: 1.0  # Default: 1.0 (no length bias)
#   early_stopping: false # Default: false (LED-base baseline, ensures min_new_tokens)
#   repetition_penalty: 1.12  # Default: 1.12 (LED-base baseline, optimized)
# summary_tokenize:
#   map_max_input_tokens: 1024  # Default: 1024 (from baseline config)
#   reduce_max_input_tokens: 4096  # Default: 4096 (from baseline config)
#   truncation: true  # Default: true (from baseline config)

# Parallelism settings (useful for both OpenAI and ML providers)
# transcription_parallelism: 1  # Number of episodes to transcribe in parallel (Whisper ignores >1, OpenAI uses for parallel API calls). Can also be set via TRANSCRIPTION_PARALLELISM env var
# processing_parallelism: 2  # Number of episodes to process (metadata/summarization) in parallel. Can also be set via PROCESSING_PARALLELISM env var
# summary_batch_size: 1  # Episode-level parallelism: Number of episodes to summarize in parallel. Can also be set via SUMMARY_BATCH_SIZE env var
# summary_chunk_parallelism: 1  # Chunk-level parallelism: Number of chunks to process in parallel within a single episode (CPU-bound, local providers only). Can also be set via SUMMARY_CHUNK_PARALLELISM env var

# Audio preprocessing settings (RFC-040: Optimize audio for API providers with file size limits)
# preprocessing_enabled: false  # Enable audio preprocessing before transcription (default: false)
#   # Preprocessing optimizes audio for API providers (e.g., OpenAI Whisper 25MB limit)
#   # Benefits: File size reduction, API compatibility, cost savings, performance
#   # Requires: ffmpeg installed on system
# preprocessing_cache_dir: null  # Custom cache directory for preprocessed audio (default: .cache/preprocessing)
# preprocessing_sample_rate: 16000  # Target sample rate in Hz (default: 16000, recommended for speech)
# preprocessing_silence_threshold: "-50dB"  # Silence detection threshold (default: -50dB)
# preprocessing_silence_duration: 2.0  # Minimum silence duration to remove in seconds (default: 2.0)
# preprocessing_target_loudness: -16  # Target loudness in LUFS for normalization (default: -16)

# Advanced settings
# save_cleaned_transcript: true  # Save cleaned transcript to separate file (e.g., episode.cleaned.txt) for testing
