# Podcast Scraper Environment Variables
# Copy this file to .env in the project root and fill in your values
# DO NOT commit .env to git!

# ============================================================================
# OpenAI API Configuration
# ============================================================================

# OpenAI API key (required for OpenAI providers)
# Required when using: transcription_provider=openai, speaker_detector_provider=openai, or summary_provider=openai
OPENAI_API_KEY=sk-your-actual-api-key-here

# Optional: OpenAI Organization ID (if you're in multiple orgs)
# OPENAI_ORGANIZATION=org-your-org-id

# Optional: Custom API base URL (for proxies)
# OPENAI_API_BASE=https://api.openai.com/v1

# ============================================================================
# Logging Configuration
# ============================================================================

# Logging level (takes precedence over config file)
# Valid values: DEBUG, INFO, WARNING, ERROR, CRITICAL
# LOG_LEVEL=INFO

# Optional: Log file path
# LOG_FILE=/var/log/podcast_scraper.log

# ============================================================================
# Path Configuration
# ============================================================================

# Output directory for transcripts and metadata
# OUTPUT_DIR=/data/transcripts

# Cache directory for ML models (Transformers, Whisper, spaCy)
# If not set, uses local .cache/ directory in project root, or standard user cache
# CACHE_DIR=/cache/models

# Summary model cache directory (overrides CACHE_DIR for Transformers models)
# SUMMARY_CACHE_DIR=/cache/transformers

# ============================================================================
# Performance Configuration
# ============================================================================

# Number of parallel download workers
# Default: CPU count bounded between 1 and 8
# WORKERS=4

# Number of episodes to transcribe in parallel (episode-level parallelism)
# Note: Local Whisper ignores values > 1 (sequential only)
# Default: 1
# TRANSCRIPTION_PARALLELISM=3

# Number of episodes to process (metadata/summarization) in parallel
# Default: 2
# PROCESSING_PARALLELISM=4

# Number of episodes to summarize in parallel (episode-level parallelism)
# Default: 2
# SUMMARY_BATCH_SIZE=3

# Number of chunks to process in parallel within a single episode (CPU-bound, local providers only)
# Default: 1
# SUMMARY_CHUNK_PARALLELISM=2

# Request timeout in seconds for HTTP requests
# Default: 20 seconds
# Minimum: 1 second
# TIMEOUT=60

# Device for model execution (CPU, CUDA, MPS, or empty for auto-detection)
# Valid values: cpu, cuda, mps, or empty string (for auto-detect)
# Default: empty (auto-detect)
# SUMMARY_DEVICE=cpu

# ============================================================================
# ML Library Configuration (Advanced)
# ============================================================================

# Disable Hugging Face Hub progress bars (suppresses misleading "Downloading" messages)
# Default: 1 (disabled) - set programmatically by application, but can be overridden here
# Set to 0 to enable progress bars
# HF_HUB_DISABLE_PROGRESS_BARS=1

# PyTorch thread configuration (advanced performance tuning)
# These control how many threads PyTorch uses for CPU operations
# Not set by default - uses all available CPU cores for best performance
# Set these if you want to limit CPU usage (e.g., in Docker containers with limited resources)
# 
# Note: In test environments, these are automatically set to 1 to reduce resource usage.
# In production, leave unset for maximum performance, or set to limit resource usage.
# OMP_NUM_THREADS=4
# MKL_NUM_THREADS=4
# TORCH_NUM_THREADS=4

# ============================================================================
# Notes
# ============================================================================
#
# Priority Order:
# 1. Config file field (highest priority)
# 2. Environment variable (this file or system env)
# 3. Default value (lowest priority)
#
# Exception: LOG_LEVEL environment variable takes precedence over config file
#
# Security:
# - Never commit .env files containing API keys
# - API keys are never logged or exposed in error messages
# - Use separate keys for development/production
# - Rotate API keys regularly
#
# See Also:
# - docs/api/CONFIGURATION.md - Complete environment variable documentation
# - examples/config.example.yaml - Configuration file examples
