# Podcast Scraper Environment Variables
# Copy this file to .env in the project root and fill in your values
# DO NOT commit .env to git!

# ============================================================================
# Hugging Face / Transformers Offline Mode (Test Only)
# ============================================================================
# These are automatically set in test environments (tests/conftest.py)
# In production, models are downloaded via centralized preload script logic
# Libraries never download on their own - always use local_files_only=True
# Only set these manually if you want to force offline mode in production
# HF_HUB_OFFLINE=1
# TRANSFORMERS_OFFLINE=1

# ============================================================================
# Hugging Face Hub Configuration
# ============================================================================
# Disable progress bars to avoid misleading "Downloading" messages
# when loading models from cache (default: 1, set automatically by workflow)
HF_HUB_DISABLE_PROGRESS_BARS=1

# Optional: Override Hugging Face Hub cache directory (highest priority)
# If not set, uses CACHE_DIR/huggingface/hub (see Cache Configuration below)
# or local .cache/huggingface/hub/ in project root, or ~/.cache/huggingface/hub/
# HF_HUB_CACHE=.cache/huggingface/hub

# ============================================================================
# OpenAI API Configuration (if using OpenAI providers)
# ============================================================================
# OpenAI API key (required for OpenAI providers)
# Required when using: transcription_provider=openai, speaker_detector_provider=openai, or summary_provider=openai
# OPENAI_API_KEY=sk-your-actual-api-key-here
# Optional: OpenAI Organization ID (if you're in multiple orgs)
# OPENAI_ORGANIZATION=org-your-org-id
# Optional: Custom API base URL (for proxies)
# OPENAI_API_BASE=https://api.openai.com/v1

# ============================================================================
# Logging Configuration
# ============================================================================
# Logging level (takes precedence over config file)
# Valid values: DEBUG, INFO, WARNING, ERROR, CRITICAL
# LOG_LEVEL=INFO
# Optional: Log file path
# LOG_FILE=/var/log/podcast_scraper.log

# ============================================================================
# Path Configuration
# ============================================================================
# Output directory for transcripts and metadata
# OUTPUT_DIR=/data/transcripts

# ============================================================================
# Performance Configuration (Optional)
# ============================================================================
# Number of parallel download workers
# Default: CPU count bounded between 1 and 8
# WORKERS=4

# Number of episodes to transcribe in parallel (episode-level parallelism)
# Note: Local Whisper ignores values > 1 (sequential only)
# Default: 1
# TRANSCRIPTION_PARALLELISM=3

# Number of episodes to process (metadata/summarization) in parallel
# Default: 2
# PROCESSING_PARALLELISM=4

# Number of episodes to summarize in parallel (episode-level parallelism)
# SUMMARY_BATCH_SIZE=3

# Number of chunks to process in parallel within a single episode (CPU-bound, local providers only)
# SUMMARY_CHUNK_PARALLELISM=2

# Request timeout in seconds for HTTP requests
# Default: 20 seconds
# Minimum: 1 second
# TIMEOUT=60

# Device for model execution (CPU, CUDA, MPS, or empty for auto-detection)
# Valid values: cpu, cuda, mps, or empty string (for auto-detect)
# Default: empty (auto-detect)
# SUMMARY_DEVICE=cpu

# PyTorch thread configuration (advanced performance tuning)
# These control how many threads PyTorch uses for CPU operations
# Not set by default - uses all available CPU cores for best performance
# Set these if you want to limit CPU usage (e.g., in Docker containers with limited resources)
# Note: In test environments, these are automatically set to 1 to reduce resource usage.
# In production, leave unset for maximum performance, or set to limit resource usage.
# OMP_NUM_THREADS=4
# MKL_NUM_THREADS=4
# TORCH_NUM_THREADS=4

# ============================================================================
# Cache Configuration
# ============================================================================
# Base cache directory for all ML models (Whisper, Transformers, spaCy)
# If set, specific cache directories are automatically derived from this:
#   - Whisper: CACHE_DIR/whisper
#   - Transformers: CACHE_DIR/huggingface/hub
#   - spaCy: CACHE_DIR/spacy (if applicable)
CACHE_DIR=.cache

# Optional: Override specific cache directories (only if you need different paths)
#   - SUMMARY_CACHE_DIR: Override for Transformers models (via config system)
#     If set, used by cfg.summary_cache_dir â†’ SummaryModel(cache_dir=...)
#     If not set and CACHE_DIR is set, automatically derives CACHE_DIR/huggingface/hub
#   - WHISPER_CACHE_DIR: Override for Whisper models (via cache_utils)
#   - HF_HUB_CACHE: Override for Transformers models (via cache_utils, highest priority)
# WHISPER_CACHE_DIR=.cache/whisper
# HF_HUB_CACHE=.cache/huggingface/hub

# ============================================================================
# Notes
# ============================================================================
# Priority Order:
# 1. Config file field (highest priority)
# 2. Environment variable (this file or system env)
# 3. Default value (lowest priority)
# Exception: LOG_LEVEL environment variable takes precedence over config file
#
# Cache Directory Priority (for Transformers models):
# 1. HF_HUB_CACHE env var (highest priority - recommended)
# 2. SUMMARY_CACHE_DIR env var (via config system)
# 3. CACHE_DIR env var (derives CACHE_DIR/huggingface/hub)
# 4. Local .cache/huggingface/hub/ in project root (if exists)
# 5. ~/.cache/huggingface/hub/ (default fallback)
#
# Security:
# - Never commit .env files containing API keys
# - API keys are never logged or exposed in error messages
# - Use separate keys for development/production
# - Rotate API keys regularly
#
# See Also:
# - docs/api/CONFIGURATION.md - Complete environment variable documentation
# - examples/config.example.yaml - Configuration file examples
