# OpenAI API Configuration
# Copy this file to .env and fill in your actual values
# DO NOT commit .env to git!

# OpenAI API Key (required for OpenAI providers)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-api-key-here

# Optional: OpenAI Organization ID (if you're in multiple orgs)
# OPENAI_ORGANIZATION=org-your-org-id

# Optional: Custom API base URL (for proxies or custom endpoints)
# OPENAI_API_BASE=https://api.openai.com/v1

# Logging (optional)
LOG_LEVEL=INFO

# Optional: Log file path (logs written to both console and file)
# LOG_FILE=/var/log/podcast_scraper.log

# Path Configuration (optional)
# Output directory for transcripts and metadata
# If not set, auto-generated from RSS URL
# OUTPUT_DIR=/data/transcripts

# Cache directory for transformer models (Hugging Face)
# Can use either SUMMARY_CACHE_DIR or CACHE_DIR
# If not set, uses system default cache directory
# SUMMARY_CACHE_DIR=/cache/models
# Or use shorter alias:
# CACHE_DIR=/cache/models

# Performance Configuration (optional)
# Number of parallel download workers (default: CPU count, bounded 1-8)
# WORKERS=4

# Episode-level transcription parallelism (default: 1)
# Local Whisper ignores >1, OpenAI uses for parallel API calls
# TRANSCRIPTION_PARALLELISM=3

# Episode-level processing parallelism (default: 2)
# PROCESSING_PARALLELISM=4

# Episode-level summarization batch size (default: 2)
# SUMMARY_BATCH_SIZE=3

# Chunk-level parallelism for summarization (default: 1)
# CPU-bound, local providers only (API providers handle internally)
# SUMMARY_CHUNK_PARALLELISM=2

# Request timeout in seconds (default: 20, minimum: 1)
# TIMEOUT=60

# Device for model execution (default: None/auto-detect)
# Valid values: cpu, cuda, mps, or empty for auto-detect
# SUMMARY_DEVICE=cpu
