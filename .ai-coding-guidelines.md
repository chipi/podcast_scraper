# AI Coding Guidelines for podcast_scraper

**Purpose:** This document provides context and guidelines for AI coding assistants (Cursor, GitHub Copilot, etc.) working on this project.

**For human contributors**, see [CONTRIBUTING.md](CONTRIBUTING.md) instead.

---

## Table of Contents

1. [Project Context](#project-context)
2. [When to Run What](#when-to-run-what)
3. [Code Organization](#code-organization)
4. [Testing Requirements](#testing-requirements)
5. [Documentation Standards](#documentation-standards)
6. [Common Patterns](#common-patterns)
7. [Security & Secrets](#security--secrets)
8. [Git Workflow](#git-workflow)
9. [Decision Trees](#decision-trees)
10. [When to Ask](#when-to-ask)

---

## Project Context

### What This Project Does

**podcast_scraper** is a Python tool for downloading podcast transcripts from RSS feeds with optional Whisper transcription fallback.

**Key capabilities:**
- Download published transcripts from RSS feeds (Podcasting 2.0 namespace)
- Fallback to Whisper transcription for episodes without transcripts
- Automatic speaker detection using spaCy NER
- Episode summarization using local transformer models (BART, LED)
- Metadata generation (JSON/YAML)
- Multi-threaded downloads
- Screenplay formatting
- Service/daemon mode for automated runs

**Primary use case:** Personal, non-commercial transcript archival for research and study.

### Architecture Overview

**Design:** Modular architecture with clear separation of concerns

**13 core modules:**
1. `cli.py` - Interactive command-line interface
2. `service.py` - Non-interactive service API for daemons
3. `workflow.py` - Pipeline orchestration
4. `config.py` - Configuration model (Pydantic)
5. `rss_parser.py` - RSS feed parsing
6. `downloader.py` - HTTP operations with retry logic
7. `episode_processor.py` - Episode-level processing
8. `filesystem.py` - File system utilities
9. `whisper_integration.py` - Whisper transcription interface
10. `speaker_detection.py` - NER-based speaker extraction
11. `summarizer.py` - Transcript summarization
12. `metadata.py` - Metadata document generation
13. `progress.py` - Progress reporting abstraction

**Data flow:**
```
RSS Feed → Parse Episodes → Download/Transcribe → Detect Speakers → 
Generate Metadata → Summarize → Write Files
```

**See also:** `docs/ARCHITECTURE.md` for detailed design

### Key Technologies

**Core:**
- Python 3.10+ (type hints, Pydantic v2)
- Pydantic for configuration validation
- Click for CLI (optional dependency)
- defusedxml for safe XML parsing

**ML/AI:**
- OpenAI Whisper for transcription
- spaCy for NER (speaker detection)
- PyTorch + transformers for summarization (BART, PEGASUS, LED)

**Testing:**
- pytest with unittest.mock
- pytest markers: `@pytest.mark.slow`, `@pytest.mark.integration`
- Coverage target: >80%

**Documentation:**
- MkDocs with Material theme
- mkdocstrings for API docs (auto-generated from docstrings)
- Mermaid for diagrams

**Code Quality:**
- black (formatting, line length: 100)
- isort (import sorting)
- flake8 (linting)
- mypy (type checking)
- bandit (security scanning)

### Project Structure

```
podcast_scraper/
├── __init__.py              # Public API exports
├── cli.py                   # CLI interface
├── service.py               # Service API
├── workflow.py              # Pipeline orchestration
├── config.py                # Configuration model
├── [10 more modules]        # Core functionality
├── tests/                   # Test suite
├── docs/                    # MkDocs documentation
├── examples/                # Config examples
├── scripts/                 # Evaluation scripts
├── Makefile                 # Development commands
├── pyproject.toml           # Package metadata
├── requirements.txt         # Dependencies
└── CONTRIBUTING.md          # Contributor guide
```

---

## When to Run What

### During Development (Local)

**Before every commit:**
```bash
make format      # Auto-format with black/isort
make test        # Run tests with coverage
```

**Before every push:**
```bash
make ci          # Full CI suite (formatting, lint, type, security, test, docs, build)
```

**When to run specific checks:**
- `make lint` - After adding new functions (flake8)
- `make type` - After changing type hints (mypy)
- `make security` - After changing dependencies (bandit, pip-audit)
- `make docs` - After updating docstrings or docs/

### What CI Runs Automatically

**On every push to PR:**
- Code formatting checks (black, isort)
- Linting (flake8)
- Type checking (mypy)
- Security scans (bandit, pip-audit)
- Full test suite with coverage
- Documentation build (mkdocs build --strict)
- Package build (python -m build)

**When CI fails:**
1. Read the error message carefully
2. Run `make ci` locally to reproduce
3. Fix the issue
4. Test locally again
5. Push the fix

### What NOT to Run

**Don't run these during development:**
- ❌ Long-running integration tests (use `pytest -m "not slow"`)
- ❌ Full model downloads for every test
- ❌ Docker builds (unless working on Docker specifically)
- ❌ Deployment commands (GitHub Actions handles this)

**Use CI for:**
- Full test suite on every platform
- Documentation deployment
- Release builds

---

## Code Organization

### When to Create New Files

**Create a new module when:**
- Implementing a new major feature (>500 lines)
- Feature has distinct responsibility (SRP)
- Feature can be tested independently
- Example: Adding PostgreSQL export → create `export.py`

**Modify existing files when:**
- Fixing bugs in existing functionality
- Enhancing existing features (< 200 lines added)
- Refactoring within same module
- Example: Improving Whisper model selection → modify `whisper_integration.py`

**Don't create:**
- Helper modules with only 1-2 functions (add to appropriate existing module)
- Duplicate functionality (search codebase first)
- Temporary files (use proper temporary directories)

### Module Boundaries (STRICT)

**Respect these boundaries** - don't mix concerns:

| Module | Responsibility | What to AVOID |
|--------|---------------|---------------|
| `cli.py` | CLI interface only | Business logic, HTTP calls, file I/O |
| `service.py` | Service API, structured results | CLI interaction, user prompts |
| `workflow.py` | Orchestration only | HTTP details, file parsing, direct I/O |
| `config.py` | Configuration model | Business logic, side effects |
| `downloader.py` | HTTP operations only | Parsing, business logic |
| `filesystem.py` | File system utilities | HTTP, parsing, business logic |
| `rss_parser.py` | RSS parsing, Episode creation | HTTP, file I/O, workflow |
| `episode_processor.py` | Episode-level processing | CLI interaction, orchestration |
| `whisper_integration.py` | Whisper interface | HTTP, RSS parsing |
| `speaker_detection.py` | NER extraction | HTTP, file I/O |
| `summarizer.py` | Summarization | Episode processing, HTTP |
| `metadata.py` | Metadata generation | HTTP, RSS parsing |
| `progress.py` | Progress abstraction | Business logic |

**If you're about to:**
- Add HTTP calls to `cli.py` → ❌ Use `downloader.py` instead
- Add business logic to `config.py` → ❌ Use `workflow.py` or appropriate module
- Add CLI prompts to `service.py` → ❌ Service is non-interactive

### File Naming Conventions

**Python modules:**
- Use `snake_case.py`
- Be descriptive: `speaker_detection.py` not `speakers.py`
- Avoid generic names: `utils.py`, `helpers.py`

**Test files:**
- Mirror structure: `test_<module>.py`
- Example: `test_speaker_detection.py` tests `speaker_detection.py`

**Documentation:**
- Use `UPPER_CASE.md` for top-level docs: `README.md`, `CONTRIBUTING.md`
- Use descriptive names for guides: `TESTING_STRATEGY.md`
- PRD/RFC format: `PRD-NNN-title.md`, `RFC-NNN-title.md`

---

## Testing Requirements

### Every New Function Must Have

**1. Unit test with mocks:**
```python
@patch("podcast_scraper.downloader.requests.Session")
def test_fetch_url_with_retry(self, mock_session):
    """Test that fetch_url retries on network failure."""
    mock_session.get.side_effect = [
        requests.ConnectionError("Network error"),
        MockHTTPResponse(content="Success", status_code=200)
    ]
    result = fetch_url("https://example.com")
    self.assertEqual(result, "Success")
```

**2. Test both success and failure:**
```python
def test_sanitize_filename_valid(self):
    """Happy path."""
    pass

def test_sanitize_filename_invalid_chars(self):
    """Error case."""
    pass
```

**3. Descriptive test names:**
```python
# Good
def test_config_validation_raises_error_for_negative_workers(self):
    pass

# Bad
def test_config(self):
    pass
```

### Mock External Dependencies

**Always mock:**
- HTTP requests (`requests` module)
- Whisper models (`whisper.load_model()`, `whisper.transcribe()`)
- File I/O (use `tempfile.TemporaryDirectory`)
- spaCy models (mock NER extraction)
- Time-based operations (`time.sleep()`)

**Example:**
```python
@patch("podcast_scraper.whisper_integration.whisper")
def test_transcription(self, mock_whisper):
    mock_whisper.load_model.return_value = Mock()
    mock_whisper.transcribe.return_value = {"text": "Test"}
    # ... test code ...
```

### Test Organization

**Structure:**
```
tests/
├── __init__.py
├── conftest.py              # Shared fixtures
├── test_cli.py              # CLI tests
├── test_service.py          # Service API tests
├── test_[module].py         # One per module
└── test_integration.py      # Integration tests
```

**Test markers:**
```python
@pytest.mark.slow           # Slow tests (skip in dev)
@pytest.mark.integration    # Integration tests
@pytest.mark.e2e            # End-to-end tests (not yet implemented)
```

**Run tests:**
```bash
pytest                       # All tests
pytest -m "not slow"        # Skip slow tests
pytest -v                   # Verbose
pytest tests/test_cli.py    # Specific file
```

---

## Documentation Standards

### When to Create PRD

**Create a PRD for:**
- New user-facing features
- Significant functionality additions
- Changes affecting user workflows

**Template:** `docs/prd/PRD-NNN-title.md`

**Examples:**
- PRD-004: Metadata Generation
- PRD-005: Episode Summarization

**Structure:**
```markdown
# PRD-NNN: Feature Name

## Problem
[What problem does this solve?]

## Goals
[What are we trying to achieve?]

## Non-Goals
[What are we explicitly NOT doing?]

## Solution
[Proposed solution]

## Success Criteria
[How do we know it works?]
```

### When to Create RFC

**Create an RFC for:**
- Architectural changes
- Breaking API changes
- Design decisions needing discussion
- Technical implementation approaches

**Template:** `docs/rfc/RFC-NNN-title.md`

**Examples:**
- RFC-010: Speaker Name Detection
- RFC-012: Episode Summarization

**Structure:**
```markdown
# RFC-NNN: Feature Name

## Background
[Context and motivation]

## Proposal
[Detailed technical proposal]

## Alternatives Considered
[Other approaches and why not chosen]

## Implementation
[How to build this]

## Testing
[How to test this]
```

### When to Skip PRD/RFC

**You can skip documentation for:**
- ✅ Bug fixes (just fix it)
- ✅ Small enhancements (< 100 lines)
- ✅ Internal refactoring (no API changes)
- ✅ Documentation updates
- ✅ Test improvements
- ✅ Performance optimizations (if localized)

**Rule of thumb:** If it takes longer to document than implement, skip the doc.

### Always Update

**Update these files when:**

| File | When to Update |
|------|----------------|
| `README.md` | CLI flags change, new features, installation changes |
| `docs/ARCHITECTURE.md` | Module changes, data flow changes, design decisions |
| `docs/TESTING_STRATEGY.md` | Testing approach changes, new test infrastructure |
| `docs/api/` | Public API changes (auto-generated from docstrings) |
| `CONTRIBUTING.md` | Development workflow changes, tool changes |
| `.ai-coding-guidelines.md` | Patterns change, new guidelines needed |

### Docstring Requirements

**All public functions need docstrings:**
```python
def run_pipeline(cfg: Config) -> Tuple[int, str]:
    """Execute the main podcast scraping pipeline.
    
    This orchestrates the complete workflow from RSS feed fetching
    to transcript generation and optional metadata/summarization.
    
    Args:
        cfg: Configuration object with all pipeline settings.
        
    Returns:
        Tuple[int, str]: (count, summary) where count is episodes
            processed and summary is human-readable message.
            
    Raises:
        ValueError: If RSS URL is invalid.
        RuntimeError: If output cleanup fails.
        
    Example:
        >>> cfg = Config(rss="https://example.com/feed.xml")
        >>> count, summary = run_pipeline(cfg)
        >>> print(f"Processed {count} episodes")
    """
```

**Style:** Google-style docstrings (for mkdocstrings compatibility)

---

## Common Patterns

### Configuration Pattern

**All runtime options flow through Config:**

```python
from podcast_scraper import Config

# Good - centralized configuration
cfg = Config(
    rss="https://example.com/feed.xml",
    transcribe_missing=True,
    workers=8
)
run_pipeline(cfg)

# Bad - scattered parameters
fetch_rss(url, timeout=30)
download(episodes, workers=8)
transcribe(jobs, model="base")
```

**Adding new configuration:**
1. Add field to `Config` model in `config.py`
2. Add CLI argument in `cli.py`
3. Document in README options section
4. Update `examples/config.example.json` and `examples/config.example.yaml`

### Error Handling Pattern

**Recoverable errors - log and continue:**
```python
try:
    transcript = download_transcript(url)
except requests.RequestException as e:
    logger.warning(f"Failed to download: {e}")
    return None  # Continue with other episodes
```

**Unrecoverable errors - raise with clear message:**
```python
if not cfg.rss:
    raise ValueError("RSS URL is required")

if cfg.workers < 1:
    raise ValueError(f"Workers must be >= 1, got: {cfg.workers}")
```

**Graceful degradation for optional features:**
```python
try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    logger.warning("Whisper not available")
```

### Progress Reporting Pattern

**Use progress abstraction (don't use tqdm directly):**

```python
from podcast_scraper import progress

# Good
with progress.make_progress(
    total=len(episodes),
    desc="Downloading"
) as pbar:
    for episode in episodes:
        process(episode)
        pbar.update(1)

# Bad
from tqdm import tqdm
for episode in tqdm(episodes):
    process(episode)
```

### Lazy Loading Pattern

**For optional dependencies:**

```python
# At module level
_whisper = None

def get_whisper():
    """Lazy load Whisper library."""
    global _whisper
    if _whisper is None:
        try:
            import whisper
            _whisper = whisper
        except ImportError:
            raise ImportError(
                "Whisper not installed. "
                "Install: pip install openai-whisper"
            )
    return _whisper

# Usage
def transcribe(audio_path):
    whisper = get_whisper()
    model = whisper.load_model("base")
    # ...
```

### Logging Pattern

```python
import logging

logger = logging.getLogger(__name__)

# Good - structured logging with context
logger.info(f"Processing episode {episode.number}: {episode.title}")
logger.warning(f"Failed to download transcript for {episode.number}: {error}")
logger.error(f"Pipeline failed: {error}", exc_info=True)

# Bad - print statements
print("Processing episode")
print("Error:", error)
```

---

## Security & Secrets

### NEVER Commit

**Never add to git:**
- ❌ API keys or tokens
- ❌ Passwords or credentials
- ❌ Personal data
- ❌ Real RSS feed URLs in tests (use example.com)
- ❌ Private configuration files
- ❌ `.env` files with secrets

### Security Practices

**Input validation:**
```python
# Validate user inputs
if not isinstance(cfg.workers, int) or cfg.workers < 1:
    raise ValueError(f"Invalid workers: {cfg.workers}")

# Sanitize filenames
safe_name = sanitize_filename(episode.title)

# Check paths are within expected directory
if not output_path.is_relative_to(base_dir):
    raise ValueError(f"Path outside output dir: {output_path}")
```

**XML parsing:**
```python
# Use defusedxml for safe XML parsing
from defusedxml import ElementTree as ET

tree = ET.parse(rss_path)  # Safe against XML attacks
```

**HTTP:**
```python
# Use timeouts
response = requests.get(url, timeout=30)

# Verify SSL (default, but be explicit if needed)
response = requests.get(url, verify=True)
```

---

## Git Workflow

### Commit Message Format

```
<type>: <short description>

<detailed description if needed>

Fixes #<issue-number>
```

**Types:**
- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation
- `test`: Tests
- `refactor`: Code refactoring
- `ci`: CI/CD changes
- `chore`: Maintenance
- `perf`: Performance

**Examples:**
```
feat: add PostgreSQL export adapter

Implement export functionality to generate SQL dumps from
episode metadata. Includes schema templates and CLI flags.

Fixes #40
```

```
fix: resolve double progress bar in Whisper

Remove duplicate progress indicators when transcribing.

Fixes #19
```

### Branch Naming

```bash
# Features
feature/add-postgresql-export
feature/issue-40-etl-loading

# Fixes
fix/whisper-progress-indicator
fix/issue-19-progress-bars

# Docs
docs/update-api-reference
docs/contributing-guide

# Refactoring
refactor/simplify-config-loading
```

---

## Decision Trees

### Should I Create a PRD/RFC?

```
Is it a bug fix?
  └─ No → Is it < 100 lines of code?
      └─ No → Is it user-facing?
          ├─ Yes → Create PRD
          └─ No → Is it architectural?
              ├─ Yes → Create RFC
              └─ No → Skip, just implement
```

### Should I Create a New Module?

```
Is it > 500 lines?
  └─ Yes → Does it have a single clear responsibility?
      └─ Yes → Can it be tested independently?
          └─ Yes → Create new module
          └─ No → Add to existing module
```

### Should I Mock This Dependency?

```
Is it an external service?
  ├─ Yes (HTTP, DB, API) → Mock it
  └─ No → Is it slow (> 1s)?
      ├─ Yes (Whisper, ML models) → Mock it
      └─ No → Is it file I/O?
          ├─ Yes → Use tempfile
          └─ No → Don't mock (pure functions)
```

### When to Run make ci?

```
Am I about to push?
  └─ Yes → Run make ci
      └─ Passes? → Push
          └─ No? → Fix and retry
```

---

## When to Ask

### Ask the User When

**Requirements unclear:**
- Multiple valid approaches exist
- User preference matters (performance vs readability)
- Breaking changes are needed
- Trade-offs aren't obvious

**External dependencies:**
- Need to add new dependencies
- Need to change dependency versions
- Security implications

**Design decisions:**
- Architectural changes
- API design choices
- Performance vs correctness trade-offs

**Examples:**
- "Should I use async/await or keep synchronous?"
- "This requires adding a new dependency (X). Is that okay?"
- "This is a breaking change to the API. Should I proceed?"

### Don't Ask When

**Patterns are clear:**
- Following established patterns in codebase
- Standard bug fix approach
- Documentation-only changes
- Obvious code improvements (typos, formatting)

**Best practices:**
- Adding type hints
- Adding tests for new code
- Improving error messages
- Adding docstrings

**Examples:**
- "Should I add a docstring?" → Yes, always (don't ask)
- "Should I add tests?" → Yes, always (don't ask)
- "Should I format with black?" → Yes, always (don't ask)

---

## Quick Reference

### Most Common Commands

```bash
make format          # Format code
make test            # Run tests
make ci              # Full CI suite
make docs            # Build docs
make clean           # Clean build artifacts
```

### Most Common Patterns

```python
# Configuration
from podcast_scraper import Config
cfg = Config(rss="...", ...)

# Pipeline
from podcast_scraper import run_pipeline
count, summary = run_pipeline(cfg)

# Service API
from podcast_scraper import service
result = service.run(cfg)

# Progress
from podcast_scraper import progress
with progress.make_progress(...) as pbar:
    pbar.update(1)

# Logging
import logging
logger = logging.getLogger(__name__)
logger.info("message")
```

### Most Common Imports

```python
# Standard library (group 1)
import logging
import os
from pathlib import Path
from typing import Optional, List, Dict

# Third-party (group 2)
import requests
from pydantic import BaseModel

# Local (group 3)
from podcast_scraper import config
from podcast_scraper.models import Episode
```

---

## Related Documentation

- **[CONTRIBUTING.md](CONTRIBUTING.md)** - Human contributor guide (setup, workflow, PR process)
- **[docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)** - Architecture design and module responsibilities
- **[docs/TESTING_STRATEGY.md](docs/TESTING_STRATEGY.md)** - Comprehensive testing approach
- **[docs/api/](docs/api/)** - Auto-generated API reference
- **[README.md](README.md)** - User-facing documentation

---

## Summary for AI Assistants

**Remember:**
1. ✅ Run `make format` and `make test` before committing
2. ✅ Respect module boundaries (no business logic in CLI, no HTTP in config, etc.)
3. ✅ Mock external dependencies in tests
4. ✅ Add docstrings to all public functions (Google-style)
5. ✅ Use Config for all runtime options
6. ✅ Never commit secrets or personal data
7. ✅ Create PRD for user-facing features, RFC for architecture changes
8. ✅ Update README when CLI changes
9. ✅ Follow conventional commit format
10. ✅ Ask when uncertain, don't ask for obvious best practices

**When in doubt:**
- Check existing code for patterns
- Look at similar functions for examples
- Read ARCHITECTURE.md for design principles
- Run `make ci` to validate changes

---

**Version:** 1.0 (matches v2.3.0)  
**Last Updated:** 2025-11-19
