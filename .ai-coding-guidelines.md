# AI Coding Guidelines for podcast_scraper

## ‚ö†Ô∏è MANDATORY: READ THIS FIRST ‚ö†Ô∏è

**THIS FILE IS THE PRIMARY SOURCE OF TRUTH FOR ALL AI ACTIONS IN THIS PROJECT.**

## üö® START-OF-SESSION CHECKLIST (MANDATORY - DO THIS FIRST)

**Before taking ANY action in this project, you MUST:**

1. ‚úÖ **Read this file** (at minimum, read the CRITICAL RULES section below)
2. ‚úÖ **Acknowledge you've read it** - Say "I've read the AI guidelines" or similar
3. ‚úÖ **Confirm you understand** - The user may ask "Did you read the guidelines?" - Answer honestly
4. ‚úÖ **Reference this file** for all decisions about commits, pushes, and workflows

**If the user asks "Did you read the guidelines?" or "Check the guidelines first":**

- ‚úÖ **STOP what you're doing**
- ‚úÖ **Read `.ai-coding-guidelines.md` immediately**
- ‚úÖ **Acknowledge what you read**
- ‚úÖ **Then proceed with the task**

**Before taking ANY action, you MUST:**

1. ‚úÖ Read this entire file (especially CRITICAL sections)
2. ‚úÖ Follow ALL rules marked as CRITICAL
3. ‚úÖ Reference this file for all decisions
4. ‚úÖ Check this file when unsure about patterns or workflows

**CRITICAL RULES (MUST FOLLOW ALWAYS - NO EXCEPTIONS):**

**üö® COMMIT WORKFLOW - MANDATORY CHECKLIST (NO EXCEPTIONS):**

1. ‚ùå **NEVER commit without first showing `git status`**
2. ‚ùå **NEVER commit without first showing `git diff`**
3. ‚ùå **NEVER commit without explicit user approval**
4. ‚ùå **NEVER commit when user said "don't commit" or "wait"**
5. ‚úÖ **ALWAYS show changes BEFORE asking to commit**
6. ‚úÖ **ALWAYS wait for explicit approval (user says "commit", "yes", "go ahead", etc.)**
7. ‚úÖ **ALWAYS get commit message from user OR ask for one**

**üö® PR PUSH WORKFLOW - MANDATORY CHECKLIST (NO EXCEPTIONS):**

**CRITICAL: USER APPROVAL REQUIRED BEFORE EVERY PUSH**

1. ‚ùå **NEVER push to PR without explicit user approval**
2. ‚ùå **NEVER push without showing `git status` and `git diff` first**
3. ‚ùå **NEVER push when user said "don't push" or "wait"**
4. ‚ùå **NEVER push to PR without running `make ci` first**
5. ‚ùå **NEVER push when `make ci` has failures**
6. ‚úÖ **ALWAYS show `git status` before asking to push**
7. ‚úÖ **ALWAYS show `git diff` or summary of changes before asking to push**
8. ‚úÖ **ALWAYS run `make ci` before pushing to PR (new or updated)**
9. ‚úÖ **ALWAYS fix all CI failures before pushing**
10. ‚úÖ **ALWAYS wait for explicit approval (user says "push", "go ahead", "yes", etc.)**
11. ‚úÖ **ONLY push after steps 6-10 are complete AND user has approved**

**Purpose:** This document provides comprehensive context and guidelines for AI coding assistants (Cursor, GitHub Copilot, etc.) working on this project. Think of this as a project-specific CLAUDE.md - instructions for any AI agentic editor.

**For human contributors**, see [CONTRIBUTING.md](CONTRIBUTING.md) instead.

> **üìö For detailed technical information**, see [`docs/DEVELOPMENT_NOTES.md`](docs/DEVELOPMENT_NOTES.md) which covers:
>
> - Code style guidelines and formatting details
> - Testing requirements and test structure details
> - CI/CD integration details
> - Architecture principles
> - Logging guidelines
> - Documentation standards
> - Markdown linting
> - Environment setup details
>
> This file (`.ai-coding-guidelines.md`) focuses on **AI-specific workflow** and **critical rules** for AI assistants. For detailed technical patterns and implementation details, refer to `docs/DEVELOPMENT_NOTES.md`.

---

## üéØ How Cursor Uses This File

**Cursor automatically reads this file** when you're working in this project. It provides context to AI features like:

- **Composer** (AI code generation)
- **Chat** (AI assistant)
- **Inline edits** (AI-powered code suggestions)

**To maximize effectiveness:**

1. ‚úÖ Keep this file updated when patterns change
2. ‚úÖ Reference specific sections when asking Cursor to implement features
3. ‚úÖ Use clear, actionable language (Cursor reads this as instructions)
4. ‚úÖ Include code examples for common patterns

**Cursor-specific optimizations:**

- Sections are structured for easy semantic search
- Code examples are copy-paste ready
- Decision trees help AI make autonomous choices
- "When to Ask" section prevents unnecessary interruptions

---

## Table of Contents

1. [Project Context](#project-context)
2. [When to Run What](#when-to-run-what)
3. [Code Organization](#code-organization)
4. [Package Import Patterns](#package-import-patterns)
5. [Testing Requirements](#testing-requirements)
6. [Documentation Standards](#documentation-standards)
7. [Common Patterns](#common-patterns)
8. [AI Experiment Pipeline](#ai-experiment-pipeline)
9. [Prompt Management](#prompt-management)
10. [OpenAI Provider Integration](#openai-provider-integration)
11. [Environment Variables & Secrets](#environment-variables--secrets)
12. [Security & Secrets](#security--secrets)
13. [Git Workflow](#git-workflow)
14. [Decision Trees](#decision-trees)
15. [When to Ask](#when-to-ask)

---

## Project Context

### What This Project Does

**podcast_scraper** is a Python tool for downloading podcast transcripts from RSS feeds with optional Whisper transcription fallback.

**Key capabilities:**

- Download published transcripts from RSS feeds (Podcasting 2.0 namespace)
- Fallback to Whisper transcription for episodes without transcripts
- Automatic speaker detection using spaCy NER
- Episode summarization using local transformer models (BART, LED) or OpenAI API
- Metadata generation (JSON/YAML)
- Multi-threaded downloads
- Screenplay formatting
- Service/daemon mode for automated runs
- **AI Experiment Pipeline** (configuration-driven model evaluation)
- **Prompt Management** (versioned, parameterized prompts)

**Primary use case:** Personal, non-commercial transcript archival for research and study.

### Architecture Overview

**Design:** Modular architecture with clear separation of concerns

**13 core modules:**

1. `cli.py` - Interactive command-line interface
2. `service.py` - Non-interactive service API for daemons
3. `workflow.py` - Pipeline orchestration
4. `config.py` - Configuration model (Pydantic)
5. `rss_parser.py` - RSS feed parsing
6. `downloader.py` - HTTP operations with retry logic
7. `episode_processor.py` - Episode-level processing
8. `filesystem.py` - File system utilities
9. `whisper_integration.py` - Whisper transcription interface
10. `speaker_detection.py` - NER-based speaker extraction
11. `summarizer.py` - Transcript summarization
12. `metadata.py` - Metadata document generation
13. `progress.py` - Progress reporting abstraction

**Future modules (planned):**

- `prompt_store.py` - Prompt management and loading (RFC-017)
- `experiment_config.py` - Experiment configuration models (RFC-015)
- Provider modules (OpenAI, etc.) - Protocol-based provider system (RFC-016)

**Data flow:**

```text
RSS Feed ‚Üí Parse Episodes ‚Üí Download/Transcribe ‚Üí Detect Speakers ‚Üí
Generate Metadata ‚Üí Summarize ‚Üí Write Files
```

- Python 3.10+ (type hints, Pydantic v2)
- Pydantic for configuration validation
- Click for CLI (optional dependency)
- defusedxml for safe XML parsing
- `python-dotenv` for environment variable management

**ML/AI:**

- OpenAI Whisper for transcription
- spaCy for NER (speaker detection)
- PyTorch + transformers for summarization (BART, PEGASUS, LED)
- OpenAI API (planned) for optional providers
- Jinja2 for prompt templating (RFC-017)

**Testing:**

- pytest with unittest.mock
- pytest markers: `@pytest.mark.slow`, `@pytest.mark.integration`
- Coverage target: >80%

**Documentation:**

- MkDocs with Material theme
- mkdocstrings for API docs (auto-generated from docstrings)
- Mermaid for diagrams

**Code Quality:**

- black (formatting, line length: 100)
- isort (import sorting)
- flake8 (linting)
- mypy (type checking)
- bandit (security scanning)

### Project Structure

```text
podcast_scraper/
‚îú‚îÄ‚îÄ __init__.py              # Public API exports (lazy loading)
‚îú‚îÄ‚îÄ cli.py                   # CLI interface
‚îú‚îÄ‚îÄ service.py               # Service API
‚îú‚îÄ‚îÄ workflow.py              # Pipeline orchestration
‚îú‚îÄ‚îÄ config.py                # Configuration model
‚îú‚îÄ‚îÄ [10 more modules]        # Core functionality
‚îú‚îÄ‚îÄ tests/                   # Test suite
‚îú‚îÄ‚îÄ docs/                    # MkDocs documentation
‚îÇ   ‚îú‚îÄ‚îÄ rfc/                 # Technical RFCs
‚îÇ   ‚îú‚îÄ‚îÄ prd/                 # Product requirements
‚îÇ   ‚îî‚îÄ‚îÄ wip/                  # Work in progress docs
‚îú‚îÄ‚îÄ examples/                # Config examples
‚îú‚îÄ‚îÄ scripts/                 # Utility scripts (eval, fix_markdown, etc.)
‚îú‚îÄ‚îÄ prompts/                 # Prompt templates (planned, RFC-017)
‚îú‚îÄ‚îÄ Makefile                 # Development commands
‚îú‚îÄ‚îÄ pyproject.toml           # Package metadata
‚îú‚îÄ‚îÄ pyproject.toml          # Package metadata and dependencies
‚îî‚îÄ‚îÄ CONTRIBUTING.md          # Contributor guide
```

### During Development (Local)

**One-time setup (RECOMMENDED):**

```bash
make install-hooks    # Install pre-commit hook for automatic checks
```

Hook runs automatically on every commit:

```bash
git commit -m "your changes"
# ‚Üí Checks: black, isort, flake8, markdownlint, mypy
# ‚Üí Blocks commit if checks fail
# ‚Üí Auto-fix with: make format
```

Before pushing:

```bash
make ci          # Full CI suite (all checks + tests + docs + build)
```

Before every commit:

```bash
make format      # Auto-format with black/isort
make test        # Run tests with coverage
```

Before every push:

```bash
make ci          # Full CI suite
```

- `make lint` - After adding new functions (flake8)
- `make type` - After changing type hints (mypy)
- `make security` - After changing dependencies (bandit, pip-audit)
- `make docs` - After updating docstrings or docs/
- `make lint-markdown` - After editing markdown files

### Utility Scripts

**Available utility scripts in `scripts/` directory:**

- **`fix_markdown.py`** - Automatically fixes common markdown linting issues
  - **When to use:** Before committing markdown files, after bulk documentation edits, when CI fails on markdown linting
  - **Usage:**

    ```bash
    # Fix all markdown files
    python scripts/fix_markdown.py

    # Fix specific files
    python scripts/fix_markdown.py docs/TESTING_STRATEGY.md

    # Dry run (see what would be fixed)
    python scripts/fix_markdown.py --dry-run
    ```

  - **What it fixes:** Table separators (MD060), trailing spaces (MD009), blank lines around lists (MD032), code block languages (MD040)
  - **See:** `scripts/README.md` for full documentation

- **`check_unit_test_imports.py`** - Verifies unit tests can import without ML dependencies
- **`eval_cleaning.py`** - Evaluates transcript cleaning quality
- **`eval_summaries.py`** - Evaluates summarization quality using ROUGE metrics

**Best practice:** Run `python scripts/fix_markdown.py` before committing markdown files to catch common issues early.

### What CI Runs Automatically

> **See also:** [`docs/CI_CD.md`](docs/CI_CD.md) for complete CI/CD pipeline documentation with visualizations.

**On every push to PR (5 parallel workflows):**

1. **Python App Workflow** (Lint Job, 2-3 min):
   - Black/isort formatting checks
   - flake8 linting
   - markdownlint
   - mypy type checking
   - bandit + pip-audit security

2. **Python App Workflow** (Test Job, 10-15 min):
   - Full pytest suite with coverage

3. **Docs Workflow** (3-5 min):
   - mkdocs build --strict
   - API docs generation

4. **Docker Workflow** (on Dockerfile/project structure changes):
   - **When to test:** Only when Dockerfile, pyproject.toml structure, or package layout changes
   - **How to test:** Run `make docker-test` locally before pushing to PR
   - **What it does:** Builds Docker image and runs basic validation
   - **Why:** Catches Docker build failures before CI runs (Docker builds are slow in CI)

5. **Snyk Workflow** (weekly + on push):
   - Security vulnerability scanning

**Additional workflows:**

- **CodeQL Security** (weekly + on push): Python & Actions security scanning
- **Docs Deployment** (on main): Build ‚Üí Deploy to GitHub Pages

**When CI fails:**

1. Read the error message carefully
2. Run `make ci` locally to reproduce
3. Fix the issue (use `make format` for formatting issues)
4. Test locally again
5. Push the fix

**Prevent CI failures:**

```bash
make install-hooks    # Catch issues before commit!
```

- ‚ùå Long-running integration tests (use `pytest -m "not slow"`)
- ‚ùå Full model downloads for every test
- ‚ùå Docker builds (unless working on Docker specifically OR before pushing Docker-related changes to PR)
- ‚ùå Deployment commands (GitHub Actions handles this)

**Use CI for:**

- Full test suite on every platform
- Documentation deployment
- Release builds

---

## Code Organization

### When to Create New Files

**Create a new module when:**

- Implementing a new major feature (>500 lines)
- Feature has distinct responsibility (SRP)
- Feature can be tested independently
- Example: Adding PostgreSQL export ‚Üí create `export.py`

**Modify existing files when:**

- Fixing bugs in existing functionality
- Enhancing existing features (< 200 lines added)
- Refactoring within same module
- Example: Improving Whisper model selection ‚Üí modify `whisper_integration.py`

**Don't create:**

- Helper modules with only 1-2 functions (add to appropriate existing module)
- Duplicate functionality (search codebase first)
- Temporary files (use proper temporary directories)

### Module Boundaries (STRICT)

**Respect these boundaries** - don't mix concerns:

| Module | Responsibility | What to AVOID |
| ------ | --------------- | ------------- |
| `cli.py` | CLI interface only | Business logic, HTTP calls, file I/O |
| `service.py` | Service API, structured results | CLI interaction, user prompts |
| `workflow.py` | Orchestration only | HTTP details, file parsing, direct I/O |
| `config.py` | Configuration model | Business logic, side effects |
| `downloader.py` | HTTP operations only | Parsing, business logic |
| `filesystem.py` | File system utilities | HTTP, parsing, business logic |
| `rss_parser.py` | RSS parsing, Episode creation | HTTP, file I/O, workflow |
| `episode_processor.py` | Episode-level processing | CLI interaction, orchestration |
| `whisper_integration.py` | Whisper interface | HTTP, RSS parsing |
| `speaker_detection.py` | NER extraction | HTTP, file I/O |
| `summarizer.py` | Summarization | Episode processing, HTTP |
| `metadata.py` | Metadata generation | HTTP, RSS parsing |
| `progress.py` | Progress abstraction | Business logic |

**If you're about to:**

- Add HTTP calls to `cli.py` ‚Üí ‚ùå Use `downloader.py` instead
- Add business logic to `config.py` ‚Üí ‚ùå Use `workflow.py` or appropriate module
- Add CLI prompts to `service.py` ‚Üí ‚ùå Service is non-interactive

### File Naming Conventions

**Python modules:**

- Use `snake_case.py`
- Be descriptive: `speaker_detection.py` not `speakers.py`
- Avoid generic names: `utils.py`, `helpers.py`

**Test files:**

- Mirror structure: `test_<module>.py`
- Example: `test_speaker_detection.py` tests `speaker_detection.py`

**Documentation:**

- Use `UPPER_CASE.md` for top-level docs: `README.md`, `CONTRIBUTING.md`
- Use descriptive names for guides: `TESTING_STRATEGY.md`
- PRD/RFC format: `PRD-NNN-title.md`, `RFC-NNN-title.md`

---

## Package Import Patterns

### Lazy Loading Pattern (CRITICAL)

**The `__init__.py` uses lazy loading** to prevent circular imports:

```python
# __init__.py uses __getattr__ for lazy loading
from podcast_scraper import cli, service  # ‚úÖ Works (lazy loaded)
import podcast_scraper.cli as cli         # ‚úÖ Also works
```

- `cli.py` and `service.py` are loaded on-demand via `__getattr__`
- Prevents circular import issues
- Maintains backward compatibility

**When adding new lazy-loaded modules:**

1. Add to `__getattr__` in `__init__.py`
2. Add to `_import_cache` for caching
3. Update `__all__` if needed (but note that lazy-loaded modules aren't in `__all__`)

**Example from `__init__.py`:**

```python
_import_cache: dict[str, object] = {}

def __getattr__(name: str):
    if name in _import_cache:
        return _import_cache[name]

    if name == "cli":
        import importlib
        _cli = importlib.import_module(f"{__name__}.cli")
        _import_cache[name] = _cli
        return _cli
    # ... similar for service
    raise AttributeError(f"module {__name__!r} has no attribute {name!r}")
```python
import os
from pathlib import Path
from typing import Optional, List, Dict

# Third-party (group 2)
import requests
from pydantic import BaseModel

# Local (group 3)
from podcast_scraper import config
from podcast_scraper.models import Episode
```

```python
        requests.ConnectionError("Network error"),
        MockHTTPResponse(content="Success", status_code=200)
    ]
```

```python
    result = fetch_url("https://example.com")
    self.assertEqual(result, "Success")
```

```python
    """Happy path."""
    pass

def test_sanitize_filename_invalid_chars(self):
    """Error case."""
    pass
```python
    pass

# Bad
def test_config(self):
    pass
```

- HTTP requests (`requests` module)
- Whisper models (`whisper.load_model()`, `whisper.transcribe()`)
- File I/O (use `tempfile.TemporaryDirectory`)
- spaCy models (mock NER extraction)
- Time-based operations (`time.sleep()`)
- OpenAI API calls (when implementing OpenAI providers)

**Example:**

```python
@patch("podcast_scraper.whisper_integration.whisper")
def test_transcription(self, mock_whisper):
    mock_whisper.load_model.return_value = Mock()
    mock_whisper.transcribe.return_value = {"text": "Test"}
    # ... test code ...
```

- New user-facing features
- Significant functionality additions
- Changes affecting user workflows

**Template:** `docs/prd/PRD-NNN-title.md`

**Examples:**

- PRD-004: Metadata Generation
- PRD-005: Episode Summarization
- PRD-006: OpenAI Provider Integration
- PRD-007: AI Experiment Pipeline

**Structure:**

```markdown
# PRD-NNN: Feature Name

## Problem
[What problem does this solve?]

## Goals
[What are we trying to achieve?]

## Non-Goals
[What are we explicitly NOT doing?]

## Solution
[Proposed solution]

## Success Criteria
[How do we know it works?]
```

- Architectural changes
- Breaking API changes
- Design decisions needing discussion
- Technical implementation approaches

**Template:** `docs/rfc/RFC-NNN-title.md`

**Examples:**

- RFC-010: Speaker Name Detection
- RFC-012: Episode Summarization
- RFC-013: OpenAI Provider Implementation
- RFC-015: AI Experiment Pipeline
- RFC-016: Modularization for AI Experiments
- RFC-017: Prompt Management

**Structure:**

```markdown
# RFC-NNN: Feature Name

## Background
[Context and motivation]

## Proposal
[Detailed technical proposal]

## Alternatives Considered
[Other approaches and why not chosen]

## Implementation
[How to build this]

## Testing
[How to test this]
```

- ‚úÖ Bug fixes (just fix it)
- ‚úÖ Small enhancements (< 100 lines)
- ‚úÖ Internal refactoring (no API changes)
- ‚úÖ Documentation updates
- ‚úÖ Test improvements
- ‚úÖ Performance optimizations (if localized)

**Rule of thumb:** If it takes longer to document than implement, skip the doc.

### Always Update

**Update these files when:**

| File | When to Update |
| ---- | -------------- |
| `README.md` | CLI flags change, new features, installation changes |
| `docs/ARCHITECTURE.md` | Module changes, data flow changes, design decisions |
| `docs/TESTING_STRATEGY.md` | Testing approach changes, new test infrastructure |
| `docs/api/` | Public API changes (auto-generated from docstrings) |

### ‚ö†Ô∏è CRITICAL: Check `mkdocs.yml` and Links Before Pushing Documentation Changes

**MANDATORY CHECKLIST when adding, moving, or deleting documentation files:**

- [ ] **New files added?** ‚Üí Add to `nav` configuration in `mkdocs.yml`
- [ ] **Files moved?** ‚Üí Update path in `nav` configuration
- [ ] **Files deleted?** ‚Üí Remove from `nav` configuration
- [ ] **Links updated?** ‚Üí Use relative paths (e.g., `rfc/RFC-019.md` not `docs/rfc/RFC-019.md`)
- [ ] **All links verified?** ‚Üí Check that all internal links point to existing files
- [ ] **No broken links?** ‚Üí Run `make docs` to catch broken links before CI
- [ ] **Test locally?** ‚Üí Run `make docs` to verify build succeeds

**Why this matters:**

- Missing files in `nav` ‚Üí CI will fail with warnings
- Broken links ‚Üí CI will fail with errors (~3-5 min wasted per build)
- Wrong path format ‚Üí Links won't work in generated docs
- **Fixing locally takes seconds vs. waiting for CI to fail**

**Always check `mkdocs.yml` and verify all links before committing documentation changes!**
| `CONTRIBUTING.md` | Development workflow changes, tool changes |
| `.ai-coding-guidelines.md` | Patterns change, new guidelines needed |
| `docs/index.md`, `docs/rfc/index.md`, `docs/prd/index.md` | New RFCs/PRDs added |

### Release Notes - Historical Accuracy

**üö® CRITICAL: Do NOT modify historical release notes**

**Rule:** Never update release notes in `docs/releases/RELEASE_v*.md` for software versions that have already been shipped, unless the user explicitly asks you to do so.

**Why:** Release notes are historical documents that reflect what was actually shipped at that time. Modifying them retroactively would:

- Misrepresent what was actually in those releases
- Confuse users who reference historical documentation
- Break the historical record

**What this means:**

- ‚úÖ You CAN update current/upcoming release notes (draft releases)
- ‚ùå You CANNOT update past release notes (already shipped versions)
- ‚úÖ You CAN update other documentation (README, guides, etc.) to reflect current state
- ‚ùå You CANNOT "fix" historical release notes to match current codebase

**Examples:**

- ‚ùå Don't update `RELEASE_v2.3.1.md` to remove `requirements.txt` references (it was there when v2.3.1 shipped)
- ‚úÖ You can update `README.md` to remove `requirements.txt` references (current documentation)
- ‚ùå Don't change dependency lists in old release notes
- ‚úÖ You can update current documentation to reflect new dependency management

**Exception:** Only modify historical release notes if the user explicitly requests it (e.g., "update RELEASE_v2.3.1.md to fix a typo").

### Docstring Requirements

**All public functions need docstrings:**

```python
def run_pipeline(cfg: Config) -> Tuple[int, str]:
    """Execute the main podcast scraping pipeline.

    This orchestrates the complete workflow from RSS feed fetching
    to transcript generation and optional metadata/summarization.

    Args:
        cfg: Configuration object with all pipeline settings.

    Returns:
        Tuple[int, str]: (count, summary) where count is episodes
            processed and summary is human-readable message.

    Raises:
        ValueError: If RSS URL is invalid.
        RuntimeError: If output cleanup fails.

    Example:
        >>> cfg = Config(rss="https://example.com/feed.xml")
        >>> count, summary = run_pipeline(cfg)
        >>> print(f"Processed {count} episodes")
    """
```

run_pipeline(cfg)

```python
# Bad - scattered parameters
fetch_rss(url, timeout=30)
download(episodes, workers=8)
transcribe(jobs, model="base")
```

1. Add field to `Config` model in `config.py`
2. Add CLI argument in `cli.py`
3. Document in README options section
4. Update `examples/config.example.json` and `examples/config.example.yaml`

## Error Handling Pattern

**Recoverable errors - log and continue:**

```python
try:
    transcript = download_transcript(url)
except requests.RequestException as e:
    logger.warning(f"Failed to download: {e}")
    return None  # Continue with other episodes
```

for episode in tqdm(episodes):
    process(episode)

```python
    """Lazy load Whisper library."""
    global _whisper
    if _whisper is None:
        try:
            import whisper
            _whisper = whisper
        except ImportError:
            raise ImportError(
                "Whisper not installed. "
                "Install: pip install openai-whisper"
            )
    return _whisper

# Usage
def transcribe(audio_path):
    whisper = get_whisper()
    model = whisper.load_model("base")
    # ...
```

```python
# Good - structured logging with context

logger.info(f"Processing episode {episode.number}: {episode.title}")
logger.warning(f"Failed to download transcript for {episode.number}: {error}")
logger.error(f"Pipeline failed: {error}", exc_info=True)

# Bad - print statements

```python
print("Processing episode")
print("Error:", error)
```

- **Configuration-Driven**: Experiments defined in YAML configs
- **Separation of Generation from Scoring**: Generate predictions first, compute metrics separately
- **Two-Layer CI/CD**: Fast smoke tests (CI) + full evaluation pipeline (nightly/on-demand)
- **Test Pipeline Analogy**: Treat model evaluation like unit/integration tests

## Experiment Structure

**Example experiment config:**

```yaml
# experiments/summarization_openai_long_v1.yaml
id: "summarization_openai_long_v1"
task: "summarization"

backend:
  type: "openai"
  model: "gpt-4o-mini"

prompts:
  system: "summarization/system_v1"
  user: "summarization/long_v1"
  params:
    paragraphs_min: 3
    paragraphs_max: 6

data:
  episodes_glob: "data/eval/episodes/ep*/transcript.txt"
  id_from: "parent_dir"

params:
  max_output_tokens: 900
```

- Move gold data to `data/eval/episodes/`
- Establish baseline results

**Phase 2: Generic Runner**

- `scripts/run_experiment.py` - Load config, call backend, save predictions
- `podcast_scraper/experiment_config.py` - Pydantic models for configs

**Phase 3: CI Smoke Tests (Layer A)**

- Fast tests on subset of episodes
- Assert basic metrics (ROUGE-L > threshold)

**Phase 4: Full Eval Pipeline (Layer B)**

- Nightly/on-demand full evaluation
- Run all experiment configs
- Generate comparison reports

**Phase 5: Comparison Tooling**

- Excel workbook generation
- Side-by-side metric comparison

**See:** `docs/rfc/RFC-015-ai-experiment-pipeline.md` and `docs/prd/PRD-007-ai-experiment-pipeline.md`

---

## Prompt Management

### Overview

**Prompt Management** (RFC-017) treats prompts as first-class, versioned assets with optional templating.

**Key Principles:**

- Prompts are **provider-specific concerns** (not part of core protocol)
- File-based organization with explicit versioning
- Jinja2 templating for parameterization
- SHA256 hashing for reproducibility

### Prompt Directory Structure

```text
prompts/
  summarization/
    system_v1.j2
    long_v1.j2
    long_v2_more_narrative.j2
  ner/
    system_ner_v1.j2
    guest_host_v1.j2
```

- Aim for {{ paragraphs_min }}‚Äì{{ paragraphs_max }} paragraphs.
- Focus on key decisions, arguments, and lessons.
- Ignore sponsorships, ads, and housekeeping.
- Do not use quotes or speaker names.
- Do not invent information not implied by the transcript.

```markdown
### Using Prompts

**In application code (OpenAI providers):**

```python
from podcast_scraper.prompt_store import render_prompt, get_prompt_metadata

system_prompt = render_prompt("summarization/system_v1")
user_prompt = render_prompt(
    "summarization/long_v1",
    paragraphs_min=3,
    paragraphs_max=6
)

# Track prompt metadata for reproducibility
metadata = get_prompt_metadata("summarization/long_v1", {"paragraphs_min": 3})
```

- **Protocol-Based**: Providers implement protocols (RFC-016)
- **Backward Compatible**: Default providers (local) unchanged
- **Per-Capability Selection**: Can mix local and OpenAI providers
- **Secure API Key Management**: Environment variables, never in code

### Provider Architecture

**After modularization:**

```text
podcast_scraper/
‚îú‚îÄ‚îÄ speaker_detectors/
‚îÇ   ‚îú‚îÄ‚îÄ base.py              # SpeakerDetector protocol
‚îÇ   ‚îú‚îÄ‚îÄ factory.py           # Factory (selects provider)
‚îÇ   ‚îú‚îÄ‚îÄ ner_detector.py      # Local NER provider (existing)
‚îÇ   ‚îî‚îÄ‚îÄ openai_detector.py   # NEW: OpenAI provider
‚îú‚îÄ‚îÄ transcription/
‚îÇ   ‚îú‚îÄ‚îÄ base.py              # TranscriptionProvider protocol
‚îÇ   ‚îú‚îÄ‚îÄ factory.py           # Factory
‚îÇ   ‚îú‚îÄ‚îÄ whisper_provider.py  # Local Whisper provider
‚îÇ   ‚îî‚îÄ‚îÄ openai_provider.py   # NEW: OpenAI Whisper API
‚îú‚îÄ‚îÄ summarization/
‚îÇ   ‚îú‚îÄ‚îÄ base.py              # SummarizationProvider protocol
‚îÇ   ‚îú‚îÄ‚îÄ factory.py           # Factory
‚îÇ   ‚îú‚îÄ‚îÄ local_provider.py    # Local transformers provider
‚îÇ   ‚îî‚îÄ‚îÄ openai_provider.py   # NEW: OpenAI GPT provider
```

```python
# Load .env file automatically (if present)
load_dotenv()

class Config(BaseModel):
    # ... other fields ...

    openai_api_key: Optional[str] = Field(
        default=None,
        description="OpenAI API key (or set OPENAI_API_KEY env var)"
    )

    @validator("openai_api_key", pre=True, always=True)
    def resolve_openai_api_key(cls, v):
        """Resolve API key from config field, env var, or .env file."""
        # Priority: config field > env var > .env file (already loaded)
        return v or os.getenv("OPENAI_API_KEY")

```

1. Config file field (`openai_api_key`)
2. System environment variable (`OPENAI_API_KEY`)
3. `.env` file (loaded by `python-dotenv`)

**Security:**

- ‚úÖ `.env` files are in `.gitignore`
- ‚úÖ Never commit `.env` files
- ‚úÖ Document required env vars in `README.md` and `CONTRIBUTING.md`
- ‚úÖ Use `examples/.env.example` as template

**See:** `docs/rfc/RFC-013-openai-provider-implementation.md` section "API Key Management"

---

## Security & Secrets

### NEVER Commit

**Never add to git:**

- ‚ùå API keys or tokens
- ‚ùå Passwords or credentials
- ‚ùå Personal data
- ‚ùå Real RSS feed URLs in tests (use example.com)
- ‚ùå Private configuration files
- ‚ùå `.env` files with secrets

### Security Practices

**Input validation:**

```python
# Validate user inputs
if not isinstance(cfg.workers, int) or cfg.workers < 1:
    raise ValueError(f"Invalid workers: {cfg.workers}")

# Sanitize filenames
safe_name = sanitize_filename(episode.title)

# Check paths are within expected directory
if not output_path.is_relative_to(base_dir):
    raise ValueError(f"Path outside output dir: {output_path}")
```

- Pre-commit hooks catch linting, formatting, and quality issues before they reach CI
- Bypassing hooks allows broken code to be committed
- Fixing issues locally is faster than waiting for CI to fail
- Respects the project's code quality standards

**What to do instead:**

1. ‚úÖ **Fix linting issues** - Run `make format` or fix issues manually
2. ‚úÖ **Fix markdown issues** - Run `python scripts/fix_markdown.py` for markdown files
3. ‚úÖ **Run checks locally** - Verify `make ci` passes before committing
4. ‚úÖ **Only use `--no-verify`** if the user explicitly says "use --no-verify" or "skip hooks"

**Exception:** Only bypass hooks if the user explicitly requests it (e.g., "commit with --no-verify" or "skip the hooks"). Even then, explain why it's not recommended.

### CRITICAL: Always Show Changes and Get Approval Before Committing

**üö® MANDATORY CHECKLIST - NO EXCEPTIONS - FOLLOW THIS EXACT SEQUENCE:**

**NEVER automatically commit changes without user approval.**

**Required workflow (MUST FOLLOW IN ORDER):**

1. ‚úÖ Make code changes
2. ‚úÖ **If editing markdown files:** Run `python scripts/fix_markdown.py` to fix common linting issues
3. ‚úÖ **MANDATORY: Run `git status`** - Show user what files changed
4. ‚úÖ **MANDATORY: Run `git diff`** - Show user the actual changes (for each modified file)
5. ‚úÖ **MANDATORY: Wait for explicit user approval** - User must explicitly say "commit", "yes", "go ahead", etc.
6. ‚úÖ **MANDATORY: Get commit message** - User provides message OR you ask "What commit message should I use?"
7. ‚úÖ **THEN commit** - Only after steps 3-6 are complete

**‚ö†Ô∏è CRITICAL: Steps 2-5 are MANDATORY. Do NOT skip any step. Do NOT assume approval.**

**What to show:**

- List of changed files (`git status`)
- Diff of changes (`git diff` or `git diff <file>`)
- Summary of what was changed

**When user approves:**

- User explicitly says "commit", "yes", "go ahead", etc.
- User provides commit message
- User reviews diffs and approves

**NEVER commit when:**

- ‚ùå User hasn't explicitly approved
- ‚ùå User hasn't reviewed changes
- ‚ùå User said "don't commit" or "wait"
- ‚ùå Changes are incomplete or untested

**Example workflow:**

```text
AI: I've fixed the issue. Here are the changes:

    - Modified: config.py (removed duplicate validator)
    - Modified: Makefile (removed || true from security-audit)

    [Shows git diff]

    Should I commit these changes?

User: Yes, commit with message "fix: remove duplicate validator and fix security audit"

AI: [Commits with user's message]
```

1. ‚úÖ Run `make ci` locally to verify all checks pass
2. ‚úÖ **Conditionally run `make docker-test`** if Docker-related changes exist (see below)
3. ‚úÖ Fix any failures before pushing
4. ‚úÖ Only push after `make ci` (and `make docker-test` if applicable) passes completely

**What `make ci` checks:**

- Formatting (black, isort)
- Linting (flake8, markdownlint)
- Type checking (mypy)
- Security (bandit, pip-audit)
- Tests (pytest with coverage)
- Documentation build (mkdocs)
- Package build (source + wheel)

**When to run `make docker-test` (CONDITIONAL):**

**Run `make docker-test` BEFORE pushing to PR ONLY if you made significant changes to:**

- ‚úÖ **Dockerfile** (`docker/Dockerfile`) - Any changes to build process, dependencies, or structure
- ‚úÖ **Project structure** - Changes to package layout, subpackages, or file organization that could affect Docker build
- ‚úÖ **Dependencies** - Changes to `pyproject.toml` that affect Docker installation (core dependencies, package metadata, subpackages)
- ‚úÖ **Package configuration** - Changes to `[tool.setuptools]` in `pyproject.toml` (packages list, package-dir, etc.)

**When NOT to run `make docker-test`:**

- ‚ùå Only Python code changes (no Dockerfile or structure changes)
- ‚ùå Only documentation changes
- ‚ùå Only test file changes
- ‚ùå Only configuration file changes (that don't affect Docker)

**Why this matters:**

- Prevents CI failures that waste time
- Catches issues locally before they reach GitHub
- Ensures PR is ready for review
- Maintains code quality standards
- Docker builds are slow - only run when necessary

**Workflow:**

```bash
# After making changes and committing:

# Step 1: Show user what will be pushed
git status
git diff --stat  # or git log --oneline origin/branch-name..HEAD

# Step 2: Always run full CI
make ci

# Step 3: Conditionally run Docker tests (if Docker-related changes)
# Check if you modified Dockerfile, pyproject.toml structure, or package layout
if [ -n "$(git diff --name-only HEAD | grep -E '(Dockerfile|pyproject\.toml|package structure)')" ]; then
    make docker-test
fi

# Step 4: ASK USER FOR APPROVAL
# "I've made the following changes: [summary]
#  CI checks passed. Should I push these changes to the PR?"

# Step 5: WAIT for user to explicitly approve (user says "yes", "push", "go ahead", etc.)

# Step 6: ONLY THEN push (after user approval)
git push origin feature/branch-name

# If CI fails ‚Üí Fix issues ‚Üí Run make ci (and docker-test) again ‚Üí Ask for approval again
```

- ‚ùå **User hasn't explicitly approved the push** (MOST IMPORTANT - CHECK THIS FIRST)
- ‚ùå **You haven't shown `git status` first**
- ‚ùå **You haven't shown `git diff` or change summary first**
- ‚ùå **User said "don't push" or "wait"**
- ‚ùå `make ci` hasn't been run
- ‚ùå `make ci` has failures
- ‚ùå `make docker-test` hasn't been run (when Docker-related changes exist)
- ‚ùå `make docker-test` has failures (when Docker-related changes exist)
- ‚ùå Tests are failing
- ‚ùå Linting errors exist
- ‚ùå Documentation build fails

**CORRECT PUSH WORKFLOW:**

```bash
# Step 1: Show what will be pushed
git status
git diff origin/feature/branch-name..HEAD  # or git log --oneline

# Step 2: Run CI checks
make ci
# (and make docker-test if applicable)

# Step 3: ASK USER FOR APPROVAL
# "I've resolved the conflicts and fixed linting errors.
#  Changes include: [summary]
#  CI checks passed. Should I push these changes to the PR?"

# Step 4: WAIT for user to say "yes", "push", "go ahead", etc.

# Step 5: ONLY THEN push
git push origin feature/branch-name
```

- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation
- `test`: Tests
- `refactor`: Code refactoring
- `ci`: CI/CD changes
- `chore`: Maintenance
- `perf`: Performance

**Examples:**

```text
feat: add PostgreSQL export adapter

Implement export functionality to generate SQL dumps from
episode metadata. Includes schema templates and CLI flags.

Fixes #40
```

fix/issue-19-progress-bars

```text
# Docs

docs/update-api-reference
docs/contributing-guide

# Refactoring

refactor/simplify-config-loading
```

```text
              ‚îú‚îÄ Yes ‚Üí Create RFC
              ‚îî‚îÄ No ‚Üí Skip, just implement
```

```text
          ‚îî‚îÄ No ‚Üí Don't mock (pure functions)
          ‚îî‚îÄ No? ‚Üí Fix and retry
```

- Multiple valid approaches exist
- User preference matters (performance vs readability)
- Breaking changes are needed
- Trade-offs aren't obvious

**External dependencies:**

- Need to add new dependencies
- Need to change dependency versions
- Security implications

**Design decisions:**

- Architectural changes
- API design choices
- Performance vs correctness trade-offs

**Examples:**

- "Should I use async/await or keep synchronous?"
- "This requires adding a new dependency (X). Is that okay?"
- "This is a breaking change to the API. Should I proceed?"

### Don't Ask When

**Patterns are clear:**

- Following established patterns in codebase
- Standard bug fix approach
- Documentation-only changes
- Obvious code improvements (typos, formatting)

**Best practices:**

- Adding type hints
- Adding tests for new code
- Improving error messages
- Adding docstrings

**Examples:**

- "Should I add a docstring?" ‚Üí Yes, always (don't ask)
- "Should I add tests?" ‚Üí Yes, always (don't ask)
- "Should I format with black?" ‚Üí Yes, always (don't ask)

---

## Quick Reference

### Most Common Commands

```bash
make install-hooks   # Install pre-commit hook (one-time setup)
make format          # Format code
make test            # Run tests
make ci              # Full CI suite
make docs            # Build docs
make lint-markdown   # Check markdown files
make clean           # Clean build artifacts
```

```python
# Configuration
from podcast_scraper import Config
cfg = Config(rss="...", ...)

# Pipeline
from podcast_scraper import run_pipeline
count, summary = run_pipeline(cfg)

# Service API
from podcast_scraper import service
result = service.run(cfg)

# Progress
from podcast_scraper import progress
with progress.make_progress(...) as pbar:
    pbar.update(1)

# Logging
import logging
logger = logging.getLogger(**name**)
logger.info("message")

# Prompts (when implemented)
from podcast_scraper.prompt_store import render_prompt
prompt = render_prompt("summarization/long_v1", **params)
```

```python
from pathlib import Path
from typing import Optional, List, Dict

# Third-party (group 2)
import requests
from pydantic import BaseModel

# Local (group 3)
from podcast_scraper import config
from podcast_scraper.models import Episode
```

## Related Documentation

- **[CONTRIBUTING.md](CONTRIBUTING.md)** - Human contributor guide (quick start, setup, workflow, PR process)
- **[docs/DEVELOPMENT_NOTES.md](docs/DEVELOPMENT_NOTES.md)** - **üìö Detailed technical information** (code style, testing, CI/CD, architecture, logging, documentation standards, markdown linting)
- **[docs/CI_CD.md](docs/CI_CD.md)** - CI/CD pipeline documentation with visualizations
- **[docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)** - Architecture design and module responsibilities
- **[docs/TESTING_STRATEGY.md](docs/TESTING_STRATEGY.md)** - Comprehensive testing approach
- **[docs/api/](docs/api/)** - Auto-generated API reference
- **[README.md](README.md)** - User-facing documentation
- **[docs/rfc/RFC-015-ai-experiment-pipeline.md](docs/rfc/RFC-015-ai-experiment-pipeline.md)** - AI experiment pipeline design
- **[docs/rfc/RFC-016-modularization-for-ai-experiments.md](docs/rfc/RFC-016-modularization-for-ai-experiments.md)** - Provider system architecture
- **[docs/rfc/RFC-017-prompt-management.md](docs/rfc/RFC-017-prompt-management.md)** - Prompt management system
- **[docs/rfc/RFC-013-openai-provider-implementation.md](docs/rfc/RFC-013-openai-provider-implementation.md)** - OpenAI provider design

**When you need detailed technical information:**

- Code style details ‚Üí See `docs/DEVELOPMENT_NOTES.md` (Code Style Guidelines section)
- Testing details ‚Üí See `docs/DEVELOPMENT_NOTES.md` (Testing Requirements section)
- CI/CD details ‚Üí See `docs/DEVELOPMENT_NOTES.md` (CI/CD Integration section)
- Architecture patterns ‚Üí See `docs/DEVELOPMENT_NOTES.md` (Architecture Principles section)
- Logging patterns ‚Üí See `docs/DEVELOPMENT_NOTES.md` (Logging Guidelines section)
- Documentation standards ‚Üí See `docs/DEVELOPMENT_NOTES.md` (Documentation Standards section)

---

## Summary for AI Assistants

**Remember:**

**üö® TOP 3 CRITICAL RULES (MUST FOLLOW - NO EXCEPTIONS):**

1. ‚úÖ **NEVER bypass pre-commit hooks** (CRITICAL)
   - **MANDATORY:** Fix linting/formatting issues instead of using `--no-verify`
   - **EXCEPTION:** Only use `--no-verify` if user explicitly requests it
   - **NEVER bypass hooks without explicit user request.**

2. ‚úÖ **ALWAYS show changes and get approval before committing** (CRITICAL)
   - **MANDATORY CHECKLIST:** `git status` ‚Üí `git diff` ‚Üí Wait for approval ‚Üí Get message ‚Üí THEN commit
   - **NEVER skip any step. NEVER assume approval.**

3. ‚úÖ **ALWAYS show `git status` and `git diff` before pushing to PR** (CRITICAL)
   - **MANDATORY:** Show user what will be pushed ‚Üí Get approval ‚Üí THEN push
   - **NEVER push without showing changes first.**
4. ‚úÖ **ALWAYS get explicit user approval before pushing to PR** (CRITICAL)
   - **MANDATORY:** Ask user ‚Üí Wait for "yes"/"push"/"go ahead" ‚Üí THEN push
   - **NEVER push without explicit user approval.**
4. ‚úÖ **ALWAYS run `make ci` before pushing to PR** (new or updated PR) (CRITICAL)
   - **MANDATORY:** Run `make ci` ‚Üí Verify passes ‚Üí Fix failures ‚Üí Ask for approval ‚Üí THEN push
   - **NEVER push without running `make ci` first.**
5. ‚úÖ **CONDITIONALLY run `make docker-test` before pushing to PR** (when Docker-related changes exist) (CRITICAL)
   - **MANDATORY:** If you changed Dockerfile, pyproject.toml structure, or package layout ‚Üí Run `make docker-test` ‚Üí Verify passes ‚Üí Ask for approval ‚Üí THEN push
   - **NEVER push Docker-related changes without running `make docker-test` first.**
4. ‚úÖ Install pre-commit hook with `make install-hooks` (prevents CI failures)
5. ‚úÖ Run `python scripts/fix_markdown.py` before committing markdown files (fixes common linting issues)
6. ‚úÖ Run `make format` and `make test` before committing (if no hook)
5. ‚úÖ Respect module boundaries (no business logic in CLI, no HTTP in config, etc.)
6. ‚úÖ Mock external dependencies in tests
7. ‚úÖ Add docstrings to all public functions (Google-style)
8. ‚úÖ Use Config for all runtime options
9. ‚úÖ Never commit secrets or personal data
10. ‚úÖ Create PRD for user-facing features, RFC for architecture changes
11. ‚úÖ Update README when CLI changes
12. ‚úÖ Follow conventional commit format
13. ‚úÖ Ask when uncertain, don't ask for obvious best practices
14. ‚úÖ Use lazy loading pattern for `cli` and `service` imports
15. ‚úÖ Update index files (`docs/index.md`, `docs/rfc/index.md`, `docs/prd/index.md`) when adding RFCs/PRDs
16. ‚úÖ Use `python-dotenv` for environment variable management (`.env` files)
17. ‚úÖ Treat prompts as provider-specific concerns (not part of core protocol)

**When in doubt:**

- Check existing code for patterns
- Look at similar functions for examples
- Read ARCHITECTURE.md for design principles
- Read docs/CI_CD.md for CI/CD pipeline details
- Read relevant RFCs/PRDs for feature context
- Run `make ci` to validate changes

---

---

## üîí How to Ensure AI Follows These Rules

**As a user, you can enforce these rules by:**

### 1. **Start-of-Session Reminder**

At the beginning of a session, ask:

- "Did you read the AI guidelines?"
- "Check the guidelines before we start"
- "Follow the guidelines in `.ai-coding-guidelines.md`"

### 2. **Before Critical Actions**

Before commits or pushes, ask:

- "Show me what you're about to commit/push"
- "Did you check the guidelines for commits/pushes?"
- "Wait, let me review first" (this should STOP the AI)

### 3. **Verification Questions**

Ask the AI to confirm:

- "What are the rules for pushing to PR?"
- "What's the commit workflow?"
- "Show me the checklist before you push"

### 4. **Explicit Stop Commands**

If AI tries to push/commit without approval:

- Say "STOP" or "Don't push yet"
- The AI MUST stop and wait for approval

### 5. **Regular Reminders**

Periodically remind:

- "Remember: always ask before pushing"
- "Check the guidelines again"
- "What does the guideline say about [action]?"

**The AI should acknowledge these reminders and follow them.**

---

**Version:** 2.1 (added explicit user approval requirements and enforcement guide)
**Last Updated:** 2025-12-26
