# AI Coding Guidelines for podcast_scraper

**Purpose:** This document provides comprehensive context and guidelines for AI coding assistants (Cursor, GitHub Copilot, etc.) working on this project. Think of this as a project-specific CLAUDE.md - instructions for any AI agentic editor.

**For human contributors**, see [CONTRIBUTING.md](CONTRIBUTING.md) instead.

---

## üéØ How Cursor Uses This File

**Cursor automatically reads this file** when you're working in this project. It provides context to AI features like:

- **Composer** (AI code generation)
- **Chat** (AI assistant)
- **Inline edits** (AI-powered code suggestions)

**To maximize effectiveness:**

1. ‚úÖ Keep this file updated when patterns change
2. ‚úÖ Reference specific sections when asking Cursor to implement features
3. ‚úÖ Use clear, actionable language (Cursor reads this as instructions)
4. ‚úÖ Include code examples for common patterns

**Cursor-specific optimizations:**

- Sections are structured for easy semantic search
- Code examples are copy-paste ready
- Decision trees help AI make autonomous choices
- "When to Ask" section prevents unnecessary interruptions

---

## Table of Contents

1. [Project Context](#project-context)
2. [When to Run What](#when-to-run-what)
3. [Code Organization](#code-organization)
4. [Package Import Patterns](#package-import-patterns)
5. [Testing Requirements](#testing-requirements)
6. [Documentation Standards](#documentation-standards)
7. [Common Patterns](#common-patterns)
8. [AI Experiment Pipeline](#ai-experiment-pipeline)
9. [Prompt Management](#prompt-management)
10. [OpenAI Provider Integration](#openai-provider-integration)
11. [Environment Variables & Secrets](#environment-variables--secrets)
12. [Security & Secrets](#security--secrets)
13. [Git Workflow](#git-workflow)
14. [Decision Trees](#decision-trees)
15. [When to Ask](#when-to-ask)

---

## Project Context

### What This Project Does

**podcast_scraper** is a Python tool for downloading podcast transcripts from RSS feeds with optional Whisper transcription fallback.

**Key capabilities:**

- Download published transcripts from RSS feeds (Podcasting 2.0 namespace)
- Fallback to Whisper transcription for episodes without transcripts
- Automatic speaker detection using spaCy NER
- Episode summarization using local transformer models (BART, LED) or OpenAI API
- Metadata generation (JSON/YAML)
- Multi-threaded downloads
- Screenplay formatting
- Service/daemon mode for automated runs
- **AI Experiment Pipeline** (configuration-driven model evaluation)
- **Prompt Management** (versioned, parameterized prompts)

**Primary use case:** Personal, non-commercial transcript archival for research and study.

### Architecture Overview

**Design:** Modular architecture with clear separation of concerns

**13 core modules:**

1. `cli.py` - Interactive command-line interface
2. `service.py` - Non-interactive service API for daemons
3. `workflow.py` - Pipeline orchestration
4. `config.py` - Configuration model (Pydantic)
5. `rss_parser.py` - RSS feed parsing
6. `downloader.py` - HTTP operations with retry logic
7. `episode_processor.py` - Episode-level processing
8. `filesystem.py` - File system utilities
9. `whisper_integration.py` - Whisper transcription interface
10. `speaker_detection.py` - NER-based speaker extraction
11. `summarizer.py` - Transcript summarization
12. `metadata.py` - Metadata document generation
13. `progress.py` - Progress reporting abstraction

**Future modules (planned):**

- `prompt_store.py` - Prompt management and loading (RFC-017)
- `experiment_config.py` - Experiment configuration models (RFC-015)
- Provider modules (OpenAI, etc.) - Protocol-based provider system (RFC-016)

**Data flow:**

```text
RSS Feed ‚Üí Parse Episodes ‚Üí Download/Transcribe ‚Üí Detect Speakers ‚Üí 
Generate Metadata ‚Üí Summarize ‚Üí Write Files
```

**See also:** `docs/ARCHITECTURE.md` for detailed design

### Key Technologies

**Core:**

- Python 3.10+ (type hints, Pydantic v2)
- Pydantic for configuration validation
- Click for CLI (optional dependency)
- defusedxml for safe XML parsing
- `python-dotenv` for environment variable management

**ML/AI:**

- OpenAI Whisper for transcription
- spaCy for NER (speaker detection)
- PyTorch + transformers for summarization (BART, PEGASUS, LED)
- OpenAI API (planned) for optional providers
- Jinja2 for prompt templating (RFC-017)

**Testing:**

- pytest with unittest.mock
- pytest markers: `@pytest.mark.slow`, `@pytest.mark.integration`
- Coverage target: >80%

**Documentation:**

- MkDocs with Material theme
- mkdocstrings for API docs (auto-generated from docstrings)
- Mermaid for diagrams

**Code Quality:**

- black (formatting, line length: 100)
- isort (import sorting)
- flake8 (linting)
- mypy (type checking)
- bandit (security scanning)

### Project Structure

```text
podcast_scraper/
‚îú‚îÄ‚îÄ __init__.py              # Public API exports (lazy loading)
‚îú‚îÄ‚îÄ cli.py                   # CLI interface
‚îú‚îÄ‚îÄ service.py               # Service API
‚îú‚îÄ‚îÄ workflow.py              # Pipeline orchestration
‚îú‚îÄ‚îÄ config.py                # Configuration model
‚îú‚îÄ‚îÄ [10 more modules]        # Core functionality
‚îú‚îÄ‚îÄ tests/                   # Test suite
‚îú‚îÄ‚îÄ docs/                    # MkDocs documentation
‚îÇ   ‚îú‚îÄ‚îÄ rfc/                 # Technical RFCs
‚îÇ   ‚îú‚îÄ‚îÄ prd/                 # Product requirements
‚îÇ   ‚îî‚îÄ‚îÄ wip/                  # Work in progress docs
‚îú‚îÄ‚îÄ examples/                # Config examples
‚îú‚îÄ‚îÄ scripts/                 # Evaluation scripts
‚îú‚îÄ‚îÄ prompts/                 # Prompt templates (planned, RFC-017)
‚îú‚îÄ‚îÄ Makefile                 # Development commands
‚îú‚îÄ‚îÄ pyproject.toml           # Package metadata
‚îú‚îÄ‚îÄ requirements.txt         # Dependencies
‚îî‚îÄ‚îÄ CONTRIBUTING.md          # Contributor guide
```

---

## When to Run What

### During Development (Local)

**One-time setup (RECOMMENDED):**
```bash
make install-hooks    # Install pre-commit hook for automatic checks
```

**With pre-commit hook installed:**
```bash
# Hook runs automatically on every commit:
git commit -m "your changes"
# ‚Üí Checks: black, isort, flake8, markdownlint, mypy
# ‚Üí Blocks commit if checks fail
# ‚Üí Auto-fix with: make format

# Before pushing:
make ci          # Full CI suite (all checks + tests + docs + build)
```

**Without pre-commit hook:**
```bash
# Before every commit:
make format      # Auto-format with black/isort
make test        # Run tests with coverage

# Before every push:
make ci          # Full CI suite
```

**When to run specific checks:**

- `make lint` - After adding new functions (flake8)
- `make type` - After changing type hints (mypy)
- `make security` - After changing dependencies (bandit, pip-audit)
- `make docs` - After updating docstrings or docs/
- `make lint-markdown` - After editing markdown files

### What CI Runs Automatically

> **See also:** [`docs/CI_CD.md`](docs/CI_CD.md) for complete CI/CD pipeline documentation with visualizations.

**On every push to PR (5 parallel workflows):**

1. **Python App Workflow** (Lint Job, 2-3 min):
   - Black/isort formatting checks
   - flake8 linting
   - markdownlint
   - mypy type checking
   - bandit + pip-audit security

2. **Python App Workflow** (Test Job, 10-15 min):
   - Full pytest suite with coverage

3. **Docs Workflow** (3-5 min):
   - mkdocs build --strict
   - API docs generation

4. **Docker Workflow** (on Dockerfile changes):
   - Docker build and test

5. **Snyk Workflow** (weekly + on push):
   - Security vulnerability scanning

**Additional workflows:**

- **CodeQL Security** (weekly + on push): Python & Actions security scanning
- **Docs Deployment** (on main): Build ‚Üí Deploy to GitHub Pages

**When CI fails:**

1. Read the error message carefully
2. Run `make ci` locally to reproduce
3. Fix the issue (use `make format` for formatting issues)
4. Test locally again
5. Push the fix

**Prevent CI failures:**
```bash
make install-hooks    # Catch issues before commit!
```

### What NOT to Run

**Don't run these during development:**

- ‚ùå Long-running integration tests (use `pytest -m "not slow"`)
- ‚ùå Full model downloads for every test
- ‚ùå Docker builds (unless working on Docker specifically)
- ‚ùå Deployment commands (GitHub Actions handles this)

**Use CI for:**

- Full test suite on every platform
- Documentation deployment
- Release builds

---

## Code Organization

### When to Create New Files

**Create a new module when:**

- Implementing a new major feature (>500 lines)
- Feature has distinct responsibility (SRP)
- Feature can be tested independently
- Example: Adding PostgreSQL export ‚Üí create `export.py`

**Modify existing files when:**

- Fixing bugs in existing functionality
- Enhancing existing features (< 200 lines added)
- Refactoring within same module
- Example: Improving Whisper model selection ‚Üí modify `whisper_integration.py`

**Don't create:**

- Helper modules with only 1-2 functions (add to appropriate existing module)
- Duplicate functionality (search codebase first)
- Temporary files (use proper temporary directories)

### Module Boundaries (STRICT)

**Respect these boundaries** - don't mix concerns:

| Module | Responsibility | What to AVOID |
| ------ | --------------- | ------------- |
| `cli.py` | CLI interface only | Business logic, HTTP calls, file I/O |
| `service.py` | Service API, structured results | CLI interaction, user prompts |
| `workflow.py` | Orchestration only | HTTP details, file parsing, direct I/O |
| `config.py` | Configuration model | Business logic, side effects |
| `downloader.py` | HTTP operations only | Parsing, business logic |
| `filesystem.py` | File system utilities | HTTP, parsing, business logic |
| `rss_parser.py` | RSS parsing, Episode creation | HTTP, file I/O, workflow |
| `episode_processor.py` | Episode-level processing | CLI interaction, orchestration |
| `whisper_integration.py` | Whisper interface | HTTP, RSS parsing |
| `speaker_detection.py` | NER extraction | HTTP, file I/O |
| `summarizer.py` | Summarization | Episode processing, HTTP |
| `metadata.py` | Metadata generation | HTTP, RSS parsing |
| `progress.py` | Progress abstraction | Business logic |

**If you're about to:**

- Add HTTP calls to `cli.py` ‚Üí ‚ùå Use `downloader.py` instead
- Add business logic to `config.py` ‚Üí ‚ùå Use `workflow.py` or appropriate module
- Add CLI prompts to `service.py` ‚Üí ‚ùå Service is non-interactive

### File Naming Conventions

**Python modules:**

- Use `snake_case.py`
- Be descriptive: `speaker_detection.py` not `speakers.py`
- Avoid generic names: `utils.py`, `helpers.py`

**Test files:**

- Mirror structure: `test_<module>.py`
- Example: `test_speaker_detection.py` tests `speaker_detection.py`

**Documentation:**

- Use `UPPER_CASE.md` for top-level docs: `README.md`, `CONTRIBUTING.md`
- Use descriptive names for guides: `TESTING_STRATEGY.md`
- PRD/RFC format: `PRD-NNN-title.md`, `RFC-NNN-title.md`

---

## Package Import Patterns

### Lazy Loading Pattern (CRITICAL)

**The `__init__.py` uses lazy loading** to prevent circular imports:

```python
# __init__.py uses __getattr__ for lazy loading
from podcast_scraper import cli, service  # ‚úÖ Works (lazy loaded)
import podcast_scraper.cli as cli         # ‚úÖ Also works
```

**Why this matters:**

- `cli.py` and `service.py` are loaded on-demand via `__getattr__`
- Prevents circular import issues
- Maintains backward compatibility

**When adding new lazy-loaded modules:**

1. Add to `__getattr__` in `__init__.py`
2. Add to `_import_cache` for caching
3. Update `__all__` if needed (but note that lazy-loaded modules aren't in `__all__`)

**Example from `__init__.py`:**
```python
_import_cache: dict[str, object] = {}

def __getattr__(name: str):
    if name in _import_cache:
        return _import_cache[name]
    
    if name == "cli":
        import importlib
        _cli = importlib.import_module(f"{__name__}.cli")
        _import_cache[name] = _cli
        return _cli
    # ... similar for service
    raise AttributeError(f"module {__name__!r} has no attribute {name!r}")
```

### Import Organization

**Follow isort conventions (3 groups):**

```python
# Standard library (group 1)
import logging
import os
from pathlib import Path
from typing import Optional, List, Dict

# Third-party (group 2)
import requests
from pydantic import BaseModel

# Local (group 3)
from podcast_scraper import config
from podcast_scraper.models import Episode
```

**Test imports:**
```python
# Tests use absolute imports from package
from podcast_scraper import Config, run_pipeline
from podcast_scraper.downloader import fetch_url
```

---

## Testing Requirements

### Every New Function Must Have

**1. Unit test with mocks:**
```python
@patch("podcast_scraper.downloader.requests.Session")
def test_fetch_url_with_retry(self, mock_session):
    """Test that fetch_url retries on network failure."""
    mock_session.get.side_effect = [
        requests.ConnectionError("Network error"),
        MockHTTPResponse(content="Success", status_code=200)
    ]
    result = fetch_url("https://example.com")
    self.assertEqual(result, "Success")
```

**2. Test both success and failure:**
```python
def test_sanitize_filename_valid(self):
    """Happy path."""
    pass

def test_sanitize_filename_invalid_chars(self):
    """Error case."""
    pass
```

**3. Descriptive test names:**
```python
# Good
def test_config_validation_raises_error_for_negative_workers(self):
    pass

# Bad
def test_config(self):
    pass
```

### Mock External Dependencies

**Always mock:**

- HTTP requests (`requests` module)
- Whisper models (`whisper.load_model()`, `whisper.transcribe()`)
- File I/O (use `tempfile.TemporaryDirectory`)
- spaCy models (mock NER extraction)
- Time-based operations (`time.sleep()`)
- OpenAI API calls (when implementing OpenAI providers)

**Example:**
```python
@patch("podcast_scraper.whisper_integration.whisper")
def test_transcription(self, mock_whisper):
    mock_whisper.load_model.return_value = Mock()
    mock_whisper.transcribe.return_value = {"text": "Test"}
    # ... test code ...
```

### Test Organization

**Structure:**

```text
tests/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ conftest.py              # Shared fixtures
‚îú‚îÄ‚îÄ test_cli.py              # CLI tests
‚îú‚îÄ‚îÄ test_service.py          # Service API tests
‚îú‚îÄ‚îÄ test_[module].py         # One per module
‚îú‚îÄ‚îÄ test_package_imports.py  # Package structure tests
‚îî‚îÄ‚îÄ test_integration.py      # Integration tests
```

**Test markers:**
```python
@pytest.mark.slow           # Slow tests (skip in dev)
@pytest.mark.integration    # Integration tests
@pytest.mark.e2e            # End-to-end tests (not yet implemented)
```

**Run tests:**
```bash
pytest                       # All tests
pytest -m "not slow"        # Skip slow tests
pytest -v                   # Verbose
pytest tests/test_cli.py    # Specific file
```

---

## Documentation Standards

### When to Create PRD

**Create a PRD for:**

- New user-facing features
- Significant functionality additions
- Changes affecting user workflows

**Template:** `docs/prd/PRD-NNN-title.md`

**Examples:**

- PRD-004: Metadata Generation
- PRD-005: Episode Summarization
- PRD-006: OpenAI Provider Integration
- PRD-007: AI Experiment Pipeline

**Structure:**
```markdown
# PRD-NNN: Feature Name

## Problem
[What problem does this solve?]

## Goals
[What are we trying to achieve?]

## Non-Goals
[What are we explicitly NOT doing?]

## Solution
[Proposed solution]

## Success Criteria
[How do we know it works?]
```

### When to Create RFC

**Create an RFC for:**

- Architectural changes
- Breaking API changes
- Design decisions needing discussion
- Technical implementation approaches

**Template:** `docs/rfc/RFC-NNN-title.md`

**Examples:**

- RFC-010: Speaker Name Detection
- RFC-012: Episode Summarization
- RFC-013: OpenAI Provider Implementation
- RFC-015: AI Experiment Pipeline
- RFC-016: Modularization for AI Experiments
- RFC-017: Prompt Management

**Structure:**
```markdown
# RFC-NNN: Feature Name

## Background
[Context and motivation]

## Proposal
[Detailed technical proposal]

## Alternatives Considered
[Other approaches and why not chosen]

## Implementation
[How to build this]

## Testing
[How to test this]
```

### When to Skip PRD/RFC

**You can skip documentation for:**

- ‚úÖ Bug fixes (just fix it)
- ‚úÖ Small enhancements (< 100 lines)
- ‚úÖ Internal refactoring (no API changes)
- ‚úÖ Documentation updates
- ‚úÖ Test improvements
- ‚úÖ Performance optimizations (if localized)

**Rule of thumb:** If it takes longer to document than implement, skip the doc.

### Always Update

**Update these files when:**

| File | When to Update |
| ---- | -------------- |
| `README.md` | CLI flags change, new features, installation changes |
| `docs/ARCHITECTURE.md` | Module changes, data flow changes, design decisions |
| `docs/TESTING_STRATEGY.md` | Testing approach changes, new test infrastructure |
| `docs/api/` | Public API changes (auto-generated from docstrings) |
| `CONTRIBUTING.md` | Development workflow changes, tool changes |
| `.ai-coding-guidelines.md` | Patterns change, new guidelines needed |
| `docs/index.md`, `docs/rfc/index.md`, `docs/prd/index.md` | New RFCs/PRDs added |

### Docstring Requirements

**All public functions need docstrings:**
```python
def run_pipeline(cfg: Config) -> Tuple[int, str]:
    """Execute the main podcast scraping pipeline.
    
    This orchestrates the complete workflow from RSS feed fetching
    to transcript generation and optional metadata/summarization.
    
    Args:
        cfg: Configuration object with all pipeline settings.
        
    Returns:
        Tuple[int, str]: (count, summary) where count is episodes
            processed and summary is human-readable message.
            
    Raises:
        ValueError: If RSS URL is invalid.
        RuntimeError: If output cleanup fails.
        
    Example:
        >>> cfg = Config(rss="https://example.com/feed.xml")
        >>> count, summary = run_pipeline(cfg)
        >>> print(f"Processed {count} episodes")
    """
```

**Style:** Google-style docstrings (for mkdocstrings compatibility)

---

## Common Patterns

### Configuration Pattern

**All runtime options flow through Config:**
```python
from podcast_scraper import Config

# Good - centralized configuration
cfg = Config(
    rss="https://example.com/feed.xml",
    transcribe_missing=True,
    workers=8
)
run_pipeline(cfg)

# Bad - scattered parameters
fetch_rss(url, timeout=30)
download(episodes, workers=8)
transcribe(jobs, model="base")
```

**Adding new configuration:**

1. Add field to `Config` model in `config.py`
2. Add CLI argument in `cli.py`
3. Document in README options section
4. Update `examples/config.example.json` and `examples/config.example.yaml`

### Error Handling Pattern

**Recoverable errors - log and continue:**
```python
try:
    transcript = download_transcript(url)
except requests.RequestException as e:
    logger.warning(f"Failed to download: {e}")
    return None  # Continue with other episodes
```

**Unrecoverable errors - raise with clear message:**
```python
if not cfg.rss:
    raise ValueError("RSS URL is required")

if cfg.workers < 1:
    raise ValueError(f"Workers must be >= 1, got: {cfg.workers}")
```

**Graceful degradation for optional features:**
```python
try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    logger.warning("Whisper not available")
```

### Progress Reporting Pattern

**Use progress abstraction (don't use tqdm directly):**
```python
from podcast_scraper import progress

# Good
with progress.make_progress(
    total=len(episodes),
    desc="Downloading"
) as pbar:
    for episode in episodes:
        process(episode)
        pbar.update(1)

# Bad
from tqdm import tqdm
for episode in tqdm(episodes):
    process(episode)
```

### Lazy Loading Pattern

**For optional dependencies:**
```python
# At module level
_whisper = None

def get_whisper():
    """Lazy load Whisper library."""
    global _whisper
    if _whisper is None:
        try:
            import whisper
            _whisper = whisper
        except ImportError:
            raise ImportError(
                "Whisper not installed. "
                "Install: pip install openai-whisper"
            )
    return _whisper

# Usage
def transcribe(audio_path):
    whisper = get_whisper()
    model = whisper.load_model("base")
    # ...
```

### Logging Pattern

```python
import logging

logger = logging.getLogger(__name__)

# Good - structured logging with context
logger.info(f"Processing episode {episode.number}: {episode.title}")
logger.warning(f"Failed to download transcript for {episode.number}: {error}")
logger.error(f"Pipeline failed: {error}", exc_info=True)

# Bad - print statements
print("Processing episode")
print("Error:", error)
```

---

## AI Experiment Pipeline

### AI Experiment Pipeline Overview

**The AI Experiment Pipeline** (RFC-015, PRD-007) is a configuration-driven system for evaluating models, prompts, and parameters. Think of it like a test pipeline for models instead of code.

**Key Concepts:**

- **Configuration-Driven**: Experiments defined in YAML configs
- **Separation of Generation from Scoring**: Generate predictions first, compute metrics separately
- **Two-Layer CI/CD**: Fast smoke tests (CI) + full evaluation pipeline (nightly/on-demand)
- **Test Pipeline Analogy**: Treat model evaluation like unit/integration tests

### Experiment Structure

**Example experiment config:**
```yaml
# experiments/summarization_openai_long_v1.yaml
id: "summarization_openai_long_v1"
task: "summarization"

backend:
  type: "openai"
  model: "gpt-4o-mini"

prompts:
  system: "summarization/system_v1"
  user: "summarization/long_v1"
  params:
    paragraphs_min: 3
    paragraphs_max: 6

data:
  episodes_glob: "data/eval/episodes/ep*/transcript.txt"
  id_from: "parent_dir"

params:
  max_output_tokens: 900
```

**Output structure:**

```text
results/
  summarization_openai_long_v1/
    predictions.jsonl        # Model outputs per episode
    run_metadata.json        # Backend + prompts + stats
    metrics.json            # ROUGE, compression, etc. (from eval script)
```

### Implementation Phases

**Phase 1: Normalize Existing Structure**

- Move gold data to `data/eval/episodes/`
- Establish baseline results

**Phase 2: Generic Runner**

- `scripts/run_experiment.py` - Load config, call backend, save predictions
- `podcast_scraper/experiment_config.py` - Pydantic models for configs

**Phase 3: CI Smoke Tests (Layer A)**

- Fast tests on subset of episodes
- Assert basic metrics (ROUGE-L > threshold)

**Phase 4: Full Eval Pipeline (Layer B)**

- Nightly/on-demand full evaluation
- Run all experiment configs
- Generate comparison reports

**Phase 5: Comparison Tooling**

- Excel workbook generation
- Side-by-side metric comparison

**See:** `docs/rfc/RFC-015-ai-experiment-pipeline.md` and `docs/prd/PRD-007-ai-experiment-pipeline.md`

---

## Prompt Management

### Overview

**Prompt Management** (RFC-017) treats prompts as first-class, versioned assets with optional templating.

**Key Principles:**

- Prompts are **provider-specific concerns** (not part of core protocol)
- File-based organization with explicit versioning
- Jinja2 templating for parameterization
- SHA256 hashing for reproducibility

### Prompt Directory Structure

```text
prompts/
  summarization/
    system_v1.j2
    long_v1.j2
    long_v2_more_narrative.j2
  ner/
    system_ner_v1.j2
    guest_host_v1.j2
```

**Example prompt (`prompts/summarization/long_v2_more_narrative.j2`):**
```jinja2
You are summarizing a podcast episode.

Write a detailed, narrative summary with a clear story arc.

Guidelines:

- Aim for {{ paragraphs_min }}‚Äì{{ paragraphs_max }} paragraphs.
- Focus on key decisions, arguments, and lessons.
- Ignore sponsorships, ads, and housekeeping.
- Do not use quotes or speaker names.
- Do not invent information not implied by the transcript.
```

### Using Prompts

**In application code (OpenAI providers):**
```python
from podcast_scraper.prompt_store import render_prompt, get_prompt_metadata

system_prompt = render_prompt("summarization/system_v1")
user_prompt = render_prompt(
    "summarization/long_v1",
    paragraphs_min=3,
    paragraphs_max=6
)

# Track prompt metadata for reproducibility
metadata = get_prompt_metadata("summarization/long_v1", {"paragraphs_min": 3})
```

**In experiment configs:**
```yaml
prompts:
  system: "summarization/system_v1"
  user: "summarization/long_v1"
  params:
    paragraphs_min: 3
    paragraphs_max: 6
```

**Implementation:** `podcast_scraper/prompt_store.py` (RFC-017)

---

## OpenAI Provider Integration

### OpenAI Provider Overview

**OpenAI Provider Integration** (RFC-013, PRD-006) adds OpenAI API as optional providers for speaker detection, transcription, and summarization.

**Key Principles:**

- **Protocol-Based**: Providers implement protocols (RFC-016)
- **Backward Compatible**: Default providers (local) unchanged
- **Per-Capability Selection**: Can mix local and OpenAI providers
- **Secure API Key Management**: Environment variables, never in code

### Provider Architecture

**After modularization:**

```text
podcast_scraper/
‚îú‚îÄ‚îÄ speaker_detectors/
‚îÇ   ‚îú‚îÄ‚îÄ base.py              # SpeakerDetector protocol
‚îÇ   ‚îú‚îÄ‚îÄ factory.py           # Factory (selects provider)
‚îÇ   ‚îú‚îÄ‚îÄ ner_detector.py      # Local NER provider (existing)
‚îÇ   ‚îî‚îÄ‚îÄ openai_detector.py   # NEW: OpenAI provider
‚îú‚îÄ‚îÄ transcription/
‚îÇ   ‚îú‚îÄ‚îÄ base.py              # TranscriptionProvider protocol
‚îÇ   ‚îú‚îÄ‚îÄ factory.py           # Factory
‚îÇ   ‚îú‚îÄ‚îÄ whisper_provider.py  # Local Whisper provider
‚îÇ   ‚îî‚îÄ‚îÄ openai_provider.py   # NEW: OpenAI Whisper API
‚îú‚îÄ‚îÄ summarization/
‚îÇ   ‚îú‚îÄ‚îÄ base.py              # SummarizationProvider protocol
‚îÇ   ‚îú‚îÄ‚îÄ factory.py           # Factory
‚îÇ   ‚îú‚îÄ‚îÄ local_provider.py    # Local transformers provider
‚îÇ   ‚îî‚îÄ‚îÄ openai_provider.py   # NEW: OpenAI GPT provider
```

### Configuration

**Provider selection:**
```python
# In config.py
speaker_detector_type: Literal["ner", "openai"] = "ner"
transcription_provider: Literal["whisper", "openai"] = "whisper"
summary_provider: Literal["transformers", "openai"] = "transformers"
```

**API key management:** See [Environment Variables & Secrets](#environment-variables--secrets)

**See:** `docs/rfc/RFC-013-openai-provider-implementation.md` and `docs/rfc/RFC-016-modularization-for-ai-experiments.md`

---

## Environment Variables & Secrets

### API Key Management

**OpenAI API keys** are managed via environment variables using `python-dotenv`:

**Implementation in `config.py`:**
```python
from dotenv import load_dotenv
import os

# Load .env file automatically (if present)
load_dotenv()

class Config(BaseModel):
    # ... other fields ...
    
    openai_api_key: Optional[str] = Field(
        default=None,
        description="OpenAI API key (or set OPENAI_API_KEY env var)"
    )
    
    @validator("openai_api_key", pre=True, always=True)
    def resolve_openai_api_key(cls, v):
        """Resolve API key from config field, env var, or .env file."""
        # Priority: config field > env var > .env file (already loaded)
        return v or os.getenv("OPENAI_API_KEY")
```

### .env File Setup

**Create `.env` file (never commit):**
```bash
# .env (local development)
OPENAI_API_KEY=sk-...
```

**`.env.example` (template, can commit):**
```bash
# OpenAI API Key (optional, for OpenAI providers)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-api-key-here
```

**Fallback priority:**

1. Config file field (`openai_api_key`)
2. System environment variable (`OPENAI_API_KEY`)
3. `.env` file (loaded by `python-dotenv`)

**Security:**

- ‚úÖ `.env` files are in `.gitignore`
- ‚úÖ Never commit `.env` files
- ‚úÖ Document required env vars in `README.md` and `CONTRIBUTING.md`
- ‚úÖ Use `.env.example` as template

**See:** `docs/rfc/RFC-013-openai-provider-implementation.md` section "API Key Management"

---

## Security & Secrets

### NEVER Commit

**Never add to git:**

- ‚ùå API keys or tokens
- ‚ùå Passwords or credentials
- ‚ùå Personal data
- ‚ùå Real RSS feed URLs in tests (use example.com)
- ‚ùå Private configuration files
- ‚ùå `.env` files with secrets

### Security Practices

**Input validation:**
```python
# Validate user inputs
if not isinstance(cfg.workers, int) or cfg.workers < 1:
    raise ValueError(f"Invalid workers: {cfg.workers}")

# Sanitize filenames
safe_name = sanitize_filename(episode.title)

# Check paths are within expected directory
if not output_path.is_relative_to(base_dir):
    raise ValueError(f"Path outside output dir: {output_path}")
```

**XML parsing:**
```python
# Use defusedxml for safe XML parsing
from defusedxml import ElementTree as ET

tree = ET.parse(rss_path)  # Safe against XML attacks
```

**HTTP:**
```python
# Use timeouts
response = requests.get(url, timeout=30)

# Verify SSL (default, but be explicit if needed)
response = requests.get(url, verify=True)
```

---

## Git Workflow

### Commit Message Format

```text
<type>: <short description>

<detailed description if needed>

Fixes #<issue-number>
```

**Types:**

- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation
- `test`: Tests
- `refactor`: Code refactoring
- `ci`: CI/CD changes
- `chore`: Maintenance
- `perf`: Performance

**Examples:**

```text
feat: add PostgreSQL export adapter

Implement export functionality to generate SQL dumps from
episode metadata. Includes schema templates and CLI flags.

Fixes #40
```

```text
fix: resolve double progress bar in Whisper

Remove duplicate progress indicators when transcribing.

Fixes #19
```

### Branch Naming

```bash
# Features
feature/add-postgresql-export
feature/issue-40-etl-loading

# Fixes
fix/whisper-progress-indicator
fix/issue-19-progress-bars

# Docs
docs/update-api-reference
docs/contributing-guide

# Refactoring
refactor/simplify-config-loading
```

---

## Decision Trees

### Should I Create a PRD/RFC?

```text
Is it a bug fix?
  ‚îî‚îÄ No ‚Üí Is it < 100 lines of code?
      ‚îî‚îÄ No ‚Üí Is it user-facing?
          ‚îú‚îÄ Yes ‚Üí Create PRD
          ‚îî‚îÄ No ‚Üí Is it architectural?
              ‚îú‚îÄ Yes ‚Üí Create RFC
              ‚îî‚îÄ No ‚Üí Skip, just implement
```

### Should I Create a New Module?

```text
Is it > 500 lines?
  ‚îî‚îÄ Yes ‚Üí Does it have a single clear responsibility?
      ‚îî‚îÄ Yes ‚Üí Can it be tested independently?
          ‚îî‚îÄ Yes ‚Üí Create new module
          ‚îî‚îÄ No ‚Üí Add to existing module
```

### Should I Mock This Dependency?

```text
Is it an external service?
  ‚îú‚îÄ Yes (HTTP, DB, API) ‚Üí Mock it
  ‚îî‚îÄ No ‚Üí Is it slow (> 1s)?
      ‚îú‚îÄ Yes (Whisper, ML models) ‚Üí Mock it
      ‚îî‚îÄ No ‚Üí Is it file I/O?
          ‚îú‚îÄ Yes ‚Üí Use tempfile
          ‚îî‚îÄ No ‚Üí Don't mock (pure functions)
```

### When to Run make ci?

```text
Have I installed pre-commit hook?
  ‚îú‚îÄ No ‚Üí Install: make install-hooks
  ‚îî‚îÄ Yes ‚Üí Continue

Am I about to push?
  ‚îî‚îÄ Yes ‚Üí Run make ci
      ‚îî‚îÄ Passes? ‚Üí Push
          ‚îî‚îÄ No? ‚Üí Fix and retry
```

**Workflow with pre-commit hook:**

```text
Edit code ‚Üí git commit ‚Üí Hook checks ‚Üí Commit succeeds ‚Üí
make ci ‚Üí Push ‚Üí CI runs ‚Üí PR ready
       ‚Üì
    Hook fails ‚Üí make format ‚Üí Retry commit
```

---

## When to Ask

### Ask the User When

**Requirements unclear:**

- Multiple valid approaches exist
- User preference matters (performance vs readability)
- Breaking changes are needed
- Trade-offs aren't obvious

**External dependencies:**

- Need to add new dependencies
- Need to change dependency versions
- Security implications

**Design decisions:**

- Architectural changes
- API design choices
- Performance vs correctness trade-offs

**Examples:**

- "Should I use async/await or keep synchronous?"
- "This requires adding a new dependency (X). Is that okay?"
- "This is a breaking change to the API. Should I proceed?"

### Don't Ask When

**Patterns are clear:**

- Following established patterns in codebase
- Standard bug fix approach
- Documentation-only changes
- Obvious code improvements (typos, formatting)

**Best practices:**

- Adding type hints
- Adding tests for new code
- Improving error messages
- Adding docstrings

**Examples:**

- "Should I add a docstring?" ‚Üí Yes, always (don't ask)
- "Should I add tests?" ‚Üí Yes, always (don't ask)
- "Should I format with black?" ‚Üí Yes, always (don't ask)

---

## Quick Reference

### Most Common Commands

```bash
make install-hooks   # Install pre-commit hook (one-time setup)
make format          # Format code
make test            # Run tests
make ci              # Full CI suite
make docs            # Build docs
make lint-markdown   # Check markdown files
make clean           # Clean build artifacts
```

### Most Common Patterns

```python
# Configuration
from podcast_scraper import Config
cfg = Config(rss="...", ...)

# Pipeline
from podcast_scraper import run_pipeline
count, summary = run_pipeline(cfg)

# Service API
from podcast_scraper import service
result = service.run(cfg)

# Progress
from podcast_scraper import progress
with progress.make_progress(...) as pbar:
    pbar.update(1)

# Logging
import logging
logger = logging.getLogger(__name__)
logger.info("message")

# Prompts (when implemented)
from podcast_scraper.prompt_store import render_prompt
prompt = render_prompt("summarization/long_v1", **params)
```

### Most Common Imports

```python
# Standard library (group 1)
import logging
import os
from pathlib import Path
from typing import Optional, List, Dict

# Third-party (group 2)
import requests
from pydantic import BaseModel

# Local (group 3)
from podcast_scraper import config
from podcast_scraper.models import Episode
```

---

## Related Documentation

- **[CONTRIBUTING.md](CONTRIBUTING.md)** - Human contributor guide (setup, workflow, PR process)
- **[docs/CI_CD.md](docs/CI_CD.md)** - CI/CD pipeline documentation with visualizations
- **[docs/ARCHITECTURE.md](docs/ARCHITECTURE.md)** - Architecture design and module responsibilities
- **[docs/TESTING_STRATEGY.md](docs/TESTING_STRATEGY.md)** - Comprehensive testing approach
- **[docs/api/](docs/api/)** - Auto-generated API reference
- **[README.md](README.md)** - User-facing documentation
- **[docs/rfc/RFC-015-ai-experiment-pipeline.md](docs/rfc/RFC-015-ai-experiment-pipeline.md)** - AI experiment pipeline design
- **[docs/rfc/RFC-016-modularization-for-ai-experiments.md](docs/rfc/RFC-016-modularization-for-ai-experiments.md)** - Provider system architecture
- **[docs/rfc/RFC-017-prompt-management.md](docs/rfc/RFC-017-prompt-management.md)** - Prompt management system
- **[docs/rfc/RFC-013-openai-provider-implementation.md](docs/rfc/RFC-013-openai-provider-implementation.md)** - OpenAI provider design

---

## Summary for AI Assistants

**Remember:**

1. ‚úÖ Install pre-commit hook with `make install-hooks` (prevents CI failures)
2. ‚úÖ Run `make format` and `make test` before committing (if no hook)
3. ‚úÖ Respect module boundaries (no business logic in CLI, no HTTP in config, etc.)
4. ‚úÖ Mock external dependencies in tests
5. ‚úÖ Add docstrings to all public functions (Google-style)
6. ‚úÖ Use Config for all runtime options
7. ‚úÖ Never commit secrets or personal data
8. ‚úÖ Create PRD for user-facing features, RFC for architecture changes
9. ‚úÖ Update README when CLI changes
10. ‚úÖ Follow conventional commit format
11. ‚úÖ Ask when uncertain, don't ask for obvious best practices
12. ‚úÖ Use lazy loading pattern for `cli` and `service` imports
13. ‚úÖ Update index files (`docs/index.md`, `docs/rfc/index.md`, `docs/prd/index.md`) when adding RFCs/PRDs
14. ‚úÖ Use `python-dotenv` for environment variable management (`.env` files)
15. ‚úÖ Treat prompts as provider-specific concerns (not part of core protocol)

**When in doubt:**

- Check existing code for patterns
- Look at similar functions for examples
- Read ARCHITECTURE.md for design principles
- Read docs/CI_CD.md for CI/CD pipeline details
- Read relevant RFCs/PRDs for feature context
- Run `make ci` to validate changes

---

**Version:** 2.0 (matches v2.4.0 + AI experiment pipeline + prompt management + OpenAI providers)  
**Last Updated:** 2025-01-27
