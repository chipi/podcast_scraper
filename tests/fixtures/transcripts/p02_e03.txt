Practical Systems — Episode 3

Security as Design, Not a Checklist

Host: Ethan Park
Guest: Camila Ruiz

⸻

[00:00]

Ethan:
Welcome back to Practical Systems. I’m Ethan Park. Today we’re talking about security, but not in the “add a scanner at the end and hope for the best” way. My guest is Camila Ruiz. Camila’s a security engineer who’s worked across product teams, platforms, and infrastructure. Camila, welcome.

Camila:
Thanks, Ethan. Happy to be here. And I already like that framing.

⸻

[01:10]

Ethan:
Let’s start with that phrase: security as design, not a checklist. When people hear that, they nod, but I’m not sure everyone means the same thing. What do you mean?

Camila:
For me, it’s about where decisions get made. If security only shows up as a checklist at the end—penetration test, dependency scan, maybe a compliance review—then all the real design decisions are already locked in. And at that point, security can only say “no” or “that’s risky,” which nobody enjoys.

Security as design means you treat threats, trust boundaries, and failure modes as first-class inputs, like performance or usability, right from the start.

⸻

[02:35]

Ethan:
So less “did we remember to encrypt?” and more “what are we actually protecting and from whom?”

Camila:
Exactly. Encryption is a tool, not a strategy. The strategy is understanding what happens if something goes wrong. Who gets hurt? What data moves where? What assumptions are we making about users, networks, and services?

⸻

[03:20]

Ethan:
One thing I’ve seen is teams saying, “We’ll fix security later,” but later never really comes. Why does that keep happening?

Camila:
Because later is always more expensive. Once an API is public, once clients depend on behavior, once data models are in production, changing anything fundamental is painful. So security issues get reframed as “accepted risk,” which is often just another way of saying “we ran out of time.”

⸻

[04:15]

Ethan:
Let’s talk about threat modeling, because that’s usually the first thing people mention. Some engineers hear that term and immediately think “bureaucracy.” How do you keep it useful?

Camila:
By keeping it lightweight and relevant. Threat modeling shouldn’t be a three-hour meeting with a giant diagram nobody revisits. It can be a 30-minute conversation focused on three questions:
	1.	What are we building?
	2.	What could go wrong in realistic ways?
	3.	What are we going to do about the most likely or most damaging risks?

If you can’t answer those, adding another tool won’t help.

⸻

[05:40]

Ethan:
Do you have a favorite concrete example where early security thinking saved pain later?

Camila:
Yeah. I worked on a system where we decided early on that every internal service call had to be authenticated, even inside the cluster. It was slightly more work upfront, but later when someone accidentally exposed an internal endpoint, it didn’t turn into a data breach. The blast radius was tiny.

If we’d assumed “internal equals trusted,” fixing that later would’ve been a nightmare.

⸻

[06:55]

Ethan:
That brings up zero trust, which is another buzzword that gets misused. How do you think about it without going dogmatic?

Camila:
Zero trust doesn’t mean “trust nothing ever.” It means “be explicit about trust.” You still trust things—but you write that trust down in code, configuration, and policy, instead of assuming it because of network location or naming conventions.

⸻

[08:10]

Ethan:
One thing engineers struggle with is secrets. API keys, tokens, certificates—somehow they always end up in places they shouldn’t. What’s the design mindset there?

Camila:
First, assume secrets will leak. That sounds pessimistic, but it’s realistic. So design systems where leaked secrets can be rotated, scoped, and monitored.

Second, make the secure path the easy path. If storing a secret safely is annoying, people will paste it into a config file or a Slack message. That’s not a people problem; it’s a system design problem.

⸻

[09:35]

Ethan:
I love that framing. Make the right thing the easy thing.

Camila:
Exactly. Engineers respond to friction. If security adds friction everywhere, it will get bypassed.

⸻

[10:20]

Ethan:
Let’s talk about “shift left.” People say it all the time, but it often turns into “run more scanners in CI.” Is that enough?

Camila:
Scanners help, but they’re not the same as thinking. Shift left should mean involving security thinking when requirements are written, when APIs are designed, when data flows are discussed. A scanner can tell you about known bad patterns; it can’t tell you if your business logic makes sense from a threat perspective.

⸻

[11:50]

Ethan:
What about supply chain risks? Dependencies, containers, build systems—this feels like an area where individual teams feel powerless.

Camila:
True, but there are still design choices. For example:
	•	Do you pin dependency versions?
	•	Do you know where your artifacts come from?
	•	Can you reproduce builds?

You don’t need to solve everything at once, but being intentional beats blind trust.

⸻

[13:15]

Ethan:
Let’s pause for a second.
Short break.

⸻

[13:45]

Ethan:
We’re back. Before the break, we talked about supply chain risk. I want to zoom out. How do you avoid security becoming the “department of no”?

Camila:
By aligning security goals with product goals. If security can explain how a control protects uptime, customer trust, or legal exposure, it stops being abstract. Also, security teams need to offer options, not just vetoes.

⸻

[15:10]

Ethan:
Have you seen examples where security advice was technically correct but practically unhelpful?

Camila:
All the time. Like saying “just encrypt everything” without discussing key management, performance, or recovery. Or demanding perfect controls for low-risk features. Good security is about prioritization, not perfection.

⸻

[16:30]

Ethan:
Let’s talk about incidents. What does security-by-design change when something actually goes wrong?

Camila:
It changes how surprised you are. If you’ve thought about failure modes, incidents become exercises in execution, not improvisation. Logging is already there. Access is scoped. You know who owns what. That’s a huge difference.

⸻

[17:55]

Ethan:
And post-incident reviews?

Camila:
They’re more productive. Instead of “who messed up,” the question becomes “which assumption failed?” That leads to design improvements instead of blame.

⸻

[18:40]

Ethan:
What advice would you give a small team without a dedicated security person?

Camila:
Start with basics:
	•	Draw your data flows.
	•	Identify your crown jewels.
	•	Decide what happens if an attacker gets in.

You don’t need fancy tools to ask good questions.

⸻

[20:05]

Ethan:
One thing I’ve noticed is that security language can be intimidating. Does that slow adoption?

Camila:
Absolutely. When security is full of acronyms and fear, people disengage. Clear language, examples, and empathy go a long way. Remember: most engineers want to build safe systems; they just need guidance that fits reality.

⸻

[21:30]

Ethan:
If you could remove one misconception about security, what would it be?

Camila:
That security is a separate phase. It’s not. It’s a property of the system, like reliability. You can’t bolt it on at the end and expect it to work.

⸻

[22:50]

Ethan:
What are you optimistic about right now in this space?

Camila:
I’m optimistic about teams treating security as part of engineering craft. More people are learning threat modeling, understanding tradeoffs, and pushing back on performative security that doesn’t actually reduce risk.

⸻

[24:15]

Ethan:
Before we wrap up, any practical habit listeners could adopt this week?

Camila:
Next time you design something, explicitly write down what you’re trusting. Networks, users, services—anything. If it’s not written, it’s an assumption. And assumptions are where incidents hide.

⸻

[25:45]

Ethan:
That’s a great note to end on. Camila, thanks for the conversation.

Camila:
Thanks, Ethan. This was fun.

⸻

[26:00]

Ethan:
That’s it for this episode of Practical Systems. If you enjoyed it, subscribe wherever you get your podcasts. We’ll be back next week.