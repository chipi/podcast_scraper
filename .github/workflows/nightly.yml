# Nightly comprehensive test suite with full metrics collection
# Runs all tests (unit + integration + e2e, including slow/ml_models)
# Generates comprehensive metrics, reports, and trend tracking
# See RFC-025: Test Metrics and Health Tracking (Layer 3)

name: Nightly Comprehensive Tests

on:
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: write  # For publishing metrics to gh-pages branch
  pages: write  # For publishing metrics to GitHub Pages
  id-token: write  # For GitHub Pages deployment

concurrency:
  group: "pages-metrics"
  cancel-in-progress: false  # Don't cancel, let metrics accumulate

jobs:
  # Comprehensive test suite with full metrics collection
  nightly-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 300  # 5 hours for comprehensive tests with production models (~261 min audio)
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for trend tracking

    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml

    - name: Cache ML models
      uses: actions/cache@v4
      id: cache-models
      with:
        path: |
          ~/.cache/whisper
          ~/.local/share/spacy
          ~/.cache/huggingface
        key: ml-models-${{ runner.os }}-v1
        restore-keys: |
          ml-models-${{ runner.os }}-

    - name: Install full dependencies (including ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket pytest-json-report

    - name: Install ffmpeg (required for Whisper)
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends ffmpeg

    - name: Install markdownlint
      run: npm install -g markdownlint-cli

    - name: Run code quality checks
      run: |
        echo "Running code quality checks..."
        make format-check
        make lint
        make lint-markdown
        make type
        make security
        make quality
      continue-on-error: false

    - name: Preload production ML models (if cache miss)
      if: steps.cache-models.outputs.cache-hit != 'true'
      run: make preload-ml-models-production

    - name: Create reports directory
      run: mkdir -p reports

    - name: Run comprehensive test suite with metrics
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run all tests (unit + integration + e2e) with comprehensive metrics collection
        # Note: E2E tests are limited to 1 episode per test (code quality validation)
        pytest tests/unit/ tests/integration/ tests/e2e/ \
          -v \
          -m "not nightly" \
          -n $(python3 -c "import os; print(max(1, (os.cpu_count() or 2) - 2))") \
          --durations=20 \
          --junitxml=reports/junit.xml \
          --json-report \
          --json-report-file=reports/pytest.json \
          --cov=podcast_scraper \
          --cov-report=xml:reports/coverage.xml \
          --cov-report=html:reports/coverage-html \
          --cov-report=term-missing \
          --disable-socket \
          --allow-hosts=127.0.0.1,localhost \
          --reruns 2 \
          --reruns-delay 1
      env:
        PACKAGE: podcast_scraper

    - name: Run nightly-only tests with production models
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run nightly-only tests (p01-p05 full suite, production models)
        # Uses production models: Whisper base, BART-large-cnn, LED-large-16384
        # Processes all 15 episodes across 5 podcasts
        make test-nightly || echo "âš ï¸  Nightly tests failed (non-blocking)"
      env:
        PACKAGE: podcast_scraper
      continue-on-error: true  # Don't fail nightly build if nightly tests fail (for now)

    - name: Enforce coverage threshold
      if: always()
      run: |
        echo "Checking combined coverage threshold (80%)..."
        make coverage-enforce || echo "âš ï¸  Coverage below 80% threshold"
      continue-on-error: true  # Report but don't fail nightly for coverage

    - name: Generate job summary
      if: always()
      run: |
        echo "# ðŸ“Š Nightly Test Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Extract metrics from pytest JSON report
        if [ -f reports/pytest.json ]; then
          TOTAL=$(jq -r '.summary.total // 0' reports/pytest.json)
          PASSED=$(jq -r '.summary.passed // 0' reports/pytest.json)
          FAILED=$(jq -r '.summary.failed // 0' reports/pytest.json)
          SKIPPED=$(jq -r '.summary.skipped // 0' reports/pytest.json)
          DURATION=$(jq -r '.duration // 0' reports/pytest.json)

          echo "## Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Tests**: $TOTAL" >> $GITHUB_STEP_SUMMARY
          echo "- **Passed**: âœ… $PASSED" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed**: âŒ $FAILED" >> $GITHUB_STEP_SUMMARY
          echo "- **Skipped**: â­ï¸ $SKIPPED" >> $GITHUB_STEP_SUMMARY
          echo "- **Duration**: ${DURATION}s" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract coverage from coverage.xml
        if [ -f reports/coverage.xml ]; then
          COVERAGE=$(python -c "import xml.etree.ElementTree as ET; tree = ET.parse('reports/coverage.xml'); root = tree.getroot(); print(f\"{float(root.attrib.get('line-rate', 0)) * 100:.1f}%\")" 2>/dev/null || echo "N/A")
          echo "## Coverage" >> $GITHUB_STEP_SUMMARY
          echo "- **Overall Coverage**: $COVERAGE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract slowest tests from pytest output
        if [ -f reports/junit.xml ]; then
          echo "## Slowest Tests (Top 10)" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          # Extract test durations from JUnit XML
          python3 -c "
        import xml.etree.ElementTree as ET
        import sys
        try:
            tree = ET.parse('reports/junit.xml')
            root = tree.getroot()
            tests = []
            for testcase in root.findall('.//testcase'):
                name = testcase.get('name', 'unknown')
                classname = testcase.get('classname', '')
                time = float(testcase.get('time', 0))
                full_name = f'{classname}::{name}' if classname else name
                tests.append((time, full_name))
            tests.sort(reverse=True)
            for time, name in tests[:10]:
                print(f'{time:.2f}s - {name}')
        except Exception as e:
            print(f'Error parsing JUnit XML: {e}', file=sys.stderr)
        " >> $GITHUB_STEP_SUMMARY 2>&1 || echo "Unable to parse JUnit XML" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
        fi

        # Check for flaky tests (tests that passed on rerun)
        if [ -f reports/pytest.json ]; then
          FLAKY_COUNT=$(jq -r '[.tests[] | select(.outcome == "passed" and .rerun == true)] | length' reports/pytest.json 2>/dev/null || echo "0")
          if [ "$FLAKY_COUNT" -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## âš ï¸ Flaky Tests Detected" >> $GITHUB_STEP_SUMMARY
            echo "- **Count**: $FLAKY_COUNT tests passed on rerun" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Flaky Test Names:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            jq -r '.tests[] | select(.outcome == "passed" and .rerun == true) | "\(.nodeid)"' reports/pytest.json 2>/dev/null >> $GITHUB_STEP_SUMMARY || echo "Unable to extract flaky test names"
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
        fi

        # Show alerts from metrics (if available)
        if [ -f metrics/latest.json ]; then
          ALERT_COUNT=$(jq -r '.alerts | length' metrics/latest.json 2>/dev/null || echo "0")
          if [ "$ALERT_COUNT" -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## ðŸš¨ Metric Alerts" >> $GITHUB_STEP_SUMMARY
            # Extract alerts with severity indicators
            jq -r '.alerts[] |
              if .severity == "error" then "ðŸ”´ **ERROR**: \(.message)"
              elif .severity == "warning" then "âš ï¸ **WARNING**: \(.message)"
              else "â„¹ï¸ **INFO**: \(.message)"
              end' metrics/latest.json 2>/dev/null >> $GITHUB_STEP_SUMMARY || echo "Unable to extract alerts"
          fi
        fi

    - name: Upload test artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: nightly-test-reports
        path: |
          reports/junit.xml
          reports/pytest.json
          reports/coverage.xml
          reports/coverage-html/
        retention-days: 90  # Extended retention for trend tracking

    - name: Load metrics history
      if: always()
      run: |
        # Create metrics directory if it doesn't exist
        mkdir -p metrics

        # Try to fetch existing history from gh-pages branch
        git fetch origin gh-pages:gh-pages 2>/dev/null || true
        if git show gh-pages:metrics/history.jsonl > metrics/history.jsonl 2>/dev/null; then
          echo "âœ… Loaded existing history"
        else
          echo "ðŸ“ Creating new history file"
          touch metrics/history.jsonl
        fi

    - name: Collect pipeline metrics
      if: always()
      run: |
        # Run minimal pipeline to collect performance metrics
        python scripts/collect_pipeline_metrics.py \
          --output reports/pipeline_metrics.json \
          --max-episodes 1 || echo "âš ï¸ Pipeline metrics collection failed (non-blocking)"
      continue-on-error: true  # Don't fail if pipeline metrics collection fails
    - name: Generate metrics JSON
      if: always()
      run: |
        # Coverage threshold (80%) is for combined coverage only
        python scripts/generate_metrics.py \
          --reports-dir reports \
          --output metrics/latest.json \
          --history metrics/history.jsonl \
          --pipeline-metrics reports/pipeline_metrics.json \
          --coverage-threshold 80

    - name: Update metrics history
      if: always()
      run: |
        # Append latest metrics to history (JSONL format)
        if [ -f metrics/latest.json ]; then
          echo "$(cat metrics/latest.json)" >> metrics/history.jsonl
          echo "âœ… Appended to history (total lines: $(wc -l < metrics/history.jsonl))"
        fi

    - name: Generate HTML dashboard
      if: always()
      run: |
        python scripts/generate_dashboard.py \
          --metrics metrics/latest.json \
          --history metrics/history.jsonl \
          --output metrics/index.html

    - name: Upload metrics artifact
      if: always()
      uses: actions/upload-pages-artifact@v3
      with:
        path: metrics/

    - name: Post-build cleanup
      if: always()
      run: |
        # Keep model caches for next run (they're cached via GitHub Actions cache)
        rm -rf .pytest_cache .mypy_cache .build dist

  # Deploy metrics to GitHub Pages
  deploy-metrics:
    if: always()
    runs-on: ubuntu-latest
    needs: nightly-tests
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Deploy metrics to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

