# Nightly comprehensive test suite with full metrics collection
# Runs all tests (unit + integration + e2e, including slow/ml_models)
# Generates comprehensive metrics, reports, and trend tracking
# See RFC-025: Test Metrics and Health Tracking (Layer 3)

name: Nightly Comprehensive Tests

on:
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: write  # For publishing metrics to gh-pages branch
  pages: write  # For publishing metrics to GitHub Pages
  id-token: write  # For GitHub Pages deployment

concurrency:
  group: "pages-metrics"
  cancel-in-progress: false  # Don't cancel, let metrics accumulate

jobs:
  # Comprehensive test suite with full metrics collection
  nightly-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 420  # 7 hours for comprehensive tests with production models (~300 min audio on CPU)
    env:
      # Ensure consistent Hugging Face cache paths across all steps
      # Must match the paths in actions/cache for proper cache hits
      HF_HOME: /home/runner/.cache/huggingface
      TRANSFORMERS_CACHE: /home/runner/.cache/huggingface/hub
      HF_HUB_CACHE: /home/runner/.cache/huggingface/hub
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for trend tracking

    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml

    - name: Cache ML models (production + test models for nightly)
      uses: actions/cache@v4
      id: cache-models
      with:
        path: |
          ~/.cache/whisper
          ~/.cache/huggingface
        # NOTE: spaCy removed from cache - it's installed as pip package via .[ml]
        # Fully content-based cache key - NO manual versioning!
        # Hash includes:
        #   - preload script (defines download logic)
        #   - config.py (defines model names like TEST_DEFAULT_WHISPER_MODEL)
        # Any change to models or preload logic = automatic cache invalidation
        key: ml-models-nightly-${{ runner.os }}-${{ hashFiles('scripts/preload_ml_models.py', 'src/podcast_scraper/config.py') }}
        restore-keys: |
          ml-models-nightly-${{ runner.os }}-

    - name: Install full dependencies (including ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket pytest-json-report

    - name: Install ffmpeg (required for Whisper)
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends ffmpeg

    - name: Install markdownlint
      run: npm install -g markdownlint-cli

    - name: Run code quality checks
      run: |
        echo "Running code quality checks..."
        make format-check
        make lint
        make lint-markdown
        make type
        make security
        make quality
      continue-on-error: false

    # IMPORTANT: Validate cache BEFORE deciding to preload
    # This catches incomplete caches that were saved from failed runs
    - name: Validate ML model cache
      id: validate-cache
      run: |
        echo "ðŸ” Validating ML model cache..."
        echo ""

        # Track validation results
        MISSING_MODELS=""
        MODELS_COMPLETE=true

        # Check Whisper models (actual files, not just directories)
        echo "ðŸ“¦ Whisper models:"
        for model in "tiny.en" "base.en"; do
          WHISPER_PATH="$HOME/.cache/whisper/${model}.pt"
          if [ -f "$WHISPER_PATH" ] && [ -s "$WHISPER_PATH" ]; then
            SIZE=$(du -h "$WHISPER_PATH" | cut -f1)
            echo "  âœ… $model ($SIZE)"
          else
            echo "  âŒ $model - MISSING or empty"
            MISSING_MODELS="$MISSING_MODELS whisper:$model"
            MODELS_COMPLETE=false
          fi
        done

        # NOTE: spaCy is a pip dependency (.[ml]), not cached - no validation needed

        # Check Hugging Face models (check for actual model files, not just directories)
        echo ""
        echo "ðŸ“¦ Hugging Face models:"
        HF_CACHE="$HOME/.cache/huggingface/hub"

        for model in "facebook/bart-base" "allenai/led-base-16384" "facebook/bart-large-cnn" "allenai/led-large-16384"; do
          MODEL_DIR="models--$(echo $model | sed 's|/|--|g')"
          MODEL_PATH="$HF_CACHE/$MODEL_DIR"
          # Check for snapshots directory with actual files (not just empty dirs)
          if [ -d "$MODEL_PATH/snapshots" ] && [ "$(find "$MODEL_PATH/snapshots" -type f 2>/dev/null | head -1)" ]; then
            SIZE=$(du -sh "$MODEL_PATH" | cut -f1)
            echo "  âœ… $model ($SIZE)"
          else
            echo "  âŒ $model - MISSING or incomplete"
            MISSING_MODELS="$MISSING_MODELS hf:$model"
            MODELS_COMPLETE=false
          fi
        done

        # Summary and set output for conditional preload
        echo ""
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        if [ "$MODELS_COMPLETE" = true ]; then
          echo "âœ… All required ML models are cached!"
          TOTAL_SIZE=$(du -sh "$HOME/.cache" 2>/dev/null | cut -f1 || echo "unknown")
          echo "ðŸ“Š Total cache size: $TOTAL_SIZE"
          echo "models_complete=true" >> $GITHUB_OUTPUT
        else
          echo "âš ï¸  Some models are missing. Will run preload..."
          echo "Missing:$MISSING_MODELS"
          echo "models_complete=false" >> $GITHUB_OUTPUT
        fi

    # Preload if validation found missing models (regardless of cache-hit)
    - name: Preload production ML models (if incomplete)
      if: steps.validate-cache.outputs.models_complete != 'true'
      run: |
        echo "ðŸ”„ Running model preload (cache was incomplete)..."
        make preload-ml-models-production

    # Final validation after preload (should always pass now)
    # Sets ML_MODELS_VALIDATED=true for subsequent steps to skip redundant checks
    - name: Final cache validation
      id: final-validation
      run: |
        echo "ðŸ” Final validation after preload..."
        echo ""

        CACHE_VALID=true

        # Quick check of all required models
        for model in "tiny.en" "base.en"; do
          if [ ! -f "$HOME/.cache/whisper/${model}.pt" ]; then
            echo "âŒ Whisper $model still missing!"
            CACHE_VALID=false
          fi
        done

        # NOTE: spaCy is pip dependency - no validation needed

        HF_CACHE="$HOME/.cache/huggingface/hub"
        for model in "facebook/bart-base" "allenai/led-base-16384" "facebook/bart-large-cnn" "allenai/led-large-16384"; do
          MODEL_DIR="models--$(echo $model | sed 's|/|--|g')"
          if [ ! -d "$HF_CACHE/$MODEL_DIR/snapshots" ]; then
            echo "âŒ HF $model still missing!"
            CACHE_VALID=false
          fi
        done

        if [ "$CACHE_VALID" = true ]; then
          echo "âœ… All models validated successfully!"
          TOTAL_SIZE=$(du -sh "$HOME/.cache" 2>/dev/null | cut -f1 || echo "unknown")
          echo "ðŸ“Š Total cache size: $TOTAL_SIZE"
          # Signal to pytest that cache is validated - tests can trust this
          echo "ML_MODELS_VALIDATED=true" >> $GITHUB_ENV
        else
          echo "âŒ FATAL: Models still missing after preload!"
          echo "This indicates a bug in preload_ml_models.py"
          exit 1
        fi

    - name: Create reports directory
      run: mkdir -p reports

    - name: Run comprehensive test suite with metrics
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run all tests (unit + integration + e2e) with comprehensive metrics collection
        # Note: E2E tests are limited to 1 episode per test (code quality validation)
        # Excludes LLM/OpenAI tests to avoid API costs (see issue #183)
        pytest tests/unit/ tests/integration/ tests/e2e/ \
          -v \
          -m "not nightly and not llm" \
          -n $(python3 -c "import os; print(max(1, (os.cpu_count() or 2) - 2))") \
          --durations=20 \
          --junitxml=reports/junit.xml \
          --json-report \
          --json-report-file=reports/pytest.json \
          --cov=podcast_scraper \
          --cov-report=xml:reports/coverage.xml \
          --cov-report=html:reports/coverage-html \
          --cov-report=term-missing \
          --disable-socket \
          --allow-hosts=127.0.0.1,localhost \
          --reruns 2 \
          --reruns-delay 1
      env:
        PACKAGE: podcast_scraper

    - name: Run nightly-only tests with production models
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run nightly-only tests (p01-p05 full suite, production models)
        # Uses production models: Whisper base, BART-large-cnn, LED-large-16384
        # Processes all 15 episodes across 5 podcasts
        make test-nightly || echo "âš ï¸  Nightly tests failed (non-blocking)"
      env:
        PACKAGE: podcast_scraper
      continue-on-error: true  # Don't fail nightly build if nightly tests fail (for now)

    - name: Enforce coverage threshold
      if: always()
      run: |
        echo "Checking combined coverage threshold (80%)..."
        make coverage-enforce || echo "âš ï¸  Coverage below 80% threshold"
      continue-on-error: true  # Report but don't fail nightly for coverage

    - name: Generate job summary
      if: always()
      run: |
        echo "# ðŸ“Š Nightly Test Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Extract metrics from pytest JSON report
        if [ -f reports/pytest.json ]; then
          TOTAL=$(jq -r '.summary.total // 0' reports/pytest.json)
          PASSED=$(jq -r '.summary.passed // 0' reports/pytest.json)
          FAILED=$(jq -r '.summary.failed // 0' reports/pytest.json)
          SKIPPED=$(jq -r '.summary.skipped // 0' reports/pytest.json)
          DURATION=$(jq -r '.duration // 0' reports/pytest.json)

          echo "## Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Tests**: $TOTAL" >> $GITHUB_STEP_SUMMARY
          echo "- **Passed**: âœ… $PASSED" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed**: âŒ $FAILED" >> $GITHUB_STEP_SUMMARY
          echo "- **Skipped**: â­ï¸ $SKIPPED" >> $GITHUB_STEP_SUMMARY
          echo "- **Duration**: ${DURATION}s" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract coverage from coverage.xml
        if [ -f reports/coverage.xml ]; then
          COVERAGE=$(python -c "import xml.etree.ElementTree as ET; tree = ET.parse('reports/coverage.xml'); root = tree.getroot(); print(f\"{float(root.attrib.get('line-rate', 0)) * 100:.1f}%\")" 2>/dev/null || echo "N/A")
          echo "## Coverage" >> $GITHUB_STEP_SUMMARY
          echo "- **Overall Coverage**: $COVERAGE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract slowest tests from pytest output
        if [ -f reports/junit.xml ]; then
          echo "## Slowest Tests (Top 10)" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          # Extract test durations from JUnit XML
          python3 -c "
        import xml.etree.ElementTree as ET
        import sys
        try:
            tree = ET.parse('reports/junit.xml')
            root = tree.getroot()
            tests = []
            for testcase in root.findall('.//testcase'):
                name = testcase.get('name', 'unknown')
                classname = testcase.get('classname', '')
                time = float(testcase.get('time', 0))
                full_name = f'{classname}::{name}' if classname else name
                tests.append((time, full_name))
            tests.sort(reverse=True)
            for time, name in tests[:10]:
                print(f'{time:.2f}s - {name}')
        except Exception as e:
            print(f'Error parsing JUnit XML: {e}', file=sys.stderr)
        " >> $GITHUB_STEP_SUMMARY 2>&1 || echo "Unable to parse JUnit XML" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
        fi

        # Check for flaky tests (tests that passed on rerun)
        if [ -f reports/pytest.json ]; then
          FLAKY_COUNT=$(jq -r '[.tests[] | select(.outcome == "passed" and .rerun == true)] | length' reports/pytest.json 2>/dev/null || echo "0")
          if [ "$FLAKY_COUNT" -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## âš ï¸ Flaky Tests Detected" >> $GITHUB_STEP_SUMMARY
            echo "- **Count**: $FLAKY_COUNT tests passed on rerun" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Flaky Test Names:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            jq -r '.tests[] | select(.outcome == "passed" and .rerun == true) | "\(.nodeid)"' reports/pytest.json 2>/dev/null >> $GITHUB_STEP_SUMMARY || echo "Unable to extract flaky test names"
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
        fi

        # Show alerts from metrics (if available)
        if [ -f metrics/latest.json ]; then
          ALERT_COUNT=$(jq -r '.alerts | length' metrics/latest.json 2>/dev/null || echo "0")
          if [ "$ALERT_COUNT" -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## ðŸš¨ Metric Alerts" >> $GITHUB_STEP_SUMMARY
            # Extract alerts with severity indicators
            jq -r '.alerts[] |
              if .severity == "error" then "ðŸ”´ **ERROR**: \(.message)"
              elif .severity == "warning" then "âš ï¸ **WARNING**: \(.message)"
              else "â„¹ï¸ **INFO**: \(.message)"
              end' metrics/latest.json 2>/dev/null >> $GITHUB_STEP_SUMMARY || echo "Unable to extract alerts"
          fi
        fi

    - name: Upload test artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: nightly-test-reports
        path: |
          reports/junit.xml
          reports/pytest.json
          reports/coverage.xml
          reports/coverage-html/
        retention-days: 90  # Extended retention for trend tracking

    - name: Load metrics history
      if: always()
      run: |
        # Create metrics directory if it doesn't exist
        mkdir -p metrics

        # Try to fetch existing history from gh-pages branch
        git fetch origin gh-pages:gh-pages 2>/dev/null || true
        if git show gh-pages:metrics/history.jsonl > metrics/history.jsonl 2>/dev/null; then
          echo "âœ… Loaded existing history"
        else
          echo "ðŸ“ Creating new history file"
          touch metrics/history.jsonl
        fi

    - name: Collect pipeline metrics
      if: always()
      run: |
        # Run minimal pipeline to collect performance metrics
        python scripts/collect_pipeline_metrics.py \
          --output reports/pipeline_metrics.json \
          --max-episodes 1 || echo "âš ï¸ Pipeline metrics collection failed (non-blocking)"
      continue-on-error: true  # Don't fail if pipeline metrics collection fails
    - name: Generate metrics JSON
      if: always()
      run: |
        # Coverage threshold (80%) is for combined coverage only
        python scripts/generate_metrics.py \
          --reports-dir reports \
          --output metrics/latest.json \
          --history metrics/history.jsonl \
          --pipeline-metrics reports/pipeline_metrics.json \
          --coverage-threshold 80

    - name: Update metrics history
      if: always()
      run: |
        # Append latest metrics to history (JSONL format)
        if [ -f metrics/latest.json ]; then
          echo "$(cat metrics/latest.json)" >> metrics/history.jsonl
          echo "âœ… Appended to history (total lines: $(wc -l < metrics/history.jsonl))"
        fi

    - name: Generate HTML dashboard
      if: always()
      run: |
        python scripts/generate_dashboard.py \
          --metrics metrics/latest.json \
          --history metrics/history.jsonl \
          --output metrics/index.html

    - name: Upload metrics artifact
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: nightly-metrics
        path: metrics/
        retention-days: 90

    - name: Post-build cleanup
      if: always()
      run: |
        # Keep model caches for next run (they're cached via GitHub Actions cache)
        rm -rf .pytest_cache .mypy_cache .build dist

  # NOTE: Metrics are no longer deployed separately to GitHub Pages.
  # The docs.yml workflow deploys the main documentation site.
  # Nightly metrics are available as workflow artifacts for download.
  # TODO: Integrate nightly metrics into docs site at /metrics/ subdirectory

