# Nightly comprehensive test suite with full metrics collection
# Runs all tests (unit + integration + e2e, including slow/ml_models)
# Generates comprehensive metrics, reports, and trend tracking
# See RFC-025: Test Metrics and Health Tracking (Layer 3)

name: Nightly Comprehensive Tests

on:
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: write  # For publishing metrics to gh-pages branch
  pages: write  # For publishing metrics to GitHub Pages
  id-token: write  # For GitHub Pages deployment

concurrency:
  group: "pages-metrics"
  cancel-in-progress: false  # Don't cancel, let metrics accumulate

jobs:
  # Comprehensive test suite with full metrics collection
  nightly-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 420  # 7 hours for comprehensive tests with production models (~300 min audio on CPU)
    env:
      # Ensure consistent Hugging Face cache paths across all steps
      # Must match the paths in actions/cache for proper cache hits
      HF_HOME: /home/runner/.cache/huggingface
      TRANSFORMERS_CACHE: /home/runner/.cache/huggingface/hub
      HF_HUB_CACHE: /home/runner/.cache/huggingface/hub
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for trend tracking

    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h

    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml

    - name: Cache ML models (production + test models for nightly)
      uses: actions/cache@v4
      id: cache-models
      with:
        path: |
          ~/.cache/whisper
          ~/.local/share/spacy
          ~/.cache/huggingface
        # Use separate cache key for nightly to include BOTH test and production models
        # Regular CI uses 'ml-models-*' with only test models
        # v5: v4 was saved with only Whisper models (192MB), before HF models downloaded
        #     GitHub cache is immutable, so must bump to v5 to get fresh cache
        # NOTE: No restore-keys! We want exact match or clean cache miss, not partial restore
        key: ml-models-nightly-${{ runner.os }}-v5

    - name: Install full dependencies (including ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket pytest-json-report

    - name: Install ffmpeg (required for Whisper)
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends ffmpeg

    - name: Install markdownlint
      run: npm install -g markdownlint-cli

    - name: Run code quality checks
      run: |
        echo "Running code quality checks..."
        make format-check
        make lint
        make lint-markdown
        make type
        make security
        make quality
      continue-on-error: false

    - name: Preload production ML models (if cache miss)
      if: steps.cache-models.outputs.cache-hit != 'true'
      run: make preload-ml-models-production

    - name: Validate ML model cache
      id: validate-cache
      run: |
        echo "ðŸ” Validating ML model cache..."
        echo ""

        # Track validation results
        MISSING_MODELS=""
        CACHE_VALID=true

        # Check Whisper models
        echo "ðŸ“¦ Whisper models:"
        for model in "tiny.en" "base.en"; do
          WHISPER_PATH="$HOME/.cache/whisper/${model}.pt"
          if [ -f "$WHISPER_PATH" ]; then
            SIZE=$(du -h "$WHISPER_PATH" | cut -f1)
            echo "  âœ… $model ($SIZE)"
          else
            echo "  âŒ $model - MISSING at $WHISPER_PATH"
            MISSING_MODELS="$MISSING_MODELS whisper:$model"
            CACHE_VALID=false
          fi
        done

        # Check spaCy models
        echo ""
        echo "ðŸ“¦ spaCy models:"
        SPACY_PATH="$HOME/.local/share/spacy/en_core_web_sm"
        if [ -d "$SPACY_PATH" ] || python -c "import spacy; spacy.load('en_core_web_sm')" 2>/dev/null; then
          echo "  âœ… en_core_web_sm"
        else
          echo "  âŒ en_core_web_sm - MISSING"
          MISSING_MODELS="$MISSING_MODELS spacy:en_core_web_sm"
          CACHE_VALID=false
        fi

        # Check Hugging Face models
        echo ""
        echo "ðŸ“¦ Hugging Face models:"
        HF_CACHE="$HOME/.cache/huggingface/hub"
        echo "  Cache directory: $HF_CACHE"
        echo "  Exists: $([ -d "$HF_CACHE" ] && echo "Yes" || echo "No")"

        for model in "facebook/bart-base" "allenai/led-base-16384" "facebook/bart-large-cnn" "allenai/led-large-16384"; do
          # Convert model name to cache directory format (facebook/bart-base -> models--facebook--bart-base)
          # Note: tr doesn't work for string replacement, must use sed
          MODEL_DIR="models--$(echo $model | sed 's|/|--|g')"
          MODEL_PATH="$HF_CACHE/$MODEL_DIR"
          if [ -d "$MODEL_PATH" ]; then
            SIZE=$(du -sh "$MODEL_PATH" | cut -f1)
            echo "  âœ… $model ($SIZE)"
          else
            echo "  âŒ $model - MISSING at $MODEL_PATH"
            MISSING_MODELS="$MISSING_MODELS hf:$model"
            CACHE_VALID=false
          fi
        done

        # Summary
        echo ""
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        if [ "$CACHE_VALID" = true ]; then
          echo "âœ… All required ML models are cached!"
          TOTAL_SIZE=$(du -sh "$HOME/.cache" 2>/dev/null | cut -f1 || echo "unknown")
          echo "ðŸ“Š Total cache size: $TOTAL_SIZE"
        else
          echo "âŒ CACHE VALIDATION FAILED!"
          echo "Missing models:$MISSING_MODELS"
          echo ""
          echo "This usually means:"
          echo "  1. Cache was created before models were downloaded"
          echo "  2. HF_HOME env var mismatch between preload and tests"
          echo "  3. Cache key needs to be bumped to force rebuild"
          echo ""
          echo "The preload step should have run. Check preload logs above."
          exit 1
        fi

    - name: Create reports directory
      run: mkdir -p reports

    - name: Run comprehensive test suite with metrics
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run all tests (unit + integration + e2e) with comprehensive metrics collection
        # Note: E2E tests are limited to 1 episode per test (code quality validation)
        # Excludes LLM/OpenAI tests to avoid API costs (see issue #183)
        pytest tests/unit/ tests/integration/ tests/e2e/ \
          -v \
          -m "not nightly and not llm" \
          -n $(python3 -c "import os; print(max(1, (os.cpu_count() or 2) - 2))") \
          --durations=20 \
          --junitxml=reports/junit.xml \
          --json-report \
          --json-report-file=reports/pytest.json \
          --cov=podcast_scraper \
          --cov-report=xml:reports/coverage.xml \
          --cov-report=html:reports/coverage-html \
          --cov-report=term-missing \
          --disable-socket \
          --allow-hosts=127.0.0.1,localhost \
          --reruns 2 \
          --reruns-delay 1
      env:
        PACKAGE: podcast_scraper

    - name: Run nightly-only tests with production models
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run nightly-only tests (p01-p05 full suite, production models)
        # Uses production models: Whisper base, BART-large-cnn, LED-large-16384
        # Processes all 15 episodes across 5 podcasts
        make test-nightly || echo "âš ï¸  Nightly tests failed (non-blocking)"
      env:
        PACKAGE: podcast_scraper
      continue-on-error: true  # Don't fail nightly build if nightly tests fail (for now)

    - name: Enforce coverage threshold
      if: always()
      run: |
        echo "Checking combined coverage threshold (80%)..."
        make coverage-enforce || echo "âš ï¸  Coverage below 80% threshold"
      continue-on-error: true  # Report but don't fail nightly for coverage

    - name: Generate job summary
      if: always()
      run: |
        echo "# ðŸ“Š Nightly Test Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Extract metrics from pytest JSON report
        if [ -f reports/pytest.json ]; then
          TOTAL=$(jq -r '.summary.total // 0' reports/pytest.json)
          PASSED=$(jq -r '.summary.passed // 0' reports/pytest.json)
          FAILED=$(jq -r '.summary.failed // 0' reports/pytest.json)
          SKIPPED=$(jq -r '.summary.skipped // 0' reports/pytest.json)
          DURATION=$(jq -r '.duration // 0' reports/pytest.json)

          echo "## Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Tests**: $TOTAL" >> $GITHUB_STEP_SUMMARY
          echo "- **Passed**: âœ… $PASSED" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed**: âŒ $FAILED" >> $GITHUB_STEP_SUMMARY
          echo "- **Skipped**: â­ï¸ $SKIPPED" >> $GITHUB_STEP_SUMMARY
          echo "- **Duration**: ${DURATION}s" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract coverage from coverage.xml
        if [ -f reports/coverage.xml ]; then
          COVERAGE=$(python -c "import xml.etree.ElementTree as ET; tree = ET.parse('reports/coverage.xml'); root = tree.getroot(); print(f\"{float(root.attrib.get('line-rate', 0)) * 100:.1f}%\")" 2>/dev/null || echo "N/A")
          echo "## Coverage" >> $GITHUB_STEP_SUMMARY
          echo "- **Overall Coverage**: $COVERAGE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract slowest tests from pytest output
        if [ -f reports/junit.xml ]; then
          echo "## Slowest Tests (Top 10)" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          # Extract test durations from JUnit XML
          python3 -c "
        import xml.etree.ElementTree as ET
        import sys
        try:
            tree = ET.parse('reports/junit.xml')
            root = tree.getroot()
            tests = []
            for testcase in root.findall('.//testcase'):
                name = testcase.get('name', 'unknown')
                classname = testcase.get('classname', '')
                time = float(testcase.get('time', 0))
                full_name = f'{classname}::{name}' if classname else name
                tests.append((time, full_name))
            tests.sort(reverse=True)
            for time, name in tests[:10]:
                print(f'{time:.2f}s - {name}')
        except Exception as e:
            print(f'Error parsing JUnit XML: {e}', file=sys.stderr)
        " >> $GITHUB_STEP_SUMMARY 2>&1 || echo "Unable to parse JUnit XML" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
        fi

        # Check for flaky tests (tests that passed on rerun)
        if [ -f reports/pytest.json ]; then
          FLAKY_COUNT=$(jq -r '[.tests[] | select(.outcome == "passed" and .rerun == true)] | length' reports/pytest.json 2>/dev/null || echo "0")
          if [ "$FLAKY_COUNT" -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## âš ï¸ Flaky Tests Detected" >> $GITHUB_STEP_SUMMARY
            echo "- **Count**: $FLAKY_COUNT tests passed on rerun" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Flaky Test Names:" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            jq -r '.tests[] | select(.outcome == "passed" and .rerun == true) | "\(.nodeid)"' reports/pytest.json 2>/dev/null >> $GITHUB_STEP_SUMMARY || echo "Unable to extract flaky test names"
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
        fi

        # Show alerts from metrics (if available)
        if [ -f metrics/latest.json ]; then
          ALERT_COUNT=$(jq -r '.alerts | length' metrics/latest.json 2>/dev/null || echo "0")
          if [ "$ALERT_COUNT" -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## ðŸš¨ Metric Alerts" >> $GITHUB_STEP_SUMMARY
            # Extract alerts with severity indicators
            jq -r '.alerts[] |
              if .severity == "error" then "ðŸ”´ **ERROR**: \(.message)"
              elif .severity == "warning" then "âš ï¸ **WARNING**: \(.message)"
              else "â„¹ï¸ **INFO**: \(.message)"
              end' metrics/latest.json 2>/dev/null >> $GITHUB_STEP_SUMMARY || echo "Unable to extract alerts"
          fi
        fi

    - name: Upload test artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: nightly-test-reports
        path: |
          reports/junit.xml
          reports/pytest.json
          reports/coverage.xml
          reports/coverage-html/
        retention-days: 90  # Extended retention for trend tracking

    - name: Load metrics history
      if: always()
      run: |
        # Create metrics directory if it doesn't exist
        mkdir -p metrics

        # Try to fetch existing history from gh-pages branch
        git fetch origin gh-pages:gh-pages 2>/dev/null || true
        if git show gh-pages:metrics/history.jsonl > metrics/history.jsonl 2>/dev/null; then
          echo "âœ… Loaded existing history"
        else
          echo "ðŸ“ Creating new history file"
          touch metrics/history.jsonl
        fi

    - name: Collect pipeline metrics
      if: always()
      run: |
        # Run minimal pipeline to collect performance metrics
        python scripts/collect_pipeline_metrics.py \
          --output reports/pipeline_metrics.json \
          --max-episodes 1 || echo "âš ï¸ Pipeline metrics collection failed (non-blocking)"
      continue-on-error: true  # Don't fail if pipeline metrics collection fails
    - name: Generate metrics JSON
      if: always()
      run: |
        # Coverage threshold (80%) is for combined coverage only
        python scripts/generate_metrics.py \
          --reports-dir reports \
          --output metrics/latest.json \
          --history metrics/history.jsonl \
          --pipeline-metrics reports/pipeline_metrics.json \
          --coverage-threshold 80

    - name: Update metrics history
      if: always()
      run: |
        # Append latest metrics to history (JSONL format)
        if [ -f metrics/latest.json ]; then
          echo "$(cat metrics/latest.json)" >> metrics/history.jsonl
          echo "âœ… Appended to history (total lines: $(wc -l < metrics/history.jsonl))"
        fi

    - name: Generate HTML dashboard
      if: always()
      run: |
        python scripts/generate_dashboard.py \
          --metrics metrics/latest.json \
          --history metrics/history.jsonl \
          --output metrics/index.html

    - name: Upload metrics artifact
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: nightly-metrics
        path: metrics/
        retention-days: 90

    - name: Post-build cleanup
      if: always()
      run: |
        # Keep model caches for next run (they're cached via GitHub Actions cache)
        rm -rf .pytest_cache .mypy_cache .build dist

  # NOTE: Metrics are no longer deployed separately to GitHub Pages.
  # The docs.yml workflow deploys the main documentation site.
  # Nightly metrics are available as workflow artifacts for download.
  # TODO: Integrate nightly metrics into docs site at /metrics/ subdirectory

