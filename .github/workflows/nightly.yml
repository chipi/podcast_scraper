# Nightly comprehensive test suite with full metrics collection
# Runs all tests (unit + integration + e2e, including slow/ml_models)
# Generates comprehensive metrics, reports, and trend tracking
# See RFC-025: Test Metrics and Health Tracking (Layer 3)
#
# Note: Linter warnings about "Unable to resolve action" for actions/checkout@v4
# and actions/cache@v4 are false positives - these are standard GitHub Actions
# that exist in the marketplace but the linter cannot resolve them offline.
#
# Structure mirrors python-app.yml with separate jobs for isolation:
# Dependency flow:
# 1. lint, build, preload-ml-models-nightly run in parallel (no deps)
# 2. security-quality and test-unit run in parallel (both gate: lint, build)
# 3. test-integration and test-e2e run in parallel (both gate: preload-ml-models-nightly + test-unit)
# 4. nightly-only-tests gates: test-unit, test-integration, test-e2e
# 5. nightly-metrics gates: security-quality, nightly-only-tests
# 6. nightly-docs gates nightly-metrics (very last step, final validation)
#
# Jobs:
# - preload-ml-models-nightly: Preload and validate ML models
# - nightly-lint: Fast lint checks (format, lint, markdown, type)
# - nightly-security-quality: Security and quality checks (security, complexity, deadcode, docstrings, spelling)
# - nightly-docs: Documentation build
# - nightly-build: Package build
# - nightly-test-unit: Unit tests
# - nightly-test-integration: Integration tests
# - nightly-test-e2e: E2E tests (multi_episode mode - same as regular CI)
# - nightly-only-tests: Nightly-specific tests with production models
# - nightly-metrics: Collect metrics and generate dashboard

name: Nightly Comprehensive Tests

on:
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggering
  push:
    branches:
      - 'release/**'  # Run on all release branches

permissions:
  contents: write  # For publishing metrics to gh-pages branch
  pages: write  # For publishing metrics to GitHub Pages
  id-token: write  # For GitHub Pages deployment

concurrency:
  group: "pages-metrics"
  cancel-in-progress: false  # Don't cancel, let metrics accumulate

jobs:
  # ===========================================================================
  # Preload ML models - runs first, validates cache
  # ===========================================================================
  preload-ml-models-nightly:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # May need to download production models
    env:
      HF_HOME: /home/runner/.cache/huggingface
      HF_HUB_CACHE: /home/runner/.cache/huggingface/hub
    steps:
    - uses: actions/checkout@v4

    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h

    - name: Cache ML models (production + test models for nightly)
      uses: actions/cache@v4
      id: cache-models
      with:
        path: |
          ~/.cache/whisper
          ~/.cache/huggingface
        # Fully content-based cache key
        key: ml-models-nightly-${{ runner.os }}-${{ hashFiles('scripts/preload_ml_models.py', 'src/podcast_scraper/config.py') }}
        restore-keys: |
          ml-models-nightly-${{ runner.os }}-

    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml

    - name: Install full dependencies (including ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]

    - name: Validate ML model cache
      id: validate-cache
      run: |
        echo "ðŸ” Validating ML model cache..."
        MISSING_MODELS=""
        MODELS_COMPLETE=true

        # Check Whisper models
        echo "ðŸ“¦ Whisper models:"
        for model in "tiny.en" "base.en"; do
          WHISPER_PATH="$HOME/.cache/whisper/${model}.pt"
          if [ -f "$WHISPER_PATH" ] && [ -s "$WHISPER_PATH" ]; then
            SIZE=$(du -h "$WHISPER_PATH" | cut -f1)
            echo "  âœ… $model ($SIZE)"
          else
            echo "  âŒ $model - MISSING or empty"
            MISSING_MODELS="$MISSING_MODELS whisper:$model"
            MODELS_COMPLETE=false
          fi
        done

        # Check Hugging Face models
        echo ""
        echo "ðŸ“¦ Hugging Face models:"
        HF_CACHE="$HOME/.cache/huggingface/hub"
        for model in "facebook/bart-base" "allenai/led-base-16384" "facebook/bart-large-cnn" "allenai/led-large-16384"; do
          MODEL_DIR="models--$(echo $model | sed 's|/|--|g')"
          MODEL_PATH="$HF_CACHE/$MODEL_DIR"
          if [ -d "$MODEL_PATH/snapshots" ] && [ "$(find "$MODEL_PATH/snapshots" -type f 2>/dev/null | head -1)" ]; then
            SIZE=$(du -sh "$MODEL_PATH" | cut -f1)
            echo "  âœ… $model ($SIZE)"
          else
            echo "  âŒ $model - MISSING or incomplete"
            MISSING_MODELS="$MISSING_MODELS hf:$model"
            MODELS_COMPLETE=false
          fi
        done

        echo ""
        if [ "$MODELS_COMPLETE" = true ]; then
          echo "âœ… All required ML models are cached!"
          echo "models_complete=true" >> $GITHUB_OUTPUT
        else
          echo "âš ï¸  Some models are missing. Will run preload..."
          echo "models_complete=false" >> $GITHUB_OUTPUT
        fi

    - name: Preload production ML models (if incomplete)
      if: steps.validate-cache.outputs.models_complete != 'true'
      run: make preload-ml-models-production

    - name: Final cache validation
      run: |
        echo "ðŸ” Final validation..."
        CACHE_VALID=true

        for model in "tiny.en" "base.en"; do
          if [ ! -f "$HOME/.cache/whisper/${model}.pt" ]; then
            echo "âŒ Whisper $model still missing!"
            CACHE_VALID=false
          fi
        done

        HF_CACHE="$HOME/.cache/huggingface/hub"
        for model in "facebook/bart-base" "allenai/led-base-16384" "facebook/bart-large-cnn" "allenai/led-large-16384"; do
          MODEL_DIR="models--$(echo $model | sed 's|/|--|g')"
          if [ ! -d "$HF_CACHE/$MODEL_DIR/snapshots" ]; then
            echo "âŒ HF $model still missing!"
            CACHE_VALID=false
          fi
        done

        if [ "$CACHE_VALID" = true ]; then
          echo "âœ… All models validated successfully!"
          TOTAL_SIZE=$(du -sh "$HOME/.cache" 2>/dev/null | cut -f1 || echo "unknown")
          echo "ðŸ“Š Total cache size: $TOTAL_SIZE"
        else
          echo "âŒ FATAL: Models still missing after preload!"
          exit 1
        fi

  # ===========================================================================
  # Code quality checks
  # ===========================================================================
  # Fast lint checks - blocks tests to catch issues early
  nightly-lint:
    runs-on: ubuntu-latest
    timeout-minutes: 10  # Fast checks: format, lint, markdown, type
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: "20"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]

    - name: Install markdownlint
      run: npm install -g markdownlint-cli

    - name: Run fast lint checks
      run: |
        echo "Running fast lint checks..."
        make format-check
        make lint
        make lint-markdown
        make type

  # Security and quality checks - gated by lint/build, runs independently to nightly-metrics (does not gate tests)
  nightly-security-quality:
    runs-on: ubuntu-latest
    timeout-minutes: 20  # Slower checks: security, complexity, deadcode, docstrings, spelling
    needs: [nightly-lint, nightly-build]  # Wait for lint, build to pass (docs removed - doesn't gate security)
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]

    - name: Run security checks
      run: |
        echo "Running security checks..."
        make security

    - name: Run quality checks
      run: |
        echo "Running quality checks..."
        make quality

  # ===========================================================================
  # Dependency analysis - module coupling visualization and circular import detection
  # Runs in parallel with security-quality, generates dependency graphs for tracking
  # ===========================================================================
  nightly-deps-analysis:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [nightly-lint, nightly-build]  # Wait for lint, build to pass
    continue-on-error: true  # Don't fail nightly if dependency analysis has issues
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]

    - name: Generate dependency graph
      run: |
        echo "Generating module dependency graphs..."
        make deps-graph
        make deps-graph-full

    - name: Check for circular imports
      run: |
        echo "Checking for circular imports..."
        make deps-check-cycles

    - name: Run dependency analysis
      run: |
        echo "Running dependency analysis..."
        python scripts/analyze_dependencies.py --report

    - name: Upload dependency artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: nightly-deps-analysis
        path: |
          reports/deps*.svg
          reports/deps*.json
        retention-days: 90

  # ===========================================================================
  # Documentation build - validates docs can be built correctly
  # Runs as very last step after all tests and metrics pass (final validation)
  # Docs deployment happens in docs.yml workflow (separate, already gated properly)
  # ===========================================================================
  nightly-docs:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [nightly-metrics]  # Very last step - validates docs after all tests and metrics pass
    steps:
    - uses: actions/checkout@v4

    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h

    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: |
          docs/requirements.txt
          pyproject.toml

    - name: Install doc dependencies (includes ML for mkdocstrings)
      run: |
        python -m pip install --upgrade pip
        pip install -r docs/requirements.txt
        pip install -e .[ml]

    - name: Build docs
      run: make docs

  # ===========================================================================
  # Package build - validates package can be built correctly
  # Builds source distribution (sdist) and wheel distribution
  # Catches packaging issues (pyproject.toml errors, missing files, etc.)
  # Fast check (~2-3 min) that gates test-unit to catch packaging issues early
  # ===========================================================================
  nightly-build:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml

    - name: Install build tools
      run: |
        python -m pip install --upgrade pip
        pip install build

    - name: Build package
      run: make build

  # ===========================================================================
  # Unit tests - isolated job (no ML dependencies, runs in parallel with preload)
  # ===========================================================================
  nightly-test-unit:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [nightly-lint, nightly-build]  # Wait for lint, build to pass (docs removed - doesn't gate unit tests)
    steps:
    - uses: actions/checkout@v4

    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h

    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml

    - name: Install dev dependencies (no ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
        pip install pytest-json-report

    - name: Verify unit tests can import without ML dependencies
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        python scripts/check_unit_test_imports.py
      env:
        PACKAGE: podcast_scraper

    - name: Run unit tests with coverage (parallel execution)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        mkdir -p reports
        # Run tests with coverage and capture output and exit code
        set +e
        OUTPUT=$(pytest tests/unit/ -v --tb=short -n $(python3 -c "import os; print(max(1, (os.cpu_count() or 2) - 2))") --json-report --json-report-file=reports/pytest-unit.json --cov=podcast_scraper --cov-report=xml:reports/coverage-unit.xml --cov-report=term-missing --durations=20 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 50 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 50 unit tests"
          exit 1
        fi
        # Exit with pytest's exit code
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE"
          exit $PYTEST_EXIT_CODE
        fi
      env:
        PACKAGE: podcast_scraper

    - name: Upload unit test reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: nightly-unit-reports
        path: reports/
        retention-days: 30

  # ===========================================================================
  # Integration tests - isolated job
  # ===========================================================================
  nightly-test-integration:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [preload-ml-models-nightly, nightly-test-unit]  # Wait for model preload and unit tests to pass
    env:
      HF_HOME: /home/runner/.cache/huggingface
      HF_HUB_CACHE: /home/runner/.cache/huggingface/hub
      ML_MODELS_VALIDATED: "true"
    steps:
    - uses: actions/checkout@v4

    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h

    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml

    - name: Cache ML models
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/whisper
          ~/.cache/huggingface
        key: ml-models-nightly-${{ runner.os }}-${{ hashFiles('scripts/preload_ml_models.py', 'src/podcast_scraper/config.py') }}
        restore-keys: |
          ml-models-nightly-${{ runner.os }}-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket pytest-json-report

    - name: Install ffmpeg
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends ffmpeg

    - name: Run integration tests with coverage (network guard, parallel)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        mkdir -p reports
        # Run integration tests with coverage and network guard
        set +e
        OUTPUT=$(pytest tests/integration/ -v -m "integration and not llm" -n $(python3 -c "import os; print(max(1, (os.cpu_count() or 2) - 2))") --json-report --json-report-file=reports/pytest-integration.json --cov=podcast_scraper --cov-append --cov-report=xml:reports/coverage-integration.xml --cov-report=term-missing --cov-fail-under=40 --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 --durations=20 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 50 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 50 integration tests"
          exit 1
        fi
        # Exit with pytest's exit code
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE"
          exit $PYTEST_EXIT_CODE
        fi

    - name: Upload integration test reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: nightly-integration-reports
        path: reports/
        retention-days: 30

  # ===========================================================================
  # E2E tests - isolated job, multi_episode mode (same as regular CI)
  # ===========================================================================
  nightly-test-e2e:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: [preload-ml-models-nightly, nightly-test-unit]  # Wait for model preload and unit tests to pass
    env:
      HF_HOME: /home/runner/.cache/huggingface
      HF_HUB_CACHE: /home/runner/.cache/huggingface/hub
      ML_MODELS_VALIDATED: "true"
      # E2E test mode - matches regular CI (python-app.yml)
      E2E_TEST_MODE: "multi_episode"
    steps:
    - uses: actions/checkout@v4

    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h

    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml

    - name: Cache ML models
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/whisper
          ~/.cache/huggingface
        key: ml-models-nightly-${{ runner.os }}-${{ hashFiles('scripts/preload_ml_models.py', 'src/podcast_scraper/config.py') }}
        restore-keys: |
          ml-models-nightly-${{ runner.os }}-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket pytest-json-report

    - name: Install ffmpeg
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends ffmpeg

    - name: Run E2E tests with coverage (network guard, serial then parallel)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        mkdir -p reports
        # Serial tests first, then parallel (matches regular CI pattern)
        set +e
        # Run serial tests first with coverage
        OUTPUT_SERIAL=$(E2E_TEST_MODE=multi_episode pytest tests/e2e/ -v -m "e2e and serial and not nightly and not llm" --json-report --json-report-file=reports/pytest-e2e-serial.json --cov=podcast_scraper --cov-append --cov-report=xml:reports/coverage-e2e.xml --cov-report=term-missing --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 2>&1 || true)
        echo "$OUTPUT_SERIAL"
        # Run parallel tests with coverage (append to same file)
        OUTPUT=$(E2E_TEST_MODE=multi_episode pytest tests/e2e/ -v -m "e2e and not serial and not nightly and not llm" -n $(python3 -c "import os; print(max(1, (os.cpu_count() or 2) - 2))") --json-report --json-report-file=reports/pytest-e2e.json --cov=podcast_scraper --cov-append --cov-report=xml:reports/coverage-e2e.xml --cov-report=term-missing --cov-fail-under=40 --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 --durations=20 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 50 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 50 E2E tests"
          exit 1
        fi
        # Exit with pytest's exit code
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE"
          exit $PYTEST_EXIT_CODE
        fi

    - name: Upload E2E test reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: nightly-e2e-reports
        path: reports/
        retention-days: 30

  # ===========================================================================
  # Nightly-only tests - production models, full podcast suite
  # Only runs after unit, integration and E2E tests pass (expensive, run as final stage)
  # ===========================================================================
  nightly-only-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours for production model tests
    needs: [nightly-test-unit, nightly-test-integration, nightly-test-e2e]
    env:
      HF_HOME: /home/runner/.cache/huggingface
      HF_HUB_CACHE: /home/runner/.cache/huggingface/hub
      ML_MODELS_VALIDATED: "true"
      # Nightly mode uses p01-p05 podcasts with production models
      E2E_TEST_MODE: "nightly"
    steps:
    - uses: actions/checkout@v4

    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h

    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml

    - name: Cache ML models
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/whisper
          ~/.cache/huggingface
        key: ml-models-nightly-${{ runner.os }}-${{ hashFiles('scripts/preload_ml_models.py', 'src/podcast_scraper/config.py') }}
        restore-keys: |
          ml-models-nightly-${{ runner.os }}-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket pytest-json-report

    - name: Install ffmpeg
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends ffmpeg

    - name: Create reports directory
      run: mkdir -p reports

    - name: Run nightly-only tests with production models
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Uses production models: Whisper base.en, BART-large-cnn, LED-large-16384
        # Processes all 15 episodes across 5 podcasts (p01-p05)
        make test-nightly || echo "âš ï¸  Nightly tests failed (non-blocking)"
      continue-on-error: true

    - name: Upload nightly-only test reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: nightly-only-reports
        path: reports/
        retention-days: 30

  # ===========================================================================
  # Metrics collection and dashboard generation
  # ===========================================================================
  nightly-metrics:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [nightly-lint, nightly-security-quality, nightly-docs, nightly-build, nightly-test-unit, nightly-test-integration, nightly-test-e2e, nightly-only-tests]  # Wait for all jobs to complete before generating summary
    if: always()
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml

    - name: Install minimal dependencies
      run: |
        python -m pip install --upgrade pip
        # Only need coverage for combining reports - no other deps needed
        # Scripts (generate_metrics.py, generate_dashboard.py) use only stdlib
        pip install coverage[toml]

    - name: Create directories
      run: |
        mkdir -p reports
        mkdir -p metrics

    - name: Download all test reports
      uses: actions/download-artifact@v4
      with:
        path: downloaded-reports
        pattern: nightly-*-reports
        merge-multiple: false

    - name: Merge test reports
      run: |
        # Copy all reports to a single directory
        find downloaded-reports -name "*.json" -exec cp {} reports/ \;
        find downloaded-reports -name "*.xml" -exec cp {} reports/ \;
        ls -la reports/

        # Merge pytest JSON reports if multiple exist
        if ls reports/pytest-*.json 1>/dev/null 2>&1; then
          echo "Merging pytest JSON reports..."
          python3 << 'EOF'
        import json
        from pathlib import Path

        json_files = list(Path('reports').glob('pytest-*.json'))
        if not json_files:
            print("No pytest JSON files found")
            exit(0)

        merged_summary = {"total": 0, "passed": 0, "failed": 0, "skipped": 0}
        merged_tests = []
        total_duration = 0.0

        for json_file in json_files:
            try:
                with open(json_file) as f:
                    data = json.load(f)
                summary = data.get("summary", {})
                merged_summary["total"] += summary.get("total", 0)
                merged_summary["passed"] += summary.get("passed", 0)
                merged_summary["failed"] += summary.get("failed", 0)
                merged_summary["skipped"] += summary.get("skipped", 0)
                total_duration += data.get("duration", 0)
                merged_tests.extend(data.get("tests", []))
                print(f"  âœ… {json_file}")
            except Exception as e:
                print(f"  âš ï¸ {json_file}: {e}")

        merged_data = {
            "summary": merged_summary,
            "duration": total_duration,
            "tests": merged_tests
        }

        with open('reports/pytest.json', 'w') as f:
            json.dump(merged_data, f, indent=2)
        print(f"âœ… Merged pytest JSON: {merged_summary['total']} tests, {total_duration:.1f}s")
        EOF
        fi

        # Merge JUnit XML reports if multiple exist
        if ls reports/junit-*.xml 1>/dev/null 2>&1; then
          echo "Merging JUnit XML reports..."
          python3 << 'EOF'
        import xml.etree.ElementTree as ET
        from pathlib import Path

        xml_files = list(Path('reports').glob('junit-*.xml'))
        if not xml_files:
            print("No JUnit XML files found")
            exit(0)

        # Create base testsuites element
        root = ET.Element('testsuites')
        root.set('tests', '0')
        root.set('failures', '0')
        root.set('time', '0')

        total_tests = 0
        total_failures = 0
        total_time = 0.0

        for xml_file in xml_files:
            try:
                tree = ET.parse(xml_file)
                file_root = tree.getroot()
                # Handle both testsuites and testsuite elements
                for testsuite in file_root.findall('.//testsuite'):
                    root.append(testsuite)
                    total_tests += int(testsuite.get('tests', 0))
                    total_failures += int(testsuite.get('failures', 0))
                    total_time += float(testsuite.get('time', 0))
                print(f"  âœ… {xml_file}")
            except Exception as e:
                print(f"  âš ï¸ {xml_file}: {e}")

        root.set('tests', str(total_tests))
        root.set('failures', str(total_failures))
        root.set('time', str(total_time))

        tree = ET.ElementTree(root)
        tree.write('reports/junit.xml', encoding='utf-8', xml_declaration=True)
        print(f"âœ… Merged JUnit XML: {total_tests} tests, {total_failures} failures, {total_time:.1f}s")
        EOF
        fi

        # Properly merge coverage reports from all test jobs
        if ls reports/coverage-*.xml 1>/dev/null 2>&1; then
          echo "Merging coverage reports..."
          python3 << 'EOF'
        import xml.etree.ElementTree as ET
        from pathlib import Path

        def parse_coverage_xml(filepath):
            tree = ET.parse(filepath)
            root = tree.getroot()
            coverage_data = {}
            for package in root.findall('.//package'):
                for cls in package.findall('.//class'):
                    filename = cls.get('filename', '')
                    if not filename:
                        continue
                    if filename not in coverage_data:
                        coverage_data[filename] = {'hits': set(), 'misses': set()}
                    for line in cls.findall('.//line'):
                        line_num = int(line.get('number', 0))
                        hits = int(line.get('hits', 0))
                        if hits > 0:
                            coverage_data[filename]['hits'].add(line_num)
                            coverage_data[filename]['misses'].discard(line_num)
                        elif line_num not in coverage_data[filename]['hits']:
                            coverage_data[filename]['misses'].add(line_num)
            return coverage_data, root

        xml_files = list(Path('reports').glob('coverage-*.xml'))
        if not xml_files:
            print("No coverage files found")
            exit(0)

        print(f"Merging {len(xml_files)} coverage files...")
        merged_coverage = {}
        base_root = None

        for xml_file in xml_files:
            try:
                coverage_data, root = parse_coverage_xml(xml_file)
                if base_root is None:
                    base_root = root
                for filename, data in coverage_data.items():
                    if filename not in merged_coverage:
                        merged_coverage[filename] = {'hits': set(), 'misses': set()}
                    merged_coverage[filename]['hits'].update(data['hits'])
                    for miss in data['misses']:
                        if miss not in merged_coverage[filename]['hits']:
                            merged_coverage[filename]['misses'].add(miss)
                print(f"  âœ… {xml_file}")
            except Exception as e:
                print(f"  âš ï¸ {xml_file}: {e}")

        total_lines = sum(len(d['hits']) + len(d['misses']) for d in merged_coverage.values())
        covered_lines = sum(len(d['hits']) for d in merged_coverage.values())
        coverage_pct = (covered_lines / total_lines * 100) if total_lines > 0 else 0
        print(f"ðŸ“ˆ Unified: {coverage_pct:.1f}% ({covered_lines}/{total_lines})")

        if base_root is not None:
            base_root.set('line-rate', str(covered_lines / total_lines if total_lines > 0 else 0))
            ET.ElementTree(base_root).write('reports/coverage.xml', encoding='unicode', xml_declaration=True)
            print("âœ… Written to reports/coverage.xml")
        EOF
        fi

    - name: Load metrics history
      run: |
        git fetch origin gh-pages:gh-pages 2>/dev/null || true
        if git show gh-pages:metrics/history-nightly.jsonl > metrics/history-nightly.jsonl 2>/dev/null; then
          echo "âœ… Loaded existing nightly history"
        else
          echo "ðŸ“ Creating new nightly history file"
          touch metrics/history-nightly.jsonl
        fi

    - name: Generate code quality metrics
      run: |
        mkdir -p reports
        # Generate complexity metrics for dashboard
        radon cc src/podcast_scraper/ -a -s --total-average --json > reports/complexity.json 2>/dev/null || echo '{"total_average": 0}' > reports/complexity.json
        radon mi src/podcast_scraper/ -s --json > reports/maintainability.json 2>/dev/null || echo '[]' > reports/maintainability.json
        interrogate src/podcast_scraper/ --quiet --output-format json > reports/docstrings.json 2>/dev/null || echo '{"coverage_percent": 0}' > reports/docstrings.json
        # Vulture dead code detection
        vulture src/podcast_scraper/ --min-confidence 60 --sort-by-size --json > reports/vulture.json 2>/dev/null || echo '[]' > reports/vulture.json
        # Codespell spelling check
        codespell --quiet --skip="*.pyc,*.pyo,*.pyd,*.so,*.egg,*.whl,.git,.venv,__pycache__,.pytest_cache" --write-changes=reports/codespell.txt 2>/dev/null || touch reports/codespell.txt

    - name: Create JUnit XML fallback
      run: |
        if [ ! -f reports/junit.xml ]; then
          echo "âš ï¸ No JUnit XML found, creating minimal fallback"
          python3 -c "import xml.etree.ElementTree as ET; root = ET.Element('testsuites'); root.set('tests', '0'); root.set('failures', '0'); root.set('time', '0'); tree = ET.ElementTree(root); tree.write('reports/junit.xml', encoding='utf-8', xml_declaration=True)"
        fi

    - name: Create pytest JSON fallback
      run: |
        if [ ! -f reports/pytest.json ]; then
          echo "âš ï¸ No pytest JSON found, creating minimal fallback"
          echo '{"summary": {"total": 0, "passed": 0, "failed": 0, "skipped": 0}, "duration": 0, "tests": []}' > reports/pytest.json
        fi

    - name: Collect pipeline metrics
      run: |
        python scripts/collect_pipeline_metrics.py \
          --output reports/pipeline_metrics.json \
          --max-episodes 1 || echo "âš ï¸ Pipeline metrics collection failed (non-blocking)"
      continue-on-error: true

    - name: Generate metrics JSON
      run: |
        python scripts/generate_metrics.py \
          --reports-dir reports \
          --output metrics/latest-nightly.json \
          --history metrics/history-nightly.jsonl \
          --pipeline-metrics reports/pipeline_metrics.json \
          --coverage-threshold 80

    - name: Validate metrics JSON
      run: |
        if [ ! -f metrics/latest-nightly.json ]; then
          echo "âŒ Error: metrics/latest-nightly.json not found"
          exit 1
        fi
        python3 -c "import json, sys; json.load(open('metrics/latest-nightly.json'))" || exit 1
        echo "âœ… Metrics JSON is valid"

    - name: Debug metrics files
      if: failure()
      run: |
        echo "=== Metrics Files ==="
        ls -la metrics/ || echo "No metrics directory"
        echo ""
        echo "=== Latest JSON (first 30 lines) ==="
        head -30 metrics/latest-nightly.json || echo "No latest-nightly.json"
        echo ""
        echo "=== History file (line count) ==="
        wc -l metrics/history-nightly.jsonl || echo "No history-nightly.jsonl"
        echo ""
        echo "=== Reports directory ==="
        ls -la reports/ | head -20

    - name: Update metrics history
      run: |
        if [ -f metrics/latest-nightly.json ]; then
          echo "$(cat metrics/latest-nightly.json)" >> metrics/history-nightly.jsonl
          echo "âœ… Appended to nightly history (total lines: $(wc -l < metrics/history-nightly.jsonl))"
        fi

    - name: Generate unified HTML dashboard
      # Only generate dashboard HTML when NOT triggered by push to release branches
      # Release branches only update metrics data (JSON/JSONL)
      # Schedule/manual runs generate full dashboard
      # The unified dashboard has a dropdown to select which build to view
      if: github.event_name != 'push' || github.ref == 'refs/heads/main'
      run: |
        python scripts/generate_dashboard.py \
          --unified \
          --output metrics/index.html

    - name: Generate job summary
      run: |
        echo "# ðŸ“Š Nightly Test Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Job status summary
        echo "## Job Status" >> $GITHUB_STEP_SUMMARY
        echo "- **Lint**: ${{ needs.nightly-lint.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Security/Quality**: ${{ needs.nightly-security-quality.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Docs**: ${{ needs.nightly-docs.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Build**: ${{ needs.nightly-build.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Unit Tests**: ${{ needs.nightly-test-unit.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Integration Tests**: ${{ needs.nightly-test-integration.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **E2E Tests**: ${{ needs.nightly-test-e2e.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Nightly-Only Tests**: ${{ needs.nightly-only-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Extract metrics from pytest JSON reports
        for report in reports/pytest-*.json; do
          if [ -f "$report" ]; then
            TOTAL=$(jq -r '.summary.total // 0' "$report" 2>/dev/null || echo "0")
            PASSED=$(jq -r '.summary.passed // 0' "$report" 2>/dev/null || echo "0")
            FAILED=$(jq -r '.summary.failed // 0' "$report" 2>/dev/null || echo "0")
            SKIPPED=$(jq -r '.summary.skipped // 0' "$report" 2>/dev/null || echo "0")
            DURATION=$(jq -r '.duration // 0' "$report" 2>/dev/null || echo "0")
            REPORT_NAME=$(basename "$report" .json)

            echo "## Test Summary ($REPORT_NAME)" >> $GITHUB_STEP_SUMMARY
            echo "- **Total Tests**: $TOTAL" >> $GITHUB_STEP_SUMMARY
            echo "- **Passed**: âœ… $PASSED" >> $GITHUB_STEP_SUMMARY
            echo "- **Failed**: âŒ $FAILED" >> $GITHUB_STEP_SUMMARY
            echo "- **Skipped**: â­ï¸ $SKIPPED" >> $GITHUB_STEP_SUMMARY
            echo "- **Duration**: ${DURATION}s" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
        done

        # Extract coverage from coverage.xml
        if [ -f reports/coverage.xml ]; then
          COVERAGE=$(python -c "import xml.etree.ElementTree as ET; tree = ET.parse('reports/coverage.xml'); root = tree.getroot(); print(f\"{float(root.attrib.get('line-rate', 0)) * 100:.1f}%\")" 2>/dev/null || echo "N/A")
          echo "## Coverage" >> $GITHUB_STEP_SUMMARY
          echo "- **Overall Coverage**: $COVERAGE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

        # Extract slowest tests from JUnit XML reports
        for junit in reports/pytest-*.json; do
          if [ -f "$junit" ]; then
            echo "## Slowest Tests (Top 10)" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            python3 -c "
        import json, sys
        try:
            with open('$junit') as f:
                data = json.load(f)
            tests = [(t.get('duration', 0), t.get('nodeid', 'unknown')) for t in data.get('tests', [])]
            tests.sort(reverse=True)
            for time, name in tests[:10]:
                print(f'{time:.2f}s - {name}')
        except Exception as e:
            print(f'Error: {e}', file=sys.stderr)
        " >> $GITHUB_STEP_SUMMARY 2>&1 || echo "Unable to parse" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            break  # Only show once
          fi
        done

        # Check for flaky tests (tests that passed on rerun)
        for report in reports/pytest-*.json; do
          if [ -f "$report" ]; then
            FLAKY_COUNT=$(jq -r '[.tests[] | select(.outcome == "passed" and .rerun == true)] | length' "$report" 2>/dev/null || echo "0")
            if [ "$FLAKY_COUNT" -gt 0 ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "## âš ï¸ Flaky Tests Detected" >> $GITHUB_STEP_SUMMARY
              echo "- **Count**: $FLAKY_COUNT tests passed on rerun" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "### Flaky Test Names:" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              jq -r '.tests[] | select(.outcome == "passed" and .rerun == true) | "\(.nodeid)"' "$report" 2>/dev/null >> $GITHUB_STEP_SUMMARY || echo "Unable to extract"
              echo '```' >> $GITHUB_STEP_SUMMARY
            fi
          fi
        done

        # Show alerts from metrics (if available)
        if [ -f metrics/latest.json ]; then
          ALERT_COUNT=$(jq -r '.alerts | length' metrics/latest.json 2>/dev/null || echo "0")
          if [ "$ALERT_COUNT" -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## ðŸš¨ Metric Alerts" >> $GITHUB_STEP_SUMMARY
            jq -r '.alerts[] |
              if .severity == "error" then "ðŸ”´ **ERROR**: \(.message)"
              elif .severity == "warning" then "âš ï¸ **WARNING**: \(.message)"
              else "â„¹ï¸ **INFO**: \(.message)"
              end' metrics/latest.json 2>/dev/null >> $GITHUB_STEP_SUMMARY || echo "Unable to extract alerts"
          fi
        fi

    - name: Upload combined test reports (extended retention for trend tracking)
      uses: actions/upload-artifact@v4
      with:
        name: nightly-test-reports
        path: |
          reports/
        retention-days: 90

    - name: Upload metrics artifact
      uses: actions/upload-artifact@v4
      with:
        name: nightly-metrics
        path: metrics/
        retention-days: 90

    - name: Post-build cleanup
      if: always()
      run: rm -rf .pytest_cache .mypy_cache .build dist downloaded-reports

  # NOTE: Metrics deployment strategy:
  # - Schedule/manual runs: Generate metrics data + unified dashboard HTML
  # - Release branch pushes: Only generate metrics data (JSON/JSONL), no dashboard HTML
  # - The docs.yml workflow deploys everything to GitHub Pages
  # - Nightly metrics are available as workflow artifacts for download
  # - The unified dashboard uses a dropdown to select CI vs Nightly builds
