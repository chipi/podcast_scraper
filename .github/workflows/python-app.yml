# This workflow will install Python dependencies, run tests and lint with a single version of Python
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python

name: Python application

on:
  push:
    branches: [ "main" ]
    paths:
      - '**.py'
      - 'tests/**'
      - 'pyproject.toml'
      - 'Makefile'
      - 'Dockerfile'
      - '.dockerignore'
  pull_request:
    branches: [ "main" ]
    paths:
      - '**.py'
      - 'tests/**'
      - 'pyproject.toml'
      - 'Makefile'
      - 'Dockerfile'
      - '.dockerignore'

permissions:
  contents: read

jobs:
  # Fast checks - no heavy ML dependencies
  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: "20"
    - name: Install lint dependencies (no ML packages)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
    - name: Install markdownlint
      run: npm install -g markdownlint-cli
    - name: Run lint checks
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        make format-check
        make lint
        make lint-markdown
        make type
        make security

  # Unit tests - fast, no ML dependencies, network isolation enforced
  test-unit:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Install dev dependencies (no ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
    - name: Verify unit tests can import without ML dependencies
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        python scripts/check_unit_test_imports.py
      env:
        PACKAGE: podcast_scraper
    - name: Run unit tests (network isolation enforced, parallel execution)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run tests and capture output and exit code
        set +e  # Don't exit on non-zero return code yet
        OUTPUT=$(pytest tests/unit/ -v --tb=short -n auto 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (unit tests should have many tests)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 50 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 50 unit tests"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed)"
          exit $PYTEST_EXIT_CODE
        fi
    - name: Verify network isolation
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        pytest tests/unit/test_network_isolation.py -v
      env:
        PACKAGE: podcast_scraper

  # Integration tests - needs ML dependencies
  test-integration:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Install full dependencies (including ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
    - name: Run integration tests (parallel execution, with reruns for flaky tests)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run tests and capture output and exit code
        set +e  # Don't exit on non-zero return code yet
        OUTPUT=$(pytest tests/integration/ -v -m integration -n auto --reruns 2 --reruns-delay 1 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (integration tests should have many tests)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 10 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 10 integration tests"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed)"
          exit $PYTEST_EXIT_CODE
        fi
    - name: Post-build cleanup
      if: always()
      run: |
        rm -rf ~/.cache/huggingface ~/.cache/torch ~/.cache/whisper
        rm -rf .pytest_cache .mypy_cache .build dist

  # Fast E2E tests - excludes slow/ml_models tests, runs on every commit
  # Skip for documentation-only PRs
  test-e2e-fast:
    runs-on: ubuntu-latest
    if: |
      github.event_name != 'pull_request' ||
      contains(github.event.pull_request.head.ref, 'docs/') == false ||
      contains(github.event.pull_request.head.ref, 'fix/ai-guidelines-linting') == false
    steps:
    - uses: actions/checkout@v4
    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Install dev dependencies (pytest-socket for network guard)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
        pip install pytest-socket
    - name: Run fast E2E tests (excludes slow/ml_models, with network guard)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run tests with network guard: --disable-socket --allow-hosts=127.0.0.1,localhost
        # Exclude slow and ml_models tests for faster CI feedback
        set +e  # Don't exit on non-zero return code yet
        OUTPUT=$(pytest tests/workflow_e2e/ -v -m "workflow_e2e and not slow and not ml_models" -n auto --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (fast E2E tests should have many tests)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 50 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 50 fast E2E tests"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed)"
          exit $PYTEST_EXIT_CODE
        fi
    - name: Post-build cleanup
      if: always()
      run: |
        rm -rf .pytest_cache .mypy_cache .build dist

  # Slow E2E tests - includes slow/ml_models tests, runs on main branch only
  test-e2e-slow:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v4
    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Install full dependencies (including ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket
    - name: Run slow E2E tests (includes slow/ml_models, with network guard)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run all E2E tests including slow/ml_models with network guard
        set +e  # Don't exit on non-zero return code yet
        OUTPUT=$(pytest tests/workflow_e2e/ -v -m workflow_e2e -n auto --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (all E2E tests should have 99+ tests)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 90 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 90 E2E tests"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed)"
          exit $PYTEST_EXIT_CODE
        fi
    - name: Post-build cleanup
      if: always()
      run: |
        rm -rf ~/.cache/huggingface ~/.cache/torch ~/.cache/whisper
        rm -rf .pytest_cache .mypy_cache .build dist

  # Full test suite - runs all tests (for PRs, includes unit + integration)
  test:
    runs-on: ubuntu-latest
    needs: [test-unit, test-integration]
    if: github.event_name == 'pull_request'
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Install full dependencies (including ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
    - name: Run all tests (unit + integration, parallel execution)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run tests and capture output and exit code
        set +e  # Don't exit on non-zero return code yet
        OUTPUT=$(pytest tests/unit/ tests/integration/ -v --cov=podcast_scraper --cov-report=term-missing -n auto 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (should have many tests from both unit and integration)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 60 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 60 tests (unit + integration)"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed)"
          exit $PYTEST_EXIT_CODE
        fi
      env:
        PACKAGE: podcast_scraper
    - name: Post-build cleanup
      if: always()
      run: |
        rm -rf ~/.cache/huggingface ~/.cache/torch ~/.cache/whisper
        rm -rf .pytest_cache .mypy_cache .build dist

  # Documentation build
  docs:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: |
          docs/requirements.txt
          pyproject.toml
    - name: Install doc dependencies (includes ML for mkdocstrings)
      run: |
        python -m pip install --upgrade pip
        pip install -r docs/requirements.txt
        pip install -e .[ml]
    - name: Build docs
      run: make docs

  # Build package
  build:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
    - name: Install build tools
      run: |
        python -m pip install --upgrade pip
        pip install build
    - name: Build package
      run: make build
