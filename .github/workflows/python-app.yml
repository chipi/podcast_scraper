# This workflow will install Python dependencies, run tests and quality checks with a single version of Python
# Dependency flow:
# 1. lint, build, preload-ml-models run in parallel (no deps)
# 2. security-quality and test-unit run in parallel (both gate: lint, build)
# 3. test-integration and test-e2e run in parallel (both gate: preload-ml-models only)
# 4. coverage-unified gates: security-quality + all test jobs (including test-unit)
# 5. docs gates coverage-unified (very last step, final validation)
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python

name: Python application

on:
  push:
    branches: [ "main", "release/2.4" ]
    paths:
      - '**.py'
      - '**.j2'
      - 'tests/**'
      - 'pyproject.toml'
      - 'Makefile'
      - 'Dockerfile'
      - '.dockerignore'
  pull_request:
    branches: [ "main", "release/2.4" ]
    paths:
      - '**.py'
      - '**.j2'
      - 'tests/**'
      - 'pyproject.toml'
      - 'Makefile'
      - 'Dockerfile'
      - '.dockerignore'

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  # Detect if only documentation files changed (used to skip ML model preloading and coverage for docs-only PRs)
  detect-changes:
    runs-on: ubuntu-latest
    outputs:
      docs-only: ${{ steps.filter.outputs.docs }}
      code: ${{ steps.filter.outputs.code }}
    steps:
    - uses: actions/checkout@v4
    - uses: dorny/paths-filter@v3
      id: filter
      with:
      filters: |
        docs:
          - 'docs/**'
          - 'mkdocs.yml'
          - '**/*.md'
          - 'LICENSE'
          code:
            - '**.py'
            - '**.j2'
            - 'tests/**'
            - 'pyproject.toml'
            - 'Makefile'
            - 'Dockerfile'
            - '.dockerignore'
            - '.github/workflows/**'

  # Fast lint checks - blocks tests to catch issues early
  lint:
    runs-on: ubuntu-latest
    timeout-minutes: 10  # Fast checks: format, lint, markdown, type
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: "20"
    - name: Install lint dependencies (no ML packages)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
    - name: Install markdownlint
      run: npm install -g markdownlint-cli
    - name: Run fast lint checks
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        make format-check
        make lint
        make lint-markdown
        make type

  # Security and quality checks - gated by lint/build, runs independently to coverage-unified (does not gate tests)
  security-quality:
    runs-on: ubuntu-latest
    timeout-minutes: 20  # Slower checks: security, complexity, deadcode, docstrings, spelling
    needs: [lint, build]  # Wait for lint, build to pass (docs removed - doesn't gate security)
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
    - name: Run security checks
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        make security
    - name: Code quality analysis
      continue-on-error: true
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        echo "## üìä Code Quality Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Complexity analysis
        echo "### Complexity Analysis" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        radon cc src/podcast_scraper/ -a -s --total-average >> $GITHUB_STEP_SUMMARY 2>&1 || echo "No complexity data available"
        echo '```' >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Maintainability index
        echo "### Maintainability Index" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        radon mi src/podcast_scraper/ -s >> $GITHUB_STEP_SUMMARY 2>&1 || echo "No maintainability data available"
        echo '```' >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Docstring coverage
        echo "### Docstring Coverage" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        interrogate src/podcast_scraper/ -v >> $GITHUB_STEP_SUMMARY 2>&1 || echo "No docstring data available"
        echo '```' >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Dead code detection
        echo "### Dead Code Detection" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        vulture src/podcast_scraper/ --min-confidence 80 >> $GITHUB_STEP_SUMMARY 2>&1 || echo "No dead code detected"
        echo '```' >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Spell checking
        echo "### Spell Checking" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        codespell src/ docs/ --skip="*.pyc,*.json,*.xml,*.lock,*.mp3,*.whl" >> $GITHUB_STEP_SUMMARY 2>&1 || echo "No spelling errors found"
        echo '```' >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Save complexity metrics to file for metrics generation
        mkdir -p reports
        radon cc src/podcast_scraper/ -a -s --total-average --json > reports/complexity.json 2>/dev/null || echo '{"total_average": 0}' > reports/complexity.json
        radon mi src/podcast_scraper/ -s --json > reports/maintainability.json 2>/dev/null || echo '[]' > reports/maintainability.json
        interrogate src/podcast_scraper/ --quiet --output-format json > reports/docstrings.json 2>/dev/null || echo '{"coverage_percent": 0}' > reports/docstrings.json

        # Save dead code detection results (vulture supports --json)
        vulture src/podcast_scraper/ --min-confidence 80 --json > reports/vulture.json 2>/dev/null || echo '[]' > reports/vulture.json

        # Save spell checking results (codespell doesn't have JSON, parse text output)
        codespell src/ docs/ --skip="*.pyc,*.json,*.xml,*.lock,*.mp3,*.whl" > reports/codespell.txt 2>&1 || echo "" > reports/codespell.txt

  # Unit tests - fast, no ML dependencies, network isolation enforced
  test-unit:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Conservative timeout for fast unit tests
    needs: [lint, build]  # Wait for lint, build to pass (docs removed - doesn't gate unit tests)
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Install dev dependencies (no ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
    - name: Verify unit tests can import without ML dependencies
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        python scripts/tools/check_unit_test_imports.py
      env:
        PACKAGE: podcast_scraper
    - name: Run unit tests with coverage (network isolation enforced, parallel execution)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Create reports directory for coverage output
        mkdir -p reports
        # Run tests with coverage and capture output and exit code
        set +e  # Don't exit on non-zero return code yet
        OUTPUT=$(pytest tests/unit/ -v --tb=short -n $(python3 -c "import os; print(max(1, (os.cpu_count() or 2) - 2))") --json-report --json-report-file=reports/pytest-unit.json --cov=podcast_scraper --cov-report=xml:reports/coverage.xml --cov-report=term-missing 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (unit tests should have many tests)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 50 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 50 unit tests"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed or coverage below threshold)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed or coverage below threshold)"
          exit $PYTEST_EXIT_CODE
        fi
      env:
        PACKAGE: podcast_scraper
    - name: Generate coverage summary
      if: always()
      run: |
        echo "# üìä Test Coverage Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Extract coverage from coverage.xml
        if [ -f reports/coverage.xml ]; then
          COVERAGE=$(python3 -c "import xml.etree.ElementTree as ET; tree = ET.parse('reports/coverage.xml'); root = tree.getroot(); print(f\"{float(root.attrib.get('line-rate', 0)) * 100:.1f}%\")" 2>/dev/null || echo "N/A")
          BRANCH_COVERAGE=$(python3 -c "import xml.etree.ElementTree as ET; tree = ET.parse('reports/coverage.xml'); root = tree.getroot(); print(f\"{float(root.attrib.get('branch-rate', 0)) * 100:.1f}%\")" 2>/dev/null || echo "N/A")

          echo "## Coverage Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Line Coverage**: $COVERAGE" >> $GITHUB_STEP_SUMMARY
          echo "- **Branch Coverage**: $BRANCH_COVERAGE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Coverage threshold - for display only on unit tests
          # Note: Unit tests typically achieve 70-80% coverage
          # Threshold is only enforced on COMBINED coverage (see coverage-unified job)
          THRESHOLD=80
          echo "- **Combined Threshold**: ${THRESHOLD}% *(enforced on combined coverage only)*" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check if coverage meets threshold (informational for unit tests)
          COVERAGE_NUM=$(echo "$COVERAGE" | sed 's/%//' | cut -d. -f1)
          if [ "$COVERAGE_NUM" != "N/A" ] && [ "$COVERAGE_NUM" -ge "${THRESHOLD}" ]; then
            echo "‚úÖ Unit coverage meets combined threshold!" >> $GITHUB_STEP_SUMMARY
          elif [ "$COVERAGE_NUM" != "N/A" ]; then
            echo "‚ÑπÔ∏è Unit coverage below combined threshold (expected - combined includes integration/E2E)" >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "‚ö†Ô∏è Coverage report not found" >> $GITHUB_STEP_SUMMARY
        fi
    - name: Upload coverage artifacts
      if: always()
      uses: actions/upload-artifact@v6
      with:
        name: coverage-unit
        path: reports/coverage.xml
        retention-days: 7
        if-no-files-found: ignore
    - name: Upload pytest JSON report
      if: always()
      uses: actions/upload-artifact@v6
      with:
        name: pytest-unit
        path: reports/pytest-unit.json
        retention-days: 7
        if-no-files-found: ignore
    # Network isolation is enforced by pytest-socket (--disable-socket --allow-hosts)
    # The dedicated test file was removed as the blocking feature is now disabled

  # Preload ML models - runs on all commits (main and PRs), ensures models are cached for integration and e2e tests
  # This job has no dependencies and starts immediately in parallel with lint, docs, build
  # Optimized: Check cache FIRST (no Python needed), only set up Python if cache misses
  # Skip for docs-only changes (when only docs files changed, no code files) to avoid unnecessary model downloads
  preload-ml-models:
    runs-on: ubuntu-latest
    timeout-minutes: 20  # Conservative timeout for model preloading (usually cache hit, ~2 min)
    needs: [detect-changes]
    if: needs.detect-changes.outputs.docs-only != 'true' || needs.detect-changes.outputs.code == 'true'
    env:
      # Ensure consistent cache paths for Hugging Face libraries
      HF_HOME: /home/runner/.cache/huggingface
      HF_HUB_CACHE: /home/runner/.cache/huggingface/hub
    steps:
    - uses: actions/checkout@v4
    - name: Cache ML models (check first - no Python needed, if cache hits job completes in ~6-12s)
      uses: actions/cache@v4
      id: cache-models
      with:
        path: |
          ~/.cache/whisper
          ~/.cache/huggingface
        # NOTE: spaCy removed - installed as pip package via .[ml]
        # Cache key automatically invalidates when model definitions change
        key: ml-models-${{ runner.os }}-${{ hashFiles('scripts/cache/preload_ml_models.py', 'src/podcast_scraper/config.py') }}
        restore-keys: |
          ml-models-${{ runner.os }}-
    - name: Set up Python 3.11 (only if cache miss - needed for model download)
      if: steps.cache-models.outputs.cache-hit != 'true'
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Free disk space (only if cache miss - need space for model download)
      if: steps.cache-models.outputs.cache-hit != 'true'
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h
    - name: Install full dependencies (including ML)
      if: steps.cache-models.outputs.cache-hit != 'true'
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
    - name: Preload ML models (only if cache miss)
      if: steps.cache-models.outputs.cache-hit != 'true'
      run: make preload-ml-models

  # Full integration tests - all integration tests, runs on main branch only
  test-integration:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # Conservative timeout for integration tests with ML models
    needs: [preload-ml-models]  # Wait for model preload only (can run in parallel with unit tests)
    env:
      # Ensure consistent cache paths for Hugging Face libraries
      HF_HOME: /home/runner/.cache/huggingface
      HF_HUB_CACHE: /home/runner/.cache/huggingface/hub
      # Signal to tests that CI has validated cache - skip redundant checks
      ML_MODELS_VALIDATED: "true"
    if: |
      github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/release/2.4')
    steps:
    - uses: actions/checkout@v4
    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h
    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Cache ML models
      uses: actions/cache@v4
      id: cache-models
      with:
        path: |
          ~/.cache/whisper
          ~/.cache/huggingface
        # NOTE: spaCy removed - installed as pip package via .[ml]
        # Cache key automatically invalidates when model definitions change
        key: ml-models-${{ runner.os }}-${{ hashFiles('scripts/cache/preload_ml_models.py', 'src/podcast_scraper/config.py') }}
        restore-keys: |
          ml-models-${{ runner.os }}-
    - name: Install full dependencies (including ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket
    - name: Install ffmpeg (required for Whisper)
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends ffmpeg
    - name: Run all integration tests with coverage (full suite, with network guard)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Create reports directory for coverage output
        mkdir -p reports
        # Run all integration tests with coverage and network guard
        set +e  # Don't exit on non-zero return code yet
        OUTPUT=$(pytest tests/integration/ -v -m integration -n $(python3 -c "import os; print(max(1, (os.cpu_count() or 2) - 2))") --json-report --json-report-file=reports/pytest-integration.json --cov=podcast_scraper --cov-append --cov-report=xml:reports/coverage-integration.xml --cov-report=term-missing --cov-fail-under=40 --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (all integration tests should have many tests)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 50 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 50 integration tests"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed)"
          exit $PYTEST_EXIT_CODE
        fi
    - name: Upload coverage artifacts
      if: always()
      uses: actions/upload-artifact@v6
      with:
        name: coverage-integration
        path: reports/coverage-integration.xml
        retention-days: 7
        if-no-files-found: ignore
    - name: Upload pytest JSON report
      if: always()
      uses: actions/upload-artifact@v6
      with:
        name: pytest-integration
        path: reports/pytest-integration.json
        retention-days: 7
        if-no-files-found: ignore
    - name: Post-build cleanup
      if: always()
      run: |
        # Keep model caches for next run (they're cached via GitHub Actions cache)
        rm -rf .pytest_cache .mypy_cache .build dist

  # Fast integration tests - critical path only, runs on PRs only
  # Full integration tests run on main branch only
  test-integration-fast:
    runs-on: ubuntu-latest
    timeout-minutes: 20  # Conservative timeout for fast integration tests
    needs: [preload-ml-models]  # Wait for model preload only (can run in parallel with unit tests)
    env:
      # Ensure consistent cache paths for Hugging Face libraries
      HF_HOME: /home/runner/.cache/huggingface
      HF_HUB_CACHE: /home/runner/.cache/huggingface/hub
      # Signal to tests that CI has validated cache - skip redundant checks
      ML_MODELS_VALIDATED: "true"
    if: |
      github.event_name == 'pull_request' && !contains(github.event.pull_request.head.ref, 'docs/')
    steps:
    - uses: actions/checkout@v4
    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h
    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Cache ML models
      uses: actions/cache@v4
      id: cache-models
      with:
        path: |
          ~/.cache/whisper
          ~/.cache/huggingface
        # NOTE: spaCy removed - installed as pip package via .[ml]
        # Cache key automatically invalidates when model definitions change
        key: ml-models-${{ runner.os }}-${{ hashFiles('scripts/cache/preload_ml_models.py', 'src/podcast_scraper/config.py') }}
        restore-keys: |
          ml-models-${{ runner.os }}-
    - name: Install dev dependencies with ML (pytest-socket for network guard)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket
    - name: Install ffmpeg (required for Whisper)
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends ffmpeg
    - name: Run fast integration tests with coverage (critical path only, with network guard)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Create reports directory for coverage output
        mkdir -p reports
        # Run tests with coverage and network guard: --disable-socket --allow-hosts=127.0.0.1,localhost
        # Critical path tests only for faster CI feedback
        # Note: Fast tests don't enforce coverage threshold (they're a subset, full suite enforces threshold)
        set +e  # Don't exit on non-zero return code yet
        OUTPUT=$(pytest tests/integration/ -v -m "integration and critical_path" -n $(python3 -c "import os; print(max(1, (os.cpu_count() or 2) - 2))") --cov=podcast_scraper --cov-append --cov-report=xml:reports/coverage-integration.xml --cov-report=term-missing --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 --durations=20 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (critical path integration tests should have some tests)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 5 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 5 critical path integration tests"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed)"
          exit $PYTEST_EXIT_CODE
        fi
    - name: Upload coverage artifacts
      if: always()
      uses: actions/upload-artifact@v6
      with:
        name: coverage-integration-fast
        path: reports/coverage-integration.xml
        retention-days: 7
        if-no-files-found: ignore
    - name: Post-build cleanup
      if: always()
      run: |
        rm -rf .pytest_cache .mypy_cache .build dist

  # Fast E2E tests - critical path only, runs on PRs only
  # Full E2E tests run on main branch only
  test-e2e-fast:
    runs-on: ubuntu-latest
    timeout-minutes: 25  # Conservative timeout for fast E2E tests
    needs: [preload-ml-models]  # Wait for model preload only (can run in parallel with unit tests)
    env:
      # Ensure consistent cache paths for Hugging Face libraries
      HF_HOME: /home/runner/.cache/huggingface
      HF_HUB_CACHE: /home/runner/.cache/huggingface/hub
      # Signal to tests that CI has validated cache - skip redundant checks
      ML_MODELS_VALIDATED: "true"
    if: |
      github.event_name == 'pull_request' && !contains(github.event.pull_request.head.ref, 'docs/')
    steps:
    - uses: actions/checkout@v4
    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h
    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Cache ML models
      uses: actions/cache@v4
      id: cache-models
      with:
        path: |
          ~/.cache/whisper
          ~/.cache/huggingface
        # NOTE: spaCy removed - installed as pip package via .[ml]
        # Cache key automatically invalidates when model definitions change
        key: ml-models-${{ runner.os }}-${{ hashFiles('scripts/cache/preload_ml_models.py', 'src/podcast_scraper/config.py') }}
        restore-keys: |
          ml-models-${{ runner.os }}-
    - name: Install dev dependencies (pytest-socket for network guard)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket
    - name: Install ffmpeg (required for Whisper)
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends ffmpeg
    - name: Run fast E2E tests with coverage (critical path only, with network guard)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        export E2E_TEST_MODE=fast
        # Create reports directory for coverage output
        mkdir -p reports
        # Run tests with coverage and network guard: --disable-socket --allow-hosts=127.0.0.1,localhost
        # Critical path tests only for faster CI feedback
        # Note: Fast tests don't enforce coverage threshold (they're a subset, full suite enforces threshold)
        # Serial tests first, then parallel tests (matches Makefile pattern)
        set +e  # Don't exit on non-zero return code yet
        # Run serial tests first with coverage
        OUTPUT_SERIAL=$(E2E_TEST_MODE=fast pytest tests/e2e/ -v -m "e2e and critical_path and serial" --cov=podcast_scraper --cov-append --cov-report=xml:reports/coverage-e2e.xml --cov-report=term-missing --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 --durations=20 2>&1 || true)
        echo "$OUTPUT_SERIAL"
        # Run parallel tests with coverage (append to same file)
        OUTPUT=$(E2E_TEST_MODE=fast pytest tests/e2e/ -v -m "e2e and critical_path and not serial" -n $(python3 -c "import os; print(max(1, (os.cpu_count() or 2) - 2))") --cov=podcast_scraper --cov-append --cov-report=xml:reports/coverage-e2e.xml --cov-report=term-missing --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 --durations=20 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (critical path E2E tests should have some tests)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 3 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 3 critical path E2E tests"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed)"
          exit $PYTEST_EXIT_CODE
        fi
    - name: Upload coverage artifacts
      if: always()
      uses: actions/upload-artifact@v6
      with:
        name: coverage-e2e-fast
        path: reports/coverage-e2e.xml
        retention-days: 7
        if-no-files-found: ignore
    - name: Post-build cleanup
      if: always()
      run: |
        rm -rf .pytest_cache .mypy_cache .build dist

  # Full E2E tests - all E2E tests, runs on main branch only
  test-e2e:
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Conservative timeout for E2E tests with ML models
    needs: [preload-ml-models]  # Wait for model preload only (can run in parallel with unit tests)
    env:
      # Ensure consistent cache paths for Hugging Face libraries
      HF_HOME: /home/runner/.cache/huggingface
      HF_HUB_CACHE: /home/runner/.cache/huggingface/hub
      # Signal to tests that CI has validated cache - skip redundant checks
      ML_MODELS_VALIDATED: "true"
    if: |
      github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/release/2.4')
    steps:
    - uses: actions/checkout@v4
    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h
    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Cache ML models
      uses: actions/cache@v4
      id: cache-models
      with:
        path: |
          ~/.cache/whisper
          ~/.cache/huggingface
        # NOTE: spaCy removed - installed as pip package via .[ml]
        # Cache key automatically invalidates when model definitions change
        key: ml-models-${{ runner.os }}-${{ hashFiles('scripts/cache/preload_ml_models.py', 'src/podcast_scraper/config.py') }}
        restore-keys: |
          ml-models-${{ runner.os }}-
    - name: Install full dependencies (including ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket
    - name: Install ffmpeg (required for Whisper)
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends ffmpeg
    - name: Run all E2E tests with coverage (full suite, with network guard)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        export E2E_TEST_MODE=multi_episode
        # Create reports directory for coverage output
        mkdir -p reports
        # Run all E2E tests with coverage and network guard
        # Serial tests first, then parallel tests (matches Makefile pattern)
        set +e  # Don't exit on non-zero return code yet
        # Run serial tests first with coverage
        OUTPUT_SERIAL=$(E2E_TEST_MODE=multi_episode pytest tests/e2e/ -v -m "e2e and serial" --json-report --json-report-file=reports/pytest-e2e-serial.json --cov=podcast_scraper --cov-append --cov-report=xml:reports/coverage-e2e.xml --cov-report=term-missing --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 2>&1 || true)
        echo "$OUTPUT_SERIAL"
        # Run parallel tests with coverage (append to same file)
        OUTPUT=$(E2E_TEST_MODE=multi_episode pytest tests/e2e/ -v -m "e2e and not serial" -n $(python3 -c "import os; print(max(1, (os.cpu_count() or 2) - 2))") --json-report --json-report-file=reports/pytest-e2e.json --cov=podcast_scraper --cov-append --cov-report=xml:reports/coverage-e2e.xml --cov-report=term-missing --cov-fail-under=40 --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (all E2E tests should have many tests)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 50 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 50 E2E tests"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed)"
          exit $PYTEST_EXIT_CODE
        fi
    - name: Upload coverage artifacts
      if: always()
      uses: actions/upload-artifact@v6
      with:
        name: coverage-e2e
        path: reports/coverage-e2e.xml
        retention-days: 7
        if-no-files-found: ignore
    - name: Upload pytest JSON reports
      if: always()
      uses: actions/upload-artifact@v6
      with:
        name: pytest-e2e
        path: |
          reports/pytest-e2e.json
          reports/pytest-e2e-serial.json
        retention-days: 7
        if-no-files-found: ignore
    - name: Post-build cleanup
      if: always()
      run: |
        # Keep model caches for next run (they're cached via GitHub Actions cache)
        rm -rf .pytest_cache .mypy_cache .build dist

  # Documentation build - validates docs can be built correctly
  # Runs as very last step after all tests and coverage pass (final validation)
  # Docs deployment happens in docs.yml workflow (separate, already gated properly)
  docs:
    runs-on: ubuntu-latest
    needs: [coverage-unified]  # Very last step - validates docs after all tests pass
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: |
          docs/requirements.txt
          pyproject.toml
    - name: Install doc dependencies (includes ML for mkdocstrings)
      run: |
        python -m pip install --upgrade pip
        pip install -r docs/requirements.txt
        pip install -e .[ml]
    - name: Build docs
      run: make docs

  # Unified coverage report - combines unit + integration + E2E coverage
  # Waits for all test jobs to complete, then combines their coverage reports
  # Skip for docs-only changes (when only docs files changed, no code files)
  coverage-unified:
    runs-on: ubuntu-latest
    needs: [detect-changes, security-quality, test-unit, test-integration, test-integration-fast, test-e2e, test-e2e-fast]  # Wait for security/quality and all test jobs
    if: always() && (needs.detect-changes.outputs.docs-only != 'true' || needs.detect-changes.outputs.code == 'true')
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Install coverage tools only
      run: |
        python -m pip install --upgrade pip
        pip install coverage[toml]  # Only coverage tool needed for combining reports
    - name: Download coverage artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: coverage-*
        merge-multiple: false
        path: coverage-artifacts
      continue-on-error: true  # Some jobs may not have run (conditional)
    - name: Download pytest JSON artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: pytest-*
        merge-multiple: false
        path: pytest-artifacts
      continue-on-error: true  # Some artifacts may not exist
    - name: Combine pytest JSON reports
      run: |
        mkdir -p reports
        # Merge all pytest JSON reports into a single combined report
        python3 << 'EOF'
        import json
        import os
        from pathlib import Path

        combined = {
            "summary": {"total": 0, "passed": 0, "failed": 0, "skipped": 0},
            "duration": 0.0,
            "tests": []
        }

        artifacts_dir = Path("pytest-artifacts")
        if artifacts_dir.exists():
            for json_file in artifacts_dir.rglob("*.json"):
                try:
                    with open(json_file) as f:
                        data = json.load(f)
                    summary = data.get("summary", {})
                    combined["summary"]["total"] += summary.get("total", 0)
                    combined["summary"]["passed"] += summary.get("passed", 0)
                    combined["summary"]["failed"] += summary.get("failed", 0)
                    combined["summary"]["skipped"] += summary.get("skipped", 0)
                    combined["duration"] += data.get("duration", 0)
                    combined["tests"].extend(data.get("tests", []))
                    print(f"‚úÖ Merged {json_file}: {summary.get('total', 0)} tests")
                except Exception as e:
                    print(f"‚ö†Ô∏è Failed to parse {json_file}: {e}")

        # Calculate pass rate
        total = combined["summary"]["total"]
        passed = combined["summary"]["passed"]
        combined["summary"]["pass_rate"] = passed / total if total > 0 else 0.0

        with open("reports/pytest.json", "w") as f:
            json.dump(combined, f, indent=2)

        print(f"üìä Combined: {total} tests, {passed} passed, {combined['summary']['failed']} failed")
        EOF
    - name: Combine coverage reports
      run: |
        mkdir -p reports
        # Combine all coverage XML files into a unified report
        # Uses Python script to properly merge coverage data from multiple sources
        python3 << 'EOF'
        import xml.etree.ElementTree as ET
        from pathlib import Path
        import os

        def parse_coverage_xml(filepath):
            """Parse a coverage XML file and return line coverage data."""
            tree = ET.parse(filepath)
            root = tree.getroot()
            coverage_data = {}

            for package in root.findall('.//package'):
                for cls in package.findall('.//class'):
                    filename = cls.get('filename', '')
                    if not filename:
                        continue

                    if filename not in coverage_data:
                        coverage_data[filename] = {'hits': set(), 'misses': set()}

                    for line in cls.findall('.//line'):
                        line_num = int(line.get('number', 0))
                        hits = int(line.get('hits', 0))
                        if hits > 0:
                            coverage_data[filename]['hits'].add(line_num)
                            coverage_data[filename]['misses'].discard(line_num)
                        elif line_num not in coverage_data[filename]['hits']:
                            coverage_data[filename]['misses'].add(line_num)

            return coverage_data, root

        # Find all coverage XML files
        artifacts_dir = Path('coverage-artifacts')
        xml_files = list(artifacts_dir.rglob('*.xml')) if artifacts_dir.exists() else []

        if not xml_files:
            print("‚ö†Ô∏è No coverage XML files found - skipping coverage combination")
            print("This can happen if only documentation files were changed.")
            # Create a minimal coverage XML to prevent downstream failures
            root = ET.Element('coverage')
            root.set('line-rate', '0')
            root.set('branch-rate', '0')
            root.set('lines-valid', '0')
            root.set('lines-covered', '0')
            tree = ET.ElementTree(root)
            tree.write('reports/coverage-unified.xml', encoding='unicode', xml_declaration=True)
            print("‚úÖ Created minimal coverage report")
            exit(0)

        print(f"üìä Found {len(xml_files)} coverage files to combine:")
        for f in xml_files:
            print(f"  - {f}")

        # Merge all coverage data
        merged_coverage = {}
        base_root = None

        for xml_file in xml_files:
            try:
                coverage_data, root = parse_coverage_xml(xml_file)
                if base_root is None:
                    base_root = root

                for filename, data in coverage_data.items():
                    if filename not in merged_coverage:
                        merged_coverage[filename] = {'hits': set(), 'misses': set()}
                    merged_coverage[filename]['hits'].update(data['hits'])
                    # Only keep misses that aren't hits
                    for miss in data['misses']:
                        if miss not in merged_coverage[filename]['hits']:
                            merged_coverage[filename]['misses'].add(miss)

                print(f"  ‚úÖ Merged: {xml_file}")
            except Exception as e:
                print(f"  ‚ö†Ô∏è Failed to parse {xml_file}: {e}")

        # Calculate unified coverage stats
        total_lines = 0
        covered_lines = 0
        for filename, data in merged_coverage.items():
            total_lines += len(data['hits']) + len(data['misses'])
            covered_lines += len(data['hits'])

        coverage_pct = (covered_lines / total_lines * 100) if total_lines > 0 else 0
        print(f"\nüìà Unified Coverage: {coverage_pct:.1f}% ({covered_lines}/{total_lines} lines)")

        # Use the first XML as base and update its line-rate
        if base_root is not None:
            base_root.set('line-rate', str(covered_lines / total_lines if total_lines > 0 else 0))
            base_root.set('lines-valid', str(total_lines))
            base_root.set('lines-covered', str(covered_lines))

            tree = ET.ElementTree(base_root)
            tree.write('reports/coverage-unified.xml', encoding='unicode', xml_declaration=True)
            print(f"‚úÖ Written unified coverage to reports/coverage-unified.xml")
        EOF
    - name: Generate unified coverage summary
      if: always()
      run: |
        echo "# üìä Unified Test Coverage Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Extract coverage from unified report
        if [ -f reports/coverage-unified.xml ]; then
          COVERAGE=$(python3 -c "import xml.etree.ElementTree as ET; tree = ET.parse('reports/coverage-unified.xml'); root = tree.getroot(); print(f\"{float(root.attrib.get('line-rate', 0)) * 100:.1f}%\")" 2>/dev/null || echo "N/A")
          BRANCH_COVERAGE=$(python3 -c "import xml.etree.ElementTree as ET; tree = ET.parse('reports/coverage-unified.xml'); root = tree.getroot(); print(f\"{float(root.attrib.get('branch-rate', 0)) * 100:.1f}%\")" 2>/dev/null || echo "N/A")

          echo "## Unified Coverage Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Line Coverage**: $COVERAGE" >> $GITHUB_STEP_SUMMARY
          echo "- **Branch Coverage**: $BRANCH_COVERAGE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Coverage threshold for combined coverage (not per-test-type)
          # Note: Individual test types (unit/integration/e2e) have different expected coverage:
          # - Unit: ~70-80%, Integration: ~50-60%, E2E: ~50-55%
          # Only COMBINED coverage is enforced against this threshold
          THRESHOLD=80
          echo "- **Threshold**: ${THRESHOLD}%" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check if coverage meets threshold
          COVERAGE_NUM=$(echo "$COVERAGE" | sed 's/%//' | cut -d. -f1)
          if [ "$COVERAGE_NUM" != "N/A" ] && [ "$COVERAGE_NUM" -ge "${THRESHOLD}" ]; then
            echo "‚úÖ Coverage meets threshold!" >> $GITHUB_STEP_SUMMARY
          elif [ "$COVERAGE_NUM" != "N/A" ]; then
            echo "‚ö†Ô∏è Coverage below threshold (${THRESHOLD}%)" >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "‚ö†Ô∏è Unified coverage report not found" >> $GITHUB_STEP_SUMMARY
        fi
    - name: Generate code quality metrics
      if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/release/2.4')
      run: |
        mkdir -p reports
        # Generate complexity metrics for dashboard
        radon cc src/podcast_scraper/ -a -s --total-average --json > reports/complexity.json 2>/dev/null || echo '{"total_average": 0}' > reports/complexity.json
        radon mi src/podcast_scraper/ -s --json > reports/maintainability.json 2>/dev/null || echo '[]' > reports/maintainability.json
        interrogate src/podcast_scraper/ --quiet --output-format json > reports/docstrings.json 2>/dev/null || echo '{"coverage_percent": 0}' > reports/docstrings.json
    - name: Collect pipeline metrics
      if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/release/2.4')
      run: |
        # Install ML dependencies for pipeline execution
        pip install -e .[ml] || echo "‚ö†Ô∏è ML dependencies not available, skipping pipeline metrics"
        # Run minimal pipeline to collect performance metrics
        python scripts/dashboard/collect_pipeline_metrics.py \
          --output reports/pipeline_metrics.json \
          --max-episodes 1 || echo "‚ö†Ô∏è Pipeline metrics collection failed (non-blocking)"
      continue-on-error: true  # Don't fail if pipeline metrics collection fails
    - name: Generate metrics JSON from unified coverage
      if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/release/2.4')
      run: |
        # Create minimal reports structure for metrics generation
        # The unified coverage is already in reports/coverage-unified.xml
        # Copy it to reports/coverage.xml for metrics script compatibility
        if [ -f reports/coverage-unified.xml ]; then
          cp reports/coverage-unified.xml reports/coverage.xml
        fi

        # Create minimal JUnit XML if not available (metrics script expects it)
        if [ ! -f reports/junit.xml ]; then
          python3 -c "import xml.etree.ElementTree as ET; root = ET.Element('testsuites'); root.set('tests', '0'); root.set('failures', '0'); root.set('time', '0'); tree = ET.ElementTree(root); tree.write('reports/junit.xml', encoding='utf-8', xml_declaration=True)"
        fi

        # Create minimal pytest JSON if merge didn't produce one
        if [ ! -f reports/pytest.json ]; then
          echo "‚ö†Ô∏è No pytest JSON found, creating minimal fallback"
          echo '{"summary": {"total": 0, "passed": 0, "failed": 0, "skipped": 0}, "duration": 0, "tests": []}' > reports/pytest.json
        else
          echo "‚úÖ Using merged pytest.json with $(jq '.summary.total' reports/pytest.json) tests"
        fi

        # Generate metrics JSON (includes complexity and pipeline metrics)
        # Coverage threshold (80%) is for combined coverage only
        # Save with CI prefix for unified dashboard
        python scripts/dashboard/generate_metrics.py \
          --reports-dir reports \
          --output metrics/latest-ci.json \
          --history metrics/history-ci.jsonl \
          --pipeline-metrics reports/pipeline_metrics.json \
          --coverage-threshold 80 || echo "‚ö†Ô∏è Metrics generation failed (non-blocking)"
    - name: Update metrics history
      if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/release/2.4')
      run: |
        # Load existing history from gh-pages if available
        mkdir -p metrics
        git fetch origin gh-pages:gh-pages 2>/dev/null || true
        if git show gh-pages:metrics/history-ci.jsonl > metrics/history-ci.jsonl 2>/dev/null; then
          echo "‚úÖ Loaded existing CI history"
        else
          echo "üìù Creating new CI history file"
          touch metrics/history-ci.jsonl
        fi

        # Append latest metrics to history
        if [ -f metrics/latest-ci.json ]; then
          echo "$(cat metrics/latest-ci.json)" >> metrics/history-ci.jsonl
          echo "‚úÖ Appended to CI history"
        fi
    - name: Generate HTML dashboard
      if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/release/2.4')
      run: |
        python scripts/dashboard/generate_dashboard.py \
          --unified \
          --output metrics/index.html || echo "‚ö†Ô∏è Dashboard generation failed (non-blocking)"
    - name: Upload unified coverage report
      if: always()
      uses: actions/upload-artifact@v6
      with:
        name: coverage-unified
        path: reports/coverage-unified.xml
        retention-days: 30
        if-no-files-found: ignore
    - name: Upload coverage to Codecov
      if: always()
      uses: codecov/codecov-action@v4
      with:
        file: reports/coverage-unified.xml
        flags: unittests
        name: codecov-unified
        fail_ci_if_error: false
        token: ${{ secrets.CODECOV_TOKEN }}
    - name: Upload metrics as artifact
      if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/release/2.4')
      uses: actions/upload-artifact@v4
      with:
        name: metrics
        path: metrics/
        retention-days: 30

  # NOTE: Metrics deployment strategy:
  # - Main branch: Generates metrics data + unified dashboard HTML
  # - Release branches: Only generate metrics data (JSON/JSONL), no dashboard HTML
  # - The docs.yml workflow deploys everything to GitHub Pages
  # - Metrics are available as workflow artifacts for download
  # - The unified dashboard uses a dropdown to select CI vs Nightly builds

  # Build package - validates package can be built correctly
  # Builds source distribution (sdist) and wheel distribution
  # Catches packaging issues (pyproject.toml errors, missing files, etc.)
  # Fast check (~2-3 min) that gates test-unit to catch packaging issues early
  build:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Install build tools
      run: |
        python -m pip install --upgrade pip
        pip install build
    - name: Build package
      run: make build
