# This workflow will install Python dependencies, run tests and lint with a single version of Python
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python

name: Python application

on:
  push:
    branches: [ "main" ]
    paths:
      - '**.py'
      - 'tests/**'
      - 'pyproject.toml'
      - 'Makefile'
      - 'Dockerfile'
      - '.dockerignore'
  pull_request:
    branches: [ "main" ]
    paths:
      - '**.py'
      - 'tests/**'
      - 'pyproject.toml'
      - 'Makefile'
      - 'Dockerfile'
      - '.dockerignore'

permissions:
  contents: read

jobs:
  # Fast checks - no heavy ML dependencies
  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: "20"
    - name: Install lint dependencies (no ML packages)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
    - name: Install markdownlint
      run: npm install -g markdownlint-cli
    - name: Run lint checks
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        make format-check
        make lint
        make lint-markdown
        make type
        make security

  # Unit tests - fast, no ML dependencies, network isolation enforced
  test-unit:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Install dev dependencies (no ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
    - name: Verify unit tests can import without ML dependencies
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        python scripts/check_unit_test_imports.py
      env:
        PACKAGE: podcast_scraper
    - name: Run unit tests (network isolation enforced, parallel execution)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run tests and capture output and exit code
        set +e  # Don't exit on non-zero return code yet
        OUTPUT=$(pytest tests/unit/ -v --tb=short -n auto 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (unit tests should have many tests)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 50 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 50 unit tests"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed)"
          exit $PYTEST_EXIT_CODE
        fi
    - name: Verify network isolation
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        pytest tests/unit/test_network_isolation.py -v
      env:
        PACKAGE: podcast_scraper

  # Preload ML models - runs on all commits (main and PRs), ensures models are cached for integration and e2e tests
  preload-ml-models:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Cache ML models
      uses: actions/cache@v4
      id: cache-models
      with:
        path: |
          ~/.cache/whisper
          ~/.local/share/spacy
          ~/.cache/huggingface
        key: ml-models-${{ runner.os }}-v1
        restore-keys: |
          ml-models-${{ runner.os }}-
    - name: Install full dependencies (including ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
    - name: Preload ML models (if cache miss)
      if: steps.cache-models.outputs.cache-hit != 'true'
      run: make preload-ml-models

  # Full integration tests - all integration tests, runs on main branch only
  test-integration:
    runs-on: ubuntu-latest
    needs: [preload-ml-models]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v4
    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Cache ML models
      uses: actions/cache@v4
      id: cache-models
      with:
        path: |
          ~/.cache/whisper
          ~/.local/share/spacy
          ~/.cache/huggingface
        key: ml-models-${{ runner.os }}-v1
        restore-keys: |
          ml-models-${{ runner.os }}-
    - name: Install full dependencies (including ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket
    - name: Run all integration tests (full suite, with network guard)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run all integration tests with network guard
        set +e  # Don't exit on non-zero return code yet
        OUTPUT=$(pytest tests/integration/ -v -m integration -n auto --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (all integration tests should have many tests)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 50 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 50 integration tests"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed)"
          exit $PYTEST_EXIT_CODE
        fi
    - name: Post-build cleanup
      if: always()
      run: |
        # Keep model caches for next run (they're cached via GitHub Actions cache)
        rm -rf .pytest_cache .mypy_cache .build dist

  # Fast integration tests - critical path only, runs on PRs only (not on main)
  # Skip for documentation-only PRs
  test-integration-fast:
    runs-on: ubuntu-latest
    needs: [preload-ml-models]
    if: |
      github.event_name == 'pull_request' &&
      github.event.pull_request.head.ref != 'fix/ai-guidelines-linting' &&
      !contains(github.event.pull_request.head.ref, 'docs/')
    steps:
    - uses: actions/checkout@v4
    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Cache ML models
      uses: actions/cache@v4
      id: cache-models
      with:
        path: |
          ~/.cache/whisper
          ~/.local/share/spacy
          ~/.cache/huggingface
        key: ml-models-${{ runner.os }}-v1
        restore-keys: |
          ml-models-${{ runner.os }}-
    - name: Install dev dependencies with ML (pytest-socket for network guard)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket
    - name: Run fast integration tests (critical path only, with network guard)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run tests with network guard: --disable-socket --allow-hosts=127.0.0.1,localhost
        # Critical path tests only for faster CI feedback
        set +e  # Don't exit on non-zero return code yet
        OUTPUT=$(pytest tests/integration/ -v -m "integration and critical_path" -n auto --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 --durations=20 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (critical path integration tests should have some tests)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 5 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 5 critical path integration tests"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed)"
          exit $PYTEST_EXIT_CODE
        fi
    - name: Post-build cleanup
      if: always()
      run: |
        rm -rf .pytest_cache .mypy_cache .build dist

  # Fast E2E tests - critical path only, runs on PRs only (not on main)
  # Skip for documentation-only PRs
  test-e2e-fast:
    runs-on: ubuntu-latest
    needs: [preload-ml-models]
    if: |
      github.event_name == 'pull_request' &&
      github.event.pull_request.head.ref != 'fix/ai-guidelines-linting' &&
      !contains(github.event.pull_request.head.ref, 'docs/')
    steps:
    - uses: actions/checkout@v4
    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Cache ML models
      uses: actions/cache@v4
      id: cache-models
      with:
        path: |
          ~/.cache/whisper
          ~/.local/share/spacy
          ~/.cache/huggingface
        key: ml-models-${{ runner.os }}-v1
        restore-keys: |
          ml-models-${{ runner.os }}-
    - name: Install dev dependencies (pytest-socket for network guard)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket
    - name: Install ffmpeg (required for Whisper)
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends ffmpeg
    - name: Run fast E2E tests (critical path only, with network guard)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run tests with network guard: --disable-socket --allow-hosts=127.0.0.1,localhost
        # Critical path tests only for faster CI feedback
        set +e  # Don't exit on non-zero return code yet
        OUTPUT=$(pytest tests/e2e/ -v -m "e2e and critical_path" -n auto --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 --durations=20 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (critical path E2E tests should have some tests)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 3 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 3 critical path E2E tests"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed)"
          exit $PYTEST_EXIT_CODE
        fi
    - name: Post-build cleanup
      if: always()
      run: |
        rm -rf .pytest_cache .mypy_cache .build dist

  # Full E2E tests - all E2E tests, runs on main branch only
  test-e2e:
    runs-on: ubuntu-latest
    needs: [preload-ml-models]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v4
    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /home/linuxbrew/.linuxbrew
        docker image prune -af || true
        sudo apt-get clean
        sudo rm -rf /var/lib/apt/lists/*
        df -h
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: pyproject.toml
    - name: Cache ML models
      uses: actions/cache@v4
      id: cache-models
      with:
        path: |
          ~/.cache/whisper
          ~/.local/share/spacy
          ~/.cache/huggingface
        key: ml-models-${{ runner.os }}-v1
        restore-keys: |
          ml-models-${{ runner.os }}-
    - name: Install full dependencies (including ML)
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev,ml]
        pip install pytest-socket
    - name: Install ffmpeg (required for Whisper)
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends ffmpeg
    - name: Run all E2E tests (full suite, with network guard)
      run: |
        export PYTHONPATH="${PYTHONPATH}:$(pwd)"
        # Run all E2E tests with network guard
        set +e  # Don't exit on non-zero return code yet
        OUTPUT=$(pytest tests/e2e/ -v -m e2e -n auto --disable-socket --allow-hosts=127.0.0.1,localhost --reruns 2 --reruns-delay 1 2>&1)
        PYTEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        echo "$OUTPUT"
        # Verify tests were collected and run
        if echo "$OUTPUT" | grep -q "no tests collected"; then
          echo "ERROR: No tests were collected!"
          exit 1
        fi
        # Verify minimum test count (all E2E tests should have many tests)
        # Extract test count from output (handles formats like "229 passed" or "3 failed, 226 passed")
        TEST_COUNT=$(echo "$OUTPUT" | grep -oE "[0-9]+ passed" | head -1 | grep -oE "[0-9]+" || echo "0")
        if [ "$TEST_COUNT" -lt 50 ]; then
          echo "ERROR: Only $TEST_COUNT tests passed, expected at least 50 E2E tests"
          exit 1
        fi
        # Exit with pytest's exit code (fails if any tests failed)
        if [ $PYTEST_EXIT_CODE -ne 0 ]; then
          echo "ERROR: pytest exited with code $PYTEST_EXIT_CODE (some tests failed)"
          exit $PYTEST_EXIT_CODE
        fi
    - name: Post-build cleanup
      if: always()
      run: |
        # Keep model caches for next run (they're cached via GitHub Actions cache)
        rm -rf .pytest_cache .mypy_cache .build dist

  # Documentation build
  docs:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
        cache-dependency-path: |
          docs/requirements.txt
          pyproject.toml
    - name: Install doc dependencies (includes ML for mkdocstrings)
      run: |
        python -m pip install --upgrade pip
        pip install -r docs/requirements.txt
        pip install -e .[ml]
    - name: Build docs
      run: make docs

  # Build package
  build:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: "pip"
    - name: Install build tools
      run: |
        python -m pip install --upgrade pip
        pip install build
    - name: Build package
      run: make build
