id: baseline_ml_dev_authority_smoke_v1
task: summarization
backend:
  type: hf_local
  map_model: bart-small
  reduce_model: long-fast
data:
  dataset_id: curated_5feeds_smoke_v1
preprocessing_profile: cleaning_v4
map_params:
  max_new_tokens: 200
  min_new_tokens: 80
  num_beams: 4
  no_repeat_ngram_size: 3
  length_penalty: 1.0
  early_stopping: true
  repetition_penalty: 1.3
reduce_params:
  max_new_tokens: 650
  min_new_tokens: 220
  num_beams: 4
  no_repeat_ngram_size: 3
  length_penalty: 1.0
  early_stopping: true
  repetition_penalty: 1.3
tokenize:
  map_max_input_tokens: 1024
  reduce_max_input_tokens: 4096
  truncation: true
chunking:
  strategy: word_chunking
  word_chunk_size: 900
  word_overlap: 150
